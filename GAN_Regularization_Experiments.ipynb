{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0LL8iForVf2",
        "outputId": "4de5d9e2-ada5-42bb-fa4e-b0bf43b53bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping torch-fidelity as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages (Colab environment setup)\n",
        "!pip install torch torchvision torchmetrics --quiet\n",
        "!pip uninstall -y torch-fidelity -q\n",
        "!pip install torch-fidelity==0.3.0 --quiet\n",
        "!pip install matplotlib tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and reproducibility\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For evaluation metrics\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models.inception import inception_v3\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torch_fidelity import calculate_metrics\n",
        "from scipy.stats import entropy\n",
        "import pandas as pd\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "BbfzcSWyrV01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility of results\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "98CCJTvBrV3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for storing experiment results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBzUeu2DrV5u",
        "outputId": "75a18ca5-e709-43d4-ba85-69e2e279ef55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E1_Baseline"
      ],
      "metadata": {
        "id": "P74zAbmPrwca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment configuration (E1 baseline)\n",
        "experiment_id = \"E1_Baseline\"\n",
        "\n",
        "config = {\n",
        "    \"experiment_id\": experiment_id,     # Experiment ID matches report table\n",
        "    \"model\": \"DCGAN\",                   # Architecture used\n",
        "    \"regularization_type\": None,        # No regularization in baseline\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": None,   # No placement since no regularization\n",
        "    \"dataset\": \"CIFAR-10\",\n",
        "    \"image_size\": 32,\n",
        "    \"latent_dim\": 100,                  # Dimensionality of noise vector z\n",
        "    \"num_epochs\": 100,\n",
        "    \"batch_size\": 128,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"beta1\": 0.5,\n",
        "    \"fid_samples\": 10000,               # Number of images for FID/IS eval\n",
        "    \"save_images_every\": 10,            # Save sample grid every N epochs\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"pytorch_version\": torch.__version__,\n",
        "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"None\",\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\").type\n",
        "}"
      ],
      "metadata": {
        "id": "Kj0h3s_JrV8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create experiment output folders in Drive\n",
        "ROOT_DIR = \"/content/drive/MyDrive/GAN_Research/\"\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "8WrgklVfrV-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config file for full traceability\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKiIM7d9rWAk",
        "outputId": "06b178c2-db53-4f54-cfb5-3e14cc65a04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E1_Baseline/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise for sample images\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "noise_path = os.path.join(exp_path, \"fixed_noise.pt\")\n",
        "torch.save(fixed_noise, noise_path)"
      ],
      "metadata": {
        "id": "dAVDnl9rrWDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(config[\"image_size\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)     # Normalization to [-1, 1]\n",
        "])\n",
        "dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=2, pin_memory=True)\n",
        "print(f\"CIFAR-10 loaded: {len(dataset)} images, Batch size: {config['batch_size']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTrnlJQ1rWFm",
        "outputId": "ad4521ec-e7ea-4e8f-b10d-5ad7211ddc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 loaded: 50000 images, Batch size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Generator and Discriminator (DCGAN)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, feature_maps=64, out_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # First layer: fully-connected transpose conv from noise\n",
        "            nn.ConvTranspose2d(latent_dim, feature_maps * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Upsampling layers\n",
        "            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output RGB image with Tanh to match normalization\n",
        "            nn.ConvTranspose2d(feature_maps, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.main(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, feature_maps=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "            # Downsampling conv layers\n",
        "            nn.Conv2d(in_channels, feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Output single probability via Sigmoid\n",
        "            nn.Conv2d(feature_maps * 8, 1, 2, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.main(img).view(-1, 1)"
      ],
      "metadata": {
        "id": "XoTMEu6SrWIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and apply DCGAN weight init\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1 or classname.find(\"BatchNorm\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNMzx42drWMy",
        "outputId": "67069875-7128-4378-c02a-093bc01ffcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "EJ_tcCBErWPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3VVKaeLrWRt",
        "outputId": "309969b7-2703-417a-9ec7-0b271c599bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
        "        fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "        # Real images\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        # Fake images\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        # Train D loss\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Save images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # Log average losses\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save losses to CSV\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save total training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpmWTPfXrWUC",
        "outputId": "1b8868d2-d3c7-43ae-fde7-84acffc3521c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.6829, Loss_G: 2.2611\n",
            "[Epoch 2/100] Loss_D: 0.9266, Loss_G: 2.2772\n",
            "[Epoch 3/100] Loss_D: 0.9702, Loss_G: 2.0095\n",
            "[Epoch 4/100] Loss_D: 1.0438, Loss_G: 1.7816\n",
            "[Epoch 5/100] Loss_D: 1.1391, Loss_G: 1.4380\n",
            "[Epoch 6/100] Loss_D: 1.1416, Loss_G: 1.4816\n",
            "[Epoch 7/100] Loss_D: 1.0989, Loss_G: 1.4955\n",
            "[Epoch 8/100] Loss_D: 1.0283, Loss_G: 1.6086\n",
            "[Epoch 9/100] Loss_D: 1.0112, Loss_G: 1.7331\n",
            "[Epoch 10/100] Loss_D: 0.9764, Loss_G: 1.7657\n",
            "[Epoch 11/100] Loss_D: 0.9786, Loss_G: 1.7815\n",
            "[Epoch 12/100] Loss_D: 0.9785, Loss_G: 1.7551\n",
            "[Epoch 13/100] Loss_D: 0.9513, Loss_G: 1.7207\n",
            "[Epoch 14/100] Loss_D: 0.9293, Loss_G: 1.7766\n",
            "[Epoch 15/100] Loss_D: 0.8960, Loss_G: 1.8130\n",
            "[Epoch 16/100] Loss_D: 0.8931, Loss_G: 1.8299\n",
            "[Epoch 17/100] Loss_D: 0.8521, Loss_G: 1.8922\n",
            "[Epoch 18/100] Loss_D: 0.8193, Loss_G: 1.9639\n",
            "[Epoch 19/100] Loss_D: 0.8330, Loss_G: 1.9601\n",
            "[Epoch 20/100] Loss_D: 0.8032, Loss_G: 1.9797\n",
            "[Epoch 21/100] Loss_D: 0.7544, Loss_G: 2.0833\n",
            "[Epoch 22/100] Loss_D: 0.7408, Loss_G: 2.1063\n",
            "[Epoch 23/100] Loss_D: 0.7124, Loss_G: 2.1690\n",
            "[Epoch 24/100] Loss_D: 0.7182, Loss_G: 2.2394\n",
            "[Epoch 25/100] Loss_D: 0.6877, Loss_G: 2.2422\n",
            "[Epoch 26/100] Loss_D: 0.6619, Loss_G: 2.3906\n",
            "[Epoch 27/100] Loss_D: 0.6166, Loss_G: 2.4488\n",
            "[Epoch 28/100] Loss_D: 0.6204, Loss_G: 2.4623\n",
            "[Epoch 29/100] Loss_D: 0.5919, Loss_G: 2.5457\n",
            "[Epoch 30/100] Loss_D: 0.5713, Loss_G: 2.5999\n",
            "[Epoch 31/100] Loss_D: 0.5582, Loss_G: 2.6845\n",
            "[Epoch 32/100] Loss_D: 0.5815, Loss_G: 2.6418\n",
            "[Epoch 33/100] Loss_D: 0.5527, Loss_G: 2.7701\n",
            "[Epoch 34/100] Loss_D: 0.4956, Loss_G: 2.7649\n",
            "[Epoch 35/100] Loss_D: 0.4929, Loss_G: 2.8644\n",
            "[Epoch 36/100] Loss_D: 0.4906, Loss_G: 2.8654\n",
            "[Epoch 37/100] Loss_D: 0.4998, Loss_G: 2.8635\n",
            "[Epoch 38/100] Loss_D: 0.4652, Loss_G: 3.0950\n",
            "[Epoch 39/100] Loss_D: 0.4571, Loss_G: 2.9735\n",
            "[Epoch 40/100] Loss_D: 0.4664, Loss_G: 3.0305\n",
            "[Epoch 41/100] Loss_D: 0.4083, Loss_G: 3.0404\n",
            "[Epoch 42/100] Loss_D: 0.4341, Loss_G: 3.1670\n",
            "[Epoch 43/100] Loss_D: 0.3778, Loss_G: 3.1865\n",
            "[Epoch 44/100] Loss_D: 0.4679, Loss_G: 3.2528\n",
            "[Epoch 45/100] Loss_D: 0.4100, Loss_G: 3.2423\n",
            "[Epoch 46/100] Loss_D: 0.3744, Loss_G: 3.3077\n",
            "[Epoch 47/100] Loss_D: 0.4194, Loss_G: 3.3043\n",
            "[Epoch 48/100] Loss_D: 0.3505, Loss_G: 3.3691\n",
            "[Epoch 49/100] Loss_D: 0.3921, Loss_G: 3.3892\n",
            "[Epoch 50/100] Loss_D: 0.3751, Loss_G: 3.4360\n",
            "[Epoch 51/100] Loss_D: 0.3992, Loss_G: 3.4918\n",
            "[Epoch 52/100] Loss_D: 0.3656, Loss_G: 3.4505\n",
            "[Epoch 53/100] Loss_D: 0.3721, Loss_G: 3.5064\n",
            "[Epoch 54/100] Loss_D: 0.3485, Loss_G: 3.5978\n",
            "[Epoch 55/100] Loss_D: 0.3143, Loss_G: 3.6259\n",
            "[Epoch 56/100] Loss_D: 0.3281, Loss_G: 3.7517\n",
            "[Epoch 57/100] Loss_D: 0.3430, Loss_G: 3.6313\n",
            "[Epoch 58/100] Loss_D: 0.3501, Loss_G: 3.7940\n",
            "[Epoch 59/100] Loss_D: 0.2893, Loss_G: 3.7915\n",
            "[Epoch 60/100] Loss_D: 0.3183, Loss_G: 3.9285\n",
            "[Epoch 61/100] Loss_D: 0.3252, Loss_G: 3.7685\n",
            "[Epoch 62/100] Loss_D: 0.3082, Loss_G: 3.7975\n",
            "[Epoch 63/100] Loss_D: 0.2850, Loss_G: 3.8593\n",
            "[Epoch 64/100] Loss_D: 0.2948, Loss_G: 4.0060\n",
            "[Epoch 65/100] Loss_D: 0.3198, Loss_G: 3.8730\n",
            "[Epoch 66/100] Loss_D: 0.2741, Loss_G: 3.9429\n",
            "[Epoch 67/100] Loss_D: 0.3005, Loss_G: 3.9847\n",
            "[Epoch 68/100] Loss_D: 0.3053, Loss_G: 4.0412\n",
            "[Epoch 69/100] Loss_D: 0.2519, Loss_G: 4.1382\n",
            "[Epoch 70/100] Loss_D: 0.2437, Loss_G: 4.2054\n",
            "[Epoch 71/100] Loss_D: 0.2487, Loss_G: 4.3313\n",
            "[Epoch 72/100] Loss_D: 0.2907, Loss_G: 4.1217\n",
            "[Epoch 73/100] Loss_D: 0.2584, Loss_G: 4.2680\n",
            "[Epoch 74/100] Loss_D: 0.2469, Loss_G: 4.1281\n",
            "[Epoch 75/100] Loss_D: 0.2624, Loss_G: 4.2547\n",
            "[Epoch 76/100] Loss_D: 0.2866, Loss_G: 4.2453\n",
            "[Epoch 77/100] Loss_D: 0.2044, Loss_G: 4.4330\n",
            "[Epoch 78/100] Loss_D: 0.2622, Loss_G: 4.3469\n",
            "[Epoch 79/100] Loss_D: 0.2596, Loss_G: 4.2418\n",
            "[Epoch 80/100] Loss_D: 0.2213, Loss_G: 4.4002\n",
            "[Epoch 81/100] Loss_D: 0.2579, Loss_G: 4.3793\n",
            "[Epoch 82/100] Loss_D: 0.2455, Loss_G: 4.2719\n",
            "[Epoch 83/100] Loss_D: 0.2202, Loss_G: 4.5677\n",
            "[Epoch 84/100] Loss_D: 0.1765, Loss_G: 4.3812\n",
            "[Epoch 85/100] Loss_D: 0.2380, Loss_G: 4.6989\n",
            "[Epoch 86/100] Loss_D: 0.2285, Loss_G: 4.3951\n",
            "[Epoch 87/100] Loss_D: 0.2099, Loss_G: 4.3969\n",
            "[Epoch 88/100] Loss_D: 0.2313, Loss_G: 4.7338\n",
            "[Epoch 89/100] Loss_D: 0.2215, Loss_G: 4.4908\n",
            "[Epoch 90/100] Loss_D: 0.2201, Loss_G: 4.5217\n",
            "[Epoch 91/100] Loss_D: 0.1989, Loss_G: 4.8049\n",
            "[Epoch 92/100] Loss_D: 0.1680, Loss_G: 4.6828\n",
            "[Epoch 93/100] Loss_D: 0.2405, Loss_G: 4.7418\n",
            "[Epoch 94/100] Loss_D: 0.1718, Loss_G: 4.6837\n",
            "[Epoch 95/100] Loss_D: 0.2017, Loss_G: 4.7597\n",
            "[Epoch 96/100] Loss_D: 0.1928, Loss_G: 4.8555\n",
            "[Epoch 97/100] Loss_D: 0.1831, Loss_G: 4.8900\n",
            "[Epoch 98/100] Loss_D: 0.1998, Loss_G: 4.6454\n",
            "[Epoch 99/100] Loss_D: 0.1981, Loss_G: 4.7855\n",
            "[Epoch 100/100] Loss_D: 0.1633, Loss_G: 4.9466\n",
            "Training completed in 139 min 15 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAOPr5RmrWWS",
        "outputId": "de169f62-c8da-4c43-854c-914ab4c0f4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E1_Baseline/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E1_Baseline/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pp3S8RrrWYk",
        "outputId": "dd81e779-49a6-4fc1-d4cd-16a2b5c236a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:52<00:00,  1.48it/s]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:02<00:00, 32.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:02<00:00, 39.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 47.6336\n",
            "Inception Score: 3.5376 ± 0.0812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E2_L1_GenOut"
      ],
      "metadata": {
        "id": "NQJCgv26OpEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E2: L1 Regularization on Generator Output\n",
        "\n",
        "# Experiment Configuration (E2)\n",
        "experiment_id = \"E2_L1_GenOut\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L1\",                 # Type of regularisation used in this run\n",
        "    \"regularization_lambda_L1\": 0.0001,          # λ for L1 as per your experiment table\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"generator_output\",  # Apply penalty to G's output images\n",
        "})"
      ],
      "metadata": {
        "id": "UsbSYu48rWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders in Google Drive\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "voIfhvWdrWdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Save config file\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzP_PL0ZrWgB",
        "outputId": "f9af812a-2e27-47eb-d384-c59aafd3304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E2_L1_GenOut/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise for sample images\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "tUYekv1-rWic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and apply DCGAN weight init\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized for E2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVZBfv1JrWlA",
        "outputId": "4940cfd5-7788-407a-af87-0d9e02cc88af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "rkXFAXPorWnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG_xGNpBrWpz",
        "outputId": "5632c1a4-9fa5-4f77-8a42-bbda78375cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
        "        fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()     # Detach so D update doesn't backprop into G\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "\n",
        "        # Add L1 regularization on generator output\n",
        "        # Penalize absolute pixel values of G(z)\n",
        "        if config[\"regularization_type\"] == \"L1\" and config[\"regularization_placement\"] == \"generator_output\":\n",
        "            l1_reg = config[\"regularization_lambda_L1\"] * torch.mean(torch.abs(fake_imgs))\n",
        "            loss_G += l1_reg\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Save images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # Log average losses\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Write updated loss CSV\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save total training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E37YCl2PrWsR",
        "outputId": "7fd28b7b-f98e-4a08-871b-2964519ba513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.8005, Loss_G: 2.0051\n",
            "[Epoch 2/100] Loss_D: 0.9729, Loss_G: 2.1822\n",
            "[Epoch 3/100] Loss_D: 1.0951, Loss_G: 1.7652\n",
            "[Epoch 4/100] Loss_D: 1.0553, Loss_G: 1.7625\n",
            "[Epoch 5/100] Loss_D: 1.2033, Loss_G: 1.3583\n",
            "[Epoch 6/100] Loss_D: 1.1970, Loss_G: 1.3446\n",
            "[Epoch 7/100] Loss_D: 1.1838, Loss_G: 1.2788\n",
            "[Epoch 8/100] Loss_D: 1.1677, Loss_G: 1.3155\n",
            "[Epoch 9/100] Loss_D: 1.1648, Loss_G: 1.3173\n",
            "[Epoch 10/100] Loss_D: 1.1214, Loss_G: 1.3738\n",
            "[Epoch 11/100] Loss_D: 1.1266, Loss_G: 1.3891\n",
            "[Epoch 12/100] Loss_D: 1.1163, Loss_G: 1.4070\n",
            "[Epoch 13/100] Loss_D: 1.0986, Loss_G: 1.3891\n",
            "[Epoch 14/100] Loss_D: 1.0731, Loss_G: 1.4182\n",
            "[Epoch 15/100] Loss_D: 1.0731, Loss_G: 1.4332\n",
            "[Epoch 16/100] Loss_D: 1.0524, Loss_G: 1.4668\n",
            "[Epoch 17/100] Loss_D: 1.0134, Loss_G: 1.5229\n",
            "[Epoch 18/100] Loss_D: 0.9845, Loss_G: 1.5948\n",
            "[Epoch 19/100] Loss_D: 0.9558, Loss_G: 1.6014\n",
            "[Epoch 20/100] Loss_D: 0.9449, Loss_G: 1.6744\n",
            "[Epoch 21/100] Loss_D: 0.8989, Loss_G: 1.7096\n",
            "[Epoch 22/100] Loss_D: 0.8778, Loss_G: 1.7286\n",
            "[Epoch 23/100] Loss_D: 0.8630, Loss_G: 1.8531\n",
            "[Epoch 24/100] Loss_D: 0.8255, Loss_G: 1.8257\n",
            "[Epoch 25/100] Loss_D: 0.7874, Loss_G: 1.9415\n",
            "[Epoch 26/100] Loss_D: 0.7924, Loss_G: 1.9583\n",
            "[Epoch 27/100] Loss_D: 0.7889, Loss_G: 1.9686\n",
            "[Epoch 28/100] Loss_D: 0.7273, Loss_G: 2.0340\n",
            "[Epoch 29/100] Loss_D: 0.7488, Loss_G: 2.1361\n",
            "[Epoch 30/100] Loss_D: 0.6999, Loss_G: 2.1913\n",
            "[Epoch 31/100] Loss_D: 0.6838, Loss_G: 2.2417\n",
            "[Epoch 32/100] Loss_D: 0.6476, Loss_G: 2.2368\n",
            "[Epoch 33/100] Loss_D: 0.6339, Loss_G: 2.4377\n",
            "[Epoch 34/100] Loss_D: 0.6036, Loss_G: 2.4638\n",
            "[Epoch 35/100] Loss_D: 0.5827, Loss_G: 2.4754\n",
            "[Epoch 36/100] Loss_D: 0.5741, Loss_G: 2.5453\n",
            "[Epoch 37/100] Loss_D: 0.6063, Loss_G: 2.5423\n",
            "[Epoch 38/100] Loss_D: 0.5288, Loss_G: 2.6944\n",
            "[Epoch 39/100] Loss_D: 0.5285, Loss_G: 2.7817\n",
            "[Epoch 40/100] Loss_D: 0.5330, Loss_G: 2.7401\n",
            "[Epoch 41/100] Loss_D: 0.5409, Loss_G: 2.7300\n",
            "[Epoch 42/100] Loss_D: 0.4938, Loss_G: 2.7933\n",
            "[Epoch 43/100] Loss_D: 0.4878, Loss_G: 2.8294\n",
            "[Epoch 44/100] Loss_D: 0.4739, Loss_G: 2.8737\n",
            "[Epoch 45/100] Loss_D: 0.4696, Loss_G: 2.9726\n",
            "[Epoch 46/100] Loss_D: 0.4235, Loss_G: 2.9953\n",
            "[Epoch 47/100] Loss_D: 0.4199, Loss_G: 3.1656\n",
            "[Epoch 48/100] Loss_D: 0.4952, Loss_G: 3.0434\n",
            "[Epoch 49/100] Loss_D: 0.3998, Loss_G: 3.1830\n",
            "[Epoch 50/100] Loss_D: 0.3846, Loss_G: 3.1852\n",
            "[Epoch 51/100] Loss_D: 0.4116, Loss_G: 3.2830\n",
            "[Epoch 52/100] Loss_D: 0.3872, Loss_G: 3.3418\n",
            "[Epoch 53/100] Loss_D: 0.3764, Loss_G: 3.3449\n",
            "[Epoch 54/100] Loss_D: 0.3667, Loss_G: 3.4166\n",
            "[Epoch 55/100] Loss_D: 0.3691, Loss_G: 3.4727\n",
            "[Epoch 56/100] Loss_D: 0.3762, Loss_G: 3.5267\n",
            "[Epoch 57/100] Loss_D: 0.3593, Loss_G: 3.5912\n",
            "[Epoch 58/100] Loss_D: 0.3531, Loss_G: 3.5858\n",
            "[Epoch 59/100] Loss_D: 0.3163, Loss_G: 3.5703\n",
            "[Epoch 60/100] Loss_D: 0.3171, Loss_G: 3.6687\n",
            "[Epoch 61/100] Loss_D: 0.3458, Loss_G: 3.7519\n",
            "[Epoch 62/100] Loss_D: 0.2975, Loss_G: 3.6645\n",
            "[Epoch 63/100] Loss_D: 0.3037, Loss_G: 3.7928\n",
            "[Epoch 64/100] Loss_D: 0.3009, Loss_G: 3.7727\n",
            "[Epoch 65/100] Loss_D: 0.2986, Loss_G: 4.0001\n",
            "[Epoch 66/100] Loss_D: 0.3279, Loss_G: 3.8539\n",
            "[Epoch 67/100] Loss_D: 0.3022, Loss_G: 3.9391\n",
            "[Epoch 68/100] Loss_D: 0.2785, Loss_G: 4.0007\n",
            "[Epoch 69/100] Loss_D: 0.2880, Loss_G: 4.1404\n",
            "[Epoch 70/100] Loss_D: 0.2862, Loss_G: 3.8906\n",
            "[Epoch 71/100] Loss_D: 0.2866, Loss_G: 3.9992\n",
            "[Epoch 72/100] Loss_D: 0.2532, Loss_G: 4.1192\n",
            "[Epoch 73/100] Loss_D: 0.3046, Loss_G: 4.0244\n",
            "[Epoch 74/100] Loss_D: 0.2100, Loss_G: 4.0361\n",
            "[Epoch 75/100] Loss_D: 0.2920, Loss_G: 4.2092\n",
            "[Epoch 76/100] Loss_D: 0.2466, Loss_G: 4.1159\n",
            "[Epoch 77/100] Loss_D: 0.3005, Loss_G: 4.1644\n",
            "[Epoch 78/100] Loss_D: 0.2722, Loss_G: 4.2277\n",
            "[Epoch 79/100] Loss_D: 0.2455, Loss_G: 4.0388\n",
            "[Epoch 80/100] Loss_D: 0.2699, Loss_G: 4.4041\n",
            "[Epoch 81/100] Loss_D: 0.2882, Loss_G: 4.1289\n",
            "[Epoch 82/100] Loss_D: 0.2386, Loss_G: 4.2266\n",
            "[Epoch 83/100] Loss_D: 0.1931, Loss_G: 4.5708\n",
            "[Epoch 84/100] Loss_D: 0.2143, Loss_G: 4.4703\n",
            "[Epoch 85/100] Loss_D: 0.2476, Loss_G: 4.5122\n",
            "[Epoch 86/100] Loss_D: 0.1699, Loss_G: 4.5044\n",
            "[Epoch 87/100] Loss_D: 0.2405, Loss_G: 4.3647\n",
            "[Epoch 88/100] Loss_D: 0.1970, Loss_G: 4.6607\n",
            "[Epoch 89/100] Loss_D: 0.2528, Loss_G: 4.3816\n",
            "[Epoch 90/100] Loss_D: 0.2131, Loss_G: 4.5784\n",
            "[Epoch 91/100] Loss_D: 0.2260, Loss_G: 4.5234\n",
            "[Epoch 92/100] Loss_D: 0.2515, Loss_G: 4.4951\n",
            "[Epoch 93/100] Loss_D: 0.2087, Loss_G: 4.3517\n",
            "[Epoch 94/100] Loss_D: 0.1940, Loss_G: 4.5322\n",
            "[Epoch 95/100] Loss_D: 0.1837, Loss_G: 4.9812\n",
            "[Epoch 96/100] Loss_D: 0.1574, Loss_G: 4.7559\n",
            "[Epoch 97/100] Loss_D: 0.2702, Loss_G: 4.7216\n",
            "[Epoch 98/100] Loss_D: 0.2213, Loss_G: 4.6539\n",
            "[Epoch 99/100] Loss_D: 0.1489, Loss_G: 4.8920\n",
            "[Epoch 100/100] Loss_D: 0.2351, Loss_G: 4.6918\n",
            "Training completed in 140 min 1 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJUwPh1urWuv",
        "outputId": "b0b20704-de6d-489c-d127-ee9c8047f6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E2_L1_GenOut/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E2_L1_GenOut/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KIp_K2WrWxH",
        "outputId": "94bbf3a9-b3c6-4e0c-eee1-c26bdee210ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:53<00:00,  1.46it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 50.2813\n",
            "Inception Score: 3.4228 ± 0.0662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E2_L2_GenOut"
      ],
      "metadata": {
        "id": "crouExswshOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E2_L2_GenOut: L2 Regularization on Generator Output\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E2_L2_GenOut\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L2\",\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.001,     # λ for L2 penalty\n",
        "    \"regularization_placement\": \"generator_output\",\n",
        "})"
      ],
      "metadata": {
        "id": "eQYLVZPhrWzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders in Google Drive\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "-4WhcRVTrW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_9qjt44rW5J",
        "outputId": "52d839bf-0c32-4011-d583-a515f634f609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E2_L2_GenOut/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "_KOFF_R0rW7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and apply DCGAN weight init\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized for E2_L2_GenOut\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvGdHVF8rW9-",
        "outputId": "9f0a6d0a-8679-49f4-a933-0666ae0aec35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E2_L2_GenOut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "n28-YjAarXAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV7zMkECrXDO",
        "outputId": "b04079c5-d17b-4c81-bc3a-d9dacdfdd9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
        "        fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "\n",
        "        # Add L2 regularization on generator output\n",
        "        # Penalize the mean squared pixel magnitude of G(z), weighted by λ\n",
        "        if config[\"regularization_type\"] == \"L2\" and config[\"regularization_placement\"] == \"generator_output\":\n",
        "            l2_reg = config[\"regularization_lambda_L2\"] * torch.mean(fake_imgs ** 2)\n",
        "            loss_G += l2_reg\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Save images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # Log average losses\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Write updated loss CSV\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save total training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeBS187ZrXFo",
        "outputId": "5c774eda-697c-493d-fd96-004ed8858f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.6767, Loss_G: 2.1919\n",
            "[Epoch 2/100] Loss_D: 0.8573, Loss_G: 2.5960\n",
            "[Epoch 3/100] Loss_D: 1.0505, Loss_G: 1.9265\n",
            "[Epoch 4/100] Loss_D: 1.0378, Loss_G: 1.7756\n",
            "[Epoch 5/100] Loss_D: 1.1298, Loss_G: 1.5028\n",
            "[Epoch 6/100] Loss_D: 1.1546, Loss_G: 1.4251\n",
            "[Epoch 7/100] Loss_D: 1.1680, Loss_G: 1.4205\n",
            "[Epoch 8/100] Loss_D: 1.1155, Loss_G: 1.4915\n",
            "[Epoch 9/100] Loss_D: 1.1068, Loss_G: 1.4926\n",
            "[Epoch 10/100] Loss_D: 1.0709, Loss_G: 1.5466\n",
            "[Epoch 11/100] Loss_D: 1.0473, Loss_G: 1.5676\n",
            "[Epoch 12/100] Loss_D: 1.0533, Loss_G: 1.6191\n",
            "[Epoch 13/100] Loss_D: 1.0363, Loss_G: 1.5769\n",
            "[Epoch 14/100] Loss_D: 1.0203, Loss_G: 1.6400\n",
            "[Epoch 15/100] Loss_D: 1.0083, Loss_G: 1.6291\n",
            "[Epoch 16/100] Loss_D: 1.0020, Loss_G: 1.6083\n",
            "[Epoch 17/100] Loss_D: 0.9748, Loss_G: 1.6453\n",
            "[Epoch 18/100] Loss_D: 0.9392, Loss_G: 1.6680\n",
            "[Epoch 19/100] Loss_D: 0.9298, Loss_G: 1.7331\n",
            "[Epoch 20/100] Loss_D: 0.8848, Loss_G: 1.7921\n",
            "[Epoch 21/100] Loss_D: 0.8586, Loss_G: 1.8433\n",
            "[Epoch 22/100] Loss_D: 0.8420, Loss_G: 1.8965\n",
            "[Epoch 23/100] Loss_D: 0.8120, Loss_G: 1.9570\n",
            "[Epoch 24/100] Loss_D: 0.7661, Loss_G: 2.0964\n",
            "[Epoch 25/100] Loss_D: 0.7565, Loss_G: 2.0655\n",
            "[Epoch 26/100] Loss_D: 0.7280, Loss_G: 2.1198\n",
            "[Epoch 27/100] Loss_D: 0.7155, Loss_G: 2.2106\n",
            "[Epoch 28/100] Loss_D: 0.6703, Loss_G: 2.2946\n",
            "[Epoch 29/100] Loss_D: 0.6645, Loss_G: 2.2725\n",
            "[Epoch 30/100] Loss_D: 0.6520, Loss_G: 2.4265\n",
            "[Epoch 31/100] Loss_D: 0.6180, Loss_G: 2.4476\n",
            "[Epoch 32/100] Loss_D: 0.6003, Loss_G: 2.4360\n",
            "[Epoch 33/100] Loss_D: 0.6063, Loss_G: 2.4957\n",
            "[Epoch 34/100] Loss_D: 0.5615, Loss_G: 2.6017\n",
            "[Epoch 35/100] Loss_D: 0.5542, Loss_G: 2.5588\n",
            "[Epoch 36/100] Loss_D: 0.5542, Loss_G: 2.6992\n",
            "[Epoch 37/100] Loss_D: 0.5115, Loss_G: 2.6875\n",
            "[Epoch 38/100] Loss_D: 0.5112, Loss_G: 2.9375\n",
            "[Epoch 39/100] Loss_D: 0.5061, Loss_G: 2.8371\n",
            "[Epoch 40/100] Loss_D: 0.4926, Loss_G: 2.8909\n",
            "[Epoch 41/100] Loss_D: 0.4576, Loss_G: 2.9409\n",
            "[Epoch 42/100] Loss_D: 0.4700, Loss_G: 2.9927\n",
            "[Epoch 43/100] Loss_D: 0.4309, Loss_G: 3.0132\n",
            "[Epoch 44/100] Loss_D: 0.4351, Loss_G: 3.0922\n",
            "[Epoch 45/100] Loss_D: 0.4221, Loss_G: 3.2211\n",
            "[Epoch 46/100] Loss_D: 0.3947, Loss_G: 3.2669\n",
            "[Epoch 47/100] Loss_D: 0.4255, Loss_G: 3.2440\n",
            "[Epoch 48/100] Loss_D: 0.3901, Loss_G: 3.2817\n",
            "[Epoch 49/100] Loss_D: 0.4227, Loss_G: 3.3000\n",
            "[Epoch 50/100] Loss_D: 0.3687, Loss_G: 3.3526\n",
            "[Epoch 51/100] Loss_D: 0.3844, Loss_G: 3.4175\n",
            "[Epoch 52/100] Loss_D: 0.3540, Loss_G: 3.3848\n",
            "[Epoch 53/100] Loss_D: 0.3291, Loss_G: 3.4539\n",
            "[Epoch 54/100] Loss_D: 0.3808, Loss_G: 3.5887\n",
            "[Epoch 55/100] Loss_D: 0.3734, Loss_G: 3.4713\n",
            "[Epoch 56/100] Loss_D: 0.3631, Loss_G: 3.4706\n",
            "[Epoch 57/100] Loss_D: 0.3233, Loss_G: 3.6152\n",
            "[Epoch 58/100] Loss_D: 0.3604, Loss_G: 3.6766\n",
            "[Epoch 59/100] Loss_D: 0.2865, Loss_G: 3.8059\n",
            "[Epoch 60/100] Loss_D: 0.3021, Loss_G: 3.9263\n",
            "[Epoch 61/100] Loss_D: 0.2742, Loss_G: 3.8777\n",
            "[Epoch 62/100] Loss_D: 0.3274, Loss_G: 4.0328\n",
            "[Epoch 63/100] Loss_D: 0.3088, Loss_G: 4.0110\n",
            "[Epoch 64/100] Loss_D: 0.2872, Loss_G: 3.7688\n",
            "[Epoch 65/100] Loss_D: 0.2871, Loss_G: 4.0446\n",
            "[Epoch 66/100] Loss_D: 0.3084, Loss_G: 3.9189\n",
            "[Epoch 67/100] Loss_D: 0.2854, Loss_G: 3.9384\n",
            "[Epoch 68/100] Loss_D: 0.2695, Loss_G: 4.0716\n",
            "[Epoch 69/100] Loss_D: 0.3136, Loss_G: 4.1351\n",
            "[Epoch 70/100] Loss_D: 0.2385, Loss_G: 4.0436\n",
            "[Epoch 71/100] Loss_D: 0.2472, Loss_G: 4.1942\n",
            "[Epoch 72/100] Loss_D: 0.2902, Loss_G: 4.0711\n",
            "[Epoch 73/100] Loss_D: 0.2107, Loss_G: 4.2544\n",
            "[Epoch 74/100] Loss_D: 0.2925, Loss_G: 4.1649\n",
            "[Epoch 75/100] Loss_D: 0.2541, Loss_G: 4.1633\n",
            "[Epoch 76/100] Loss_D: 0.2871, Loss_G: 4.1318\n",
            "[Epoch 77/100] Loss_D: 0.2090, Loss_G: 4.3163\n",
            "[Epoch 78/100] Loss_D: 0.2486, Loss_G: 4.2064\n",
            "[Epoch 79/100] Loss_D: 0.2282, Loss_G: 4.4261\n",
            "[Epoch 80/100] Loss_D: 0.2320, Loss_G: 4.3252\n",
            "[Epoch 81/100] Loss_D: 0.2447, Loss_G: 4.5789\n",
            "[Epoch 82/100] Loss_D: 0.2704, Loss_G: 4.2195\n",
            "[Epoch 83/100] Loss_D: 0.1731, Loss_G: 4.5875\n",
            "[Epoch 84/100] Loss_D: 0.2157, Loss_G: 4.5442\n",
            "[Epoch 85/100] Loss_D: 0.2554, Loss_G: 4.3999\n",
            "[Epoch 86/100] Loss_D: 0.2260, Loss_G: 4.5103\n",
            "[Epoch 87/100] Loss_D: 0.2114, Loss_G: 4.6078\n",
            "[Epoch 88/100] Loss_D: 0.2632, Loss_G: 4.6597\n",
            "[Epoch 89/100] Loss_D: 0.2143, Loss_G: 4.5302\n",
            "[Epoch 90/100] Loss_D: 0.1740, Loss_G: 4.5447\n",
            "[Epoch 91/100] Loss_D: 0.2101, Loss_G: 4.6678\n",
            "[Epoch 92/100] Loss_D: 0.2286, Loss_G: 4.9171\n",
            "[Epoch 93/100] Loss_D: 0.1530, Loss_G: 4.7303\n",
            "[Epoch 94/100] Loss_D: 0.2302, Loss_G: 4.8934\n",
            "[Epoch 95/100] Loss_D: 0.2098, Loss_G: 4.5142\n",
            "[Epoch 96/100] Loss_D: 0.1645, Loss_G: 4.6337\n",
            "[Epoch 97/100] Loss_D: 0.2123, Loss_G: 4.5143\n",
            "[Epoch 98/100] Loss_D: 0.2280, Loss_G: 4.6045\n",
            "[Epoch 99/100] Loss_D: 0.1961, Loss_G: 4.6613\n",
            "[Epoch 100/100] Loss_D: 0.1490, Loss_G: 4.8501\n",
            "Training completed in 140 min 31 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS4aLmHSrXIP",
        "outputId": "f7133467-e160-49d8-c7eb-d62bd934c0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E2_L2_GenOut/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E2_L2_GenOut/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l14LlCprXK2",
        "outputId": "5507c850-3faa-4de1-b75a-448d7aaae795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:53<00:00,  1.47it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 45.5722\n",
            "Inception Score: 3.4781 ± 0.0819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E3_L1_DiscIn"
      ],
      "metadata": {
        "id": "usFFH-l2s16C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E3_L1_DiscIn: L1 Regularization on Discriminator Input\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E3_L1_DiscIn\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L1\",\n",
        "    \"regularization_lambda_L1\": 0.0001,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"discriminator_input\",\n",
        "})"
      ],
      "metadata": {
        "id": "vMTkjXlBrXNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "UG-JGf8hrXQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_o9fhTDrXS3",
        "outputId": "25a78d95-3b69-4a3e-a60f-f8438f7c9309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E3_L1_DiscIn/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "jQr0pQKIrXVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized for E3_L1_DiscIn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJNRGDJbrXXx",
        "outputId": "34ab80cd-4ca8-4fe1-8a0e-4781ef06e377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E3_L1_DiscIn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "uMtaygcJrXaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXwT5q9LrXc-",
        "outputId": "6b325a9a-1cff-41c6-b2b8-5059aa12f6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
        "        fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "\n",
        "        # Add L1 regularization on discriminator input\n",
        "        # Penalize absolute pixel values of fake images before they are fed to D\n",
        "        if config[\"regularization_type\"] == \"L1\" and config[\"regularization_placement\"] == \"discriminator_input\":\n",
        "            l1_fake = torch.mean(torch.abs(fake_imgs))\n",
        "            reg_term = config[\"regularization_lambda_L1\"] * l1_fake\n",
        "            loss_D += reg_term\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save image\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            fake_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(fake_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # Save running losses\n",
        "    metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPz5BIFyrXfn",
        "outputId": "2fb3650b-6baf-40f9-9bb6-a1643d4c952e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/100] Loss_D: 0.7086, Loss_G: 2.2412\n",
            "[Epoch 2/100] Loss_D: 0.8832, Loss_G: 2.6059\n",
            "[Epoch 3/100] Loss_D: 1.0935, Loss_G: 1.8180\n",
            "[Epoch 4/100] Loss_D: 0.9677, Loss_G: 2.0850\n",
            "[Epoch 5/100] Loss_D: 1.1115, Loss_G: 1.5879\n",
            "[Epoch 6/100] Loss_D: 1.1079, Loss_G: 1.5775\n",
            "[Epoch 7/100] Loss_D: 1.1012, Loss_G: 1.5388\n",
            "[Epoch 8/100] Loss_D: 1.0536, Loss_G: 1.6207\n",
            "[Epoch 9/100] Loss_D: 1.0201, Loss_G: 1.6660\n",
            "[Epoch 10/100] Loss_D: 1.0315, Loss_G: 1.6467\n",
            "[Epoch 11/100] Loss_D: 1.0412, Loss_G: 1.5895\n",
            "[Epoch 12/100] Loss_D: 1.0307, Loss_G: 1.6034\n",
            "[Epoch 13/100] Loss_D: 1.0166, Loss_G: 1.6174\n",
            "[Epoch 14/100] Loss_D: 1.0124, Loss_G: 1.6231\n",
            "[Epoch 15/100] Loss_D: 0.9778, Loss_G: 1.6456\n",
            "[Epoch 16/100] Loss_D: 0.9674, Loss_G: 1.7174\n",
            "[Epoch 17/100] Loss_D: 0.9229, Loss_G: 1.7181\n",
            "[Epoch 18/100] Loss_D: 0.9054, Loss_G: 1.7996\n",
            "[Epoch 19/100] Loss_D: 0.8715, Loss_G: 1.8304\n",
            "[Epoch 20/100] Loss_D: 0.8637, Loss_G: 1.9116\n",
            "[Epoch 21/100] Loss_D: 0.8379, Loss_G: 1.9329\n",
            "[Epoch 22/100] Loss_D: 0.7984, Loss_G: 1.9818\n",
            "[Epoch 23/100] Loss_D: 0.7908, Loss_G: 2.0055\n",
            "[Epoch 24/100] Loss_D: 0.7495, Loss_G: 2.1054\n",
            "[Epoch 25/100] Loss_D: 0.7297, Loss_G: 2.1949\n",
            "[Epoch 26/100] Loss_D: 0.7174, Loss_G: 2.2450\n",
            "[Epoch 27/100] Loss_D: 0.6803, Loss_G: 2.2442\n",
            "[Epoch 28/100] Loss_D: 0.6926, Loss_G: 2.3251\n",
            "[Epoch 29/100] Loss_D: 0.6455, Loss_G: 2.3465\n",
            "[Epoch 30/100] Loss_D: 0.6169, Loss_G: 2.4396\n",
            "[Epoch 31/100] Loss_D: 0.6015, Loss_G: 2.5576\n",
            "[Epoch 32/100] Loss_D: 0.6019, Loss_G: 2.4979\n",
            "[Epoch 33/100] Loss_D: 0.5762, Loss_G: 2.5907\n",
            "[Epoch 34/100] Loss_D: 0.5554, Loss_G: 2.6182\n",
            "[Epoch 35/100] Loss_D: 0.5376, Loss_G: 2.7377\n",
            "[Epoch 36/100] Loss_D: 0.5555, Loss_G: 2.7910\n",
            "[Epoch 37/100] Loss_D: 0.5264, Loss_G: 2.6984\n",
            "[Epoch 38/100] Loss_D: 0.5072, Loss_G: 2.8313\n",
            "[Epoch 39/100] Loss_D: 0.4821, Loss_G: 2.7832\n",
            "[Epoch 40/100] Loss_D: 0.4897, Loss_G: 2.9409\n",
            "[Epoch 41/100] Loss_D: 0.4784, Loss_G: 2.9501\n",
            "[Epoch 42/100] Loss_D: 0.4785, Loss_G: 2.9225\n",
            "[Epoch 43/100] Loss_D: 0.4327, Loss_G: 2.9931\n",
            "[Epoch 44/100] Loss_D: 0.4412, Loss_G: 3.0406\n",
            "[Epoch 45/100] Loss_D: 0.4322, Loss_G: 3.0245\n",
            "[Epoch 46/100] Loss_D: 0.4331, Loss_G: 3.1039\n",
            "[Epoch 47/100] Loss_D: 0.3968, Loss_G: 3.0685\n",
            "[Epoch 48/100] Loss_D: 0.4299, Loss_G: 3.2417\n",
            "[Epoch 49/100] Loss_D: 0.3856, Loss_G: 3.2709\n",
            "[Epoch 50/100] Loss_D: 0.3796, Loss_G: 3.2842\n",
            "[Epoch 51/100] Loss_D: 0.3857, Loss_G: 3.2611\n",
            "[Epoch 52/100] Loss_D: 0.3884, Loss_G: 3.2469\n",
            "[Epoch 53/100] Loss_D: 0.4172, Loss_G: 3.2795\n",
            "[Epoch 54/100] Loss_D: 0.3659, Loss_G: 3.2980\n",
            "[Epoch 55/100] Loss_D: 0.3197, Loss_G: 3.5110\n",
            "[Epoch 56/100] Loss_D: 0.4071, Loss_G: 3.4507\n",
            "[Epoch 57/100] Loss_D: 0.3460, Loss_G: 3.4771\n",
            "[Epoch 58/100] Loss_D: 0.3661, Loss_G: 3.5137\n",
            "[Epoch 59/100] Loss_D: 0.3562, Loss_G: 3.6356\n",
            "[Epoch 60/100] Loss_D: 0.2985, Loss_G: 3.7088\n",
            "[Epoch 61/100] Loss_D: 0.3227, Loss_G: 3.8301\n",
            "[Epoch 62/100] Loss_D: 0.3537, Loss_G: 3.6444\n",
            "[Epoch 63/100] Loss_D: 0.3429, Loss_G: 3.5886\n",
            "[Epoch 64/100] Loss_D: 0.2878, Loss_G: 3.6823\n",
            "[Epoch 65/100] Loss_D: 0.3474, Loss_G: 3.6259\n",
            "[Epoch 66/100] Loss_D: 0.2770, Loss_G: 3.8220\n",
            "[Epoch 67/100] Loss_D: 0.2574, Loss_G: 3.9572\n",
            "[Epoch 68/100] Loss_D: 0.3535, Loss_G: 3.8857\n",
            "[Epoch 69/100] Loss_D: 0.2961, Loss_G: 3.7818\n",
            "[Epoch 70/100] Loss_D: 0.2310, Loss_G: 4.0457\n",
            "[Epoch 71/100] Loss_D: 0.2639, Loss_G: 4.2845\n",
            "[Epoch 72/100] Loss_D: 0.2707, Loss_G: 3.9371\n",
            "[Epoch 73/100] Loss_D: 0.2849, Loss_G: 3.9804\n",
            "[Epoch 74/100] Loss_D: 0.2752, Loss_G: 3.9609\n",
            "[Epoch 75/100] Loss_D: 0.2661, Loss_G: 4.0353\n",
            "[Epoch 76/100] Loss_D: 0.2577, Loss_G: 4.0677\n",
            "[Epoch 77/100] Loss_D: 0.2173, Loss_G: 4.0896\n",
            "[Epoch 78/100] Loss_D: 0.3012, Loss_G: 4.0309\n",
            "[Epoch 79/100] Loss_D: 0.1896, Loss_G: 4.5005\n",
            "[Epoch 80/100] Loss_D: 0.2499, Loss_G: 4.3181\n",
            "[Epoch 81/100] Loss_D: 0.2745, Loss_G: 4.2922\n",
            "[Epoch 82/100] Loss_D: 0.2361, Loss_G: 4.2281\n",
            "[Epoch 83/100] Loss_D: 0.2286, Loss_G: 4.3980\n",
            "[Epoch 84/100] Loss_D: 0.2311, Loss_G: 4.4537\n",
            "[Epoch 85/100] Loss_D: 0.2529, Loss_G: 4.6459\n",
            "[Epoch 86/100] Loss_D: 0.2371, Loss_G: 4.2485\n",
            "[Epoch 87/100] Loss_D: 0.2588, Loss_G: 4.2041\n",
            "[Epoch 88/100] Loss_D: 0.2083, Loss_G: 4.4970\n",
            "[Epoch 89/100] Loss_D: 0.2239, Loss_G: 4.4808\n",
            "[Epoch 90/100] Loss_D: 0.2637, Loss_G: 4.2784\n",
            "[Epoch 91/100] Loss_D: 0.1807, Loss_G: 4.5332\n",
            "[Epoch 92/100] Loss_D: 0.2038, Loss_G: 4.5421\n",
            "[Epoch 93/100] Loss_D: 0.2630, Loss_G: 4.5397\n",
            "[Epoch 94/100] Loss_D: 0.1914, Loss_G: 4.3228\n",
            "[Epoch 95/100] Loss_D: 0.2085, Loss_G: 4.3878\n",
            "[Epoch 96/100] Loss_D: 0.2010, Loss_G: 4.8558\n",
            "[Epoch 97/100] Loss_D: 0.1910, Loss_G: 4.8011\n",
            "[Epoch 98/100] Loss_D: 0.1632, Loss_G: 4.7605\n",
            "[Epoch 99/100] Loss_D: 0.2181, Loss_G: 4.7873\n",
            "[Epoch 100/100] Loss_D: 0.1807, Loss_G: 4.6416\n",
            "Training completed in 140 min 31 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load logs\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "# Plotting raw curves\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXic-yP4rXiQ",
        "outputId": "197d9056-d3f4-4165-e696-0f20fbb95cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E3_L1_DiscIn/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E3_L1_DiscIn/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics['frechet_inception_distance']\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3GyVW0-rXk1",
        "outputId": "05f0be12-3348-47d8-ace3-0c16272cfa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:52<00:00,  1.48it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 50.9267\n",
            "Inception Score: 3.3823 ± 0.0799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E3_L2_DiscIn"
      ],
      "metadata": {
        "id": "ECqNlSxYtMFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E3_L2_DiscIn: L2 Regularization on Discriminator Input\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E3_L2_DiscIn\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L2\",\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.001,\n",
        "    \"regularization_placement\": \"discriminator_input\"\n",
        "})"
      ],
      "metadata": {
        "id": "aRYydbxBrXnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "Cwm3YFLVrXpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fsxi0u5re4u",
        "outputId": "9c8efdd9-6671-47ef-f6c6-178a7db142fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E3_L2_DiscIn/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "wCEujUCjrfAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=config[\"latent_dim\"]).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized for E3_L2_DiscIn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YowFqIDjrfBp",
        "outputId": "96074169-abfc-461f-f7d0-fb7a389d8729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E3_L2_DiscIn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "FQwUbsNVtV3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWkVTX2PtV5r",
        "outputId": "6aca537f-0e04-463b-a6e4-e99a65ca3aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "\n",
        "        # L2 Regularization on Discriminator Input\n",
        "        # Penalize squared pixel values of fake images before they are fed to D\n",
        "        if config[\"regularization_type\"] == \"L2\" and config[\"regularization_placement\"] == \"discriminator_input\":\n",
        "            l2_fake = torch.mean(fake_imgs ** 2)\n",
        "            reg_term = config[\"regularization_lambda_L2\"] * l2_fake\n",
        "            loss_D += reg_term\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging losses\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save sample images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            fake_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(fake_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save running loss log\n",
        "    metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCuH8Oa7tV8L",
        "outputId": "747ada7a-a3c7-4d5e-ad41-75b0520886b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/100] Loss_D: 0.7043, Loss_G: 2.2199\n",
            "[Epoch 2/100] Loss_D: 0.9207, Loss_G: 2.3024\n",
            "[Epoch 3/100] Loss_D: 1.0192, Loss_G: 1.9114\n",
            "[Epoch 4/100] Loss_D: 1.0842, Loss_G: 1.7879\n",
            "[Epoch 5/100] Loss_D: 1.1633, Loss_G: 1.4726\n",
            "[Epoch 6/100] Loss_D: 1.1561, Loss_G: 1.3987\n",
            "[Epoch 7/100] Loss_D: 1.0906, Loss_G: 1.5176\n",
            "[Epoch 8/100] Loss_D: 1.0427, Loss_G: 1.6291\n",
            "[Epoch 9/100] Loss_D: 0.9841, Loss_G: 1.7477\n",
            "[Epoch 10/100] Loss_D: 1.0022, Loss_G: 1.7641\n",
            "[Epoch 11/100] Loss_D: 1.0381, Loss_G: 1.6289\n",
            "[Epoch 12/100] Loss_D: 1.0274, Loss_G: 1.6247\n",
            "[Epoch 13/100] Loss_D: 0.9812, Loss_G: 1.6530\n",
            "[Epoch 14/100] Loss_D: 0.9750, Loss_G: 1.7393\n",
            "[Epoch 15/100] Loss_D: 0.9458, Loss_G: 1.7311\n",
            "[Epoch 16/100] Loss_D: 0.9495, Loss_G: 1.7305\n",
            "[Epoch 17/100] Loss_D: 0.9340, Loss_G: 1.7635\n",
            "[Epoch 18/100] Loss_D: 0.9174, Loss_G: 1.8025\n",
            "[Epoch 19/100] Loss_D: 0.8906, Loss_G: 1.8438\n",
            "[Epoch 20/100] Loss_D: 0.8679, Loss_G: 1.9059\n",
            "[Epoch 21/100] Loss_D: 0.8450, Loss_G: 1.9522\n",
            "[Epoch 22/100] Loss_D: 0.8115, Loss_G: 1.9947\n",
            "[Epoch 23/100] Loss_D: 0.7904, Loss_G: 2.0094\n",
            "[Epoch 24/100] Loss_D: 0.7495, Loss_G: 2.0544\n",
            "[Epoch 25/100] Loss_D: 0.7711, Loss_G: 2.0696\n",
            "[Epoch 26/100] Loss_D: 0.7314, Loss_G: 2.1487\n",
            "[Epoch 27/100] Loss_D: 0.6926, Loss_G: 2.2168\n",
            "[Epoch 28/100] Loss_D: 0.6855, Loss_G: 2.2581\n",
            "[Epoch 29/100] Loss_D: 0.6677, Loss_G: 2.3803\n",
            "[Epoch 30/100] Loss_D: 0.6345, Loss_G: 2.3999\n",
            "[Epoch 31/100] Loss_D: 0.6352, Loss_G: 2.4543\n",
            "[Epoch 32/100] Loss_D: 0.5942, Loss_G: 2.5098\n",
            "[Epoch 33/100] Loss_D: 0.5629, Loss_G: 2.5347\n",
            "[Epoch 34/100] Loss_D: 0.5747, Loss_G: 2.6124\n",
            "[Epoch 35/100] Loss_D: 0.5560, Loss_G: 2.6929\n",
            "[Epoch 36/100] Loss_D: 0.5445, Loss_G: 2.5909\n",
            "[Epoch 37/100] Loss_D: 0.5075, Loss_G: 2.8525\n",
            "[Epoch 38/100] Loss_D: 0.5467, Loss_G: 2.8440\n",
            "[Epoch 39/100] Loss_D: 0.5038, Loss_G: 2.8142\n",
            "[Epoch 40/100] Loss_D: 0.4548, Loss_G: 2.9879\n",
            "[Epoch 41/100] Loss_D: 0.5055, Loss_G: 2.9609\n",
            "[Epoch 42/100] Loss_D: 0.4679, Loss_G: 2.9953\n",
            "[Epoch 43/100] Loss_D: 0.4344, Loss_G: 3.1932\n",
            "[Epoch 44/100] Loss_D: 0.4441, Loss_G: 3.1081\n",
            "[Epoch 45/100] Loss_D: 0.4290, Loss_G: 3.1438\n",
            "[Epoch 46/100] Loss_D: 0.4445, Loss_G: 3.2211\n",
            "[Epoch 47/100] Loss_D: 0.4174, Loss_G: 3.1939\n",
            "[Epoch 48/100] Loss_D: 0.4329, Loss_G: 3.3032\n",
            "[Epoch 49/100] Loss_D: 0.3934, Loss_G: 3.2742\n",
            "[Epoch 50/100] Loss_D: 0.3962, Loss_G: 3.3965\n",
            "[Epoch 51/100] Loss_D: 0.3693, Loss_G: 3.3763\n",
            "[Epoch 52/100] Loss_D: 0.4061, Loss_G: 3.4043\n",
            "[Epoch 53/100] Loss_D: 0.3759, Loss_G: 3.4748\n",
            "[Epoch 54/100] Loss_D: 0.3889, Loss_G: 3.5205\n",
            "[Epoch 55/100] Loss_D: 0.3547, Loss_G: 3.5738\n",
            "[Epoch 56/100] Loss_D: 0.3667, Loss_G: 3.5487\n",
            "[Epoch 57/100] Loss_D: 0.3259, Loss_G: 3.6709\n",
            "[Epoch 58/100] Loss_D: 0.3250, Loss_G: 3.5655\n",
            "[Epoch 59/100] Loss_D: 0.3627, Loss_G: 3.6813\n",
            "[Epoch 60/100] Loss_D: 0.3695, Loss_G: 3.6067\n",
            "[Epoch 61/100] Loss_D: 0.3177, Loss_G: 3.6172\n",
            "[Epoch 62/100] Loss_D: 0.3426, Loss_G: 3.8781\n",
            "[Epoch 63/100] Loss_D: 0.3253, Loss_G: 3.7023\n",
            "[Epoch 64/100] Loss_D: 0.3026, Loss_G: 3.9165\n",
            "[Epoch 65/100] Loss_D: 0.3013, Loss_G: 3.8538\n",
            "[Epoch 66/100] Loss_D: 0.2673, Loss_G: 3.8967\n",
            "[Epoch 67/100] Loss_D: 0.3207, Loss_G: 3.8668\n",
            "[Epoch 68/100] Loss_D: 0.3299, Loss_G: 3.9008\n",
            "[Epoch 69/100] Loss_D: 0.2863, Loss_G: 4.0286\n",
            "[Epoch 70/100] Loss_D: 0.2773, Loss_G: 3.9536\n",
            "[Epoch 71/100] Loss_D: 0.3052, Loss_G: 3.9774\n",
            "[Epoch 72/100] Loss_D: 0.2801, Loss_G: 3.8807\n",
            "[Epoch 73/100] Loss_D: 0.3161, Loss_G: 4.0219\n",
            "[Epoch 74/100] Loss_D: 0.2569, Loss_G: 4.0191\n",
            "[Epoch 75/100] Loss_D: 0.2435, Loss_G: 4.1686\n",
            "[Epoch 76/100] Loss_D: 0.2814, Loss_G: 4.3051\n",
            "[Epoch 77/100] Loss_D: 0.2687, Loss_G: 4.3074\n",
            "[Epoch 78/100] Loss_D: 0.2100, Loss_G: 4.2150\n",
            "[Epoch 79/100] Loss_D: 0.2832, Loss_G: 4.2651\n",
            "[Epoch 80/100] Loss_D: 0.2751, Loss_G: 4.0083\n",
            "[Epoch 81/100] Loss_D: 0.2786, Loss_G: 4.3236\n",
            "[Epoch 82/100] Loss_D: 0.2372, Loss_G: 4.2142\n",
            "[Epoch 83/100] Loss_D: 0.2925, Loss_G: 4.3756\n",
            "[Epoch 84/100] Loss_D: 0.2088, Loss_G: 4.0346\n",
            "[Epoch 85/100] Loss_D: 0.2234, Loss_G: 4.4065\n",
            "[Epoch 86/100] Loss_D: 0.2548, Loss_G: 4.3055\n",
            "[Epoch 87/100] Loss_D: 0.2229, Loss_G: 4.5066\n",
            "[Epoch 88/100] Loss_D: 0.2386, Loss_G: 4.5525\n",
            "[Epoch 89/100] Loss_D: 0.2339, Loss_G: 4.4076\n",
            "[Epoch 90/100] Loss_D: 0.2387, Loss_G: 4.7178\n",
            "[Epoch 91/100] Loss_D: 0.2129, Loss_G: 4.5237\n",
            "[Epoch 92/100] Loss_D: 0.2086, Loss_G: 4.5561\n",
            "[Epoch 93/100] Loss_D: 0.2204, Loss_G: 4.4632\n",
            "[Epoch 94/100] Loss_D: 0.2313, Loss_G: 4.6714\n",
            "[Epoch 95/100] Loss_D: 0.2329, Loss_G: 4.6552\n",
            "[Epoch 96/100] Loss_D: 0.1883, Loss_G: 4.6665\n",
            "[Epoch 97/100] Loss_D: 0.1647, Loss_G: 4.7088\n",
            "[Epoch 98/100] Loss_D: 0.2474, Loss_G: 4.9029\n",
            "[Epoch 99/100] Loss_D: 0.2663, Loss_G: 4.6834\n",
            "[Epoch 100/100] Loss_D: 0.1540, Loss_G: 4.7457\n",
            "Training completed in 137 min 14 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load logs\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xl7MHg1tV-g",
        "outputId": "010f93c5-5172-4d50-c20b-3bd27aaba02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E3_L2_DiscIn/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E3_L2_DiscIn/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQAcSCTHtWBA",
        "outputId": "20eb2de9-714c-4d01-8d8b-a41a1a285803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [01:05<00:00,  1.18it/s]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 186MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 173MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 49.5559\n",
            "Inception Score: 3.5425 ± 0.0933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E4_L1_GenW"
      ],
      "metadata": {
        "id": "k5Ifni_Itirb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E4_L1_GenW: L1 Regularization on Generator Weights\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E4_L1_GenW\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L1\",\n",
        "    \"regularization_lambda_L1\": 0.0001,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"generator_weights\"\n",
        "})"
      ],
      "metadata": {
        "id": "z600GFs1tWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "-rkDchkPtWF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dqeTKkorfCs",
        "outputId": "7111b90a-e123-4e9a-ee0a-f14bbe93569f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E4_L1_GenW/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "WmTS75wQtqKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"Generator and Discriminator initialized for E4_L1_GenW\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es4Vz5ZrtqIp",
        "outputId": "6468683b-b88c-426c-841c-91355d456753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E4_L1_GenW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "LnHBPYvXtqGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EvdrOGXtqDR",
        "outputId": "5f192255-554f-4e68-ac0b-064c89977450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "\n",
        "        # L1 Regularization on Generator Weights\n",
        "        # Penalize absolute values of ConvTranspose2d kernel weights in G\n",
        "        if config[\"regularization_type\"] == \"L1\" and config[\"regularization_placement\"] == \"generator_weights\":\n",
        "            l1_reg = torch.tensor(0., device=device)\n",
        "            for module in G.modules():\n",
        "                if isinstance(module, nn.ConvTranspose2d):\n",
        "                    l1_reg += torch.sum(torch.abs(module.weight))\n",
        "            loss_G += config[\"regularization_lambda_L1\"] * l1_reg\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save image\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save running loss log\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBmD-gJPtp_t",
        "outputId": "1efb9250-33d6-46df-813c-7ef87222cbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.9783, Loss_G: 3.1414\n",
            "[Epoch 2/100] Loss_D: 1.3982, Loss_G: 1.2256\n",
            "[Epoch 3/100] Loss_D: 1.3882, Loss_G: 0.9006\n",
            "[Epoch 4/100] Loss_D: 1.3873, Loss_G: 0.8013\n",
            "[Epoch 5/100] Loss_D: 1.3868, Loss_G: 0.7588\n",
            "[Epoch 6/100] Loss_D: 1.3870, Loss_G: 0.7388\n",
            "[Epoch 7/100] Loss_D: 1.3869, Loss_G: 0.7284\n",
            "[Epoch 8/100] Loss_D: 1.3868, Loss_G: 0.7229\n",
            "[Epoch 9/100] Loss_D: 1.3866, Loss_G: 0.7207\n",
            "[Epoch 10/100] Loss_D: 1.3865, Loss_G: 0.7202\n",
            "[Epoch 11/100] Loss_D: 1.3866, Loss_G: 0.7199\n",
            "[Epoch 12/100] Loss_D: 1.3865, Loss_G: 0.7195\n",
            "[Epoch 13/100] Loss_D: 1.3864, Loss_G: 0.7206\n",
            "[Epoch 14/100] Loss_D: 1.3864, Loss_G: 0.7206\n",
            "[Epoch 15/100] Loss_D: 1.3863, Loss_G: 0.7232\n",
            "[Epoch 16/100] Loss_D: 1.3864, Loss_G: 0.7243\n",
            "[Epoch 17/100] Loss_D: 1.3865, Loss_G: 0.7415\n",
            "[Epoch 18/100] Loss_D: 1.3866, Loss_G: 0.7197\n",
            "[Epoch 19/100] Loss_D: 1.3863, Loss_G: 0.7179\n",
            "[Epoch 20/100] Loss_D: 1.3865, Loss_G: 0.7495\n",
            "[Epoch 21/100] Loss_D: 1.3866, Loss_G: 0.7193\n",
            "[Epoch 22/100] Loss_D: 1.3863, Loss_G: 0.7176\n",
            "[Epoch 23/100] Loss_D: 1.3694, Loss_G: 0.9246\n",
            "[Epoch 24/100] Loss_D: 1.3146, Loss_G: 1.3927\n",
            "[Epoch 25/100] Loss_D: 1.2921, Loss_G: 1.6095\n",
            "[Epoch 26/100] Loss_D: 1.3703, Loss_G: 1.3378\n",
            "[Epoch 27/100] Loss_D: 1.3977, Loss_G: 1.0518\n",
            "[Epoch 28/100] Loss_D: 1.3941, Loss_G: 0.9163\n",
            "[Epoch 29/100] Loss_D: 1.3908, Loss_G: 0.8451\n",
            "[Epoch 30/100] Loss_D: 1.3889, Loss_G: 0.8103\n",
            "[Epoch 31/100] Loss_D: 1.3889, Loss_G: 0.7914\n",
            "[Epoch 32/100] Loss_D: 1.3883, Loss_G: 0.7752\n",
            "[Epoch 33/100] Loss_D: 1.3870, Loss_G: 0.7718\n",
            "[Epoch 34/100] Loss_D: 1.3867, Loss_G: 0.7731\n",
            "[Epoch 35/100] Loss_D: 1.3866, Loss_G: 0.7734\n",
            "[Epoch 36/100] Loss_D: 1.3869, Loss_G: 0.7858\n",
            "[Epoch 37/100] Loss_D: 1.3876, Loss_G: 0.7640\n",
            "[Epoch 38/100] Loss_D: 1.3863, Loss_G: 0.7672\n",
            "[Epoch 39/100] Loss_D: 1.3859, Loss_G: 0.7674\n",
            "[Epoch 40/100] Loss_D: 1.3845, Loss_G: 0.7818\n",
            "[Epoch 41/100] Loss_D: 1.3845, Loss_G: 0.7847\n",
            "[Epoch 42/100] Loss_D: 1.3842, Loss_G: 0.7935\n",
            "[Epoch 43/100] Loss_D: 1.3847, Loss_G: 0.7908\n",
            "[Epoch 44/100] Loss_D: 1.3830, Loss_G: 0.7991\n",
            "[Epoch 45/100] Loss_D: 1.3741, Loss_G: 0.8376\n",
            "[Epoch 46/100] Loss_D: 1.3715, Loss_G: 0.8813\n",
            "[Epoch 47/100] Loss_D: 1.3791, Loss_G: 0.8699\n",
            "[Epoch 48/100] Loss_D: 1.3886, Loss_G: 0.8349\n",
            "[Epoch 49/100] Loss_D: 1.3876, Loss_G: 0.8117\n",
            "[Epoch 50/100] Loss_D: 1.3826, Loss_G: 0.8171\n",
            "[Epoch 51/100] Loss_D: 1.3785, Loss_G: 0.8326\n",
            "[Epoch 52/100] Loss_D: 1.3706, Loss_G: 0.8591\n",
            "[Epoch 53/100] Loss_D: 1.3560, Loss_G: 0.9100\n",
            "[Epoch 54/100] Loss_D: 1.3341, Loss_G: 0.9812\n",
            "[Epoch 55/100] Loss_D: 1.3045, Loss_G: 1.0718\n",
            "[Epoch 56/100] Loss_D: 1.2774, Loss_G: 1.1660\n",
            "[Epoch 57/100] Loss_D: 1.2539, Loss_G: 1.2470\n",
            "[Epoch 58/100] Loss_D: 1.2112, Loss_G: 1.3234\n",
            "[Epoch 59/100] Loss_D: 1.1625, Loss_G: 1.4525\n",
            "[Epoch 60/100] Loss_D: 1.1116, Loss_G: 1.5554\n",
            "[Epoch 61/100] Loss_D: 1.0559, Loss_G: 1.6608\n",
            "[Epoch 62/100] Loss_D: 1.0103, Loss_G: 1.7779\n",
            "[Epoch 63/100] Loss_D: 0.9637, Loss_G: 1.8876\n",
            "[Epoch 64/100] Loss_D: 0.8967, Loss_G: 1.9921\n",
            "[Epoch 65/100] Loss_D: 0.8558, Loss_G: 2.1227\n",
            "[Epoch 66/100] Loss_D: 0.8058, Loss_G: 2.2305\n",
            "[Epoch 67/100] Loss_D: 0.7617, Loss_G: 2.3678\n",
            "[Epoch 68/100] Loss_D: 0.7384, Loss_G: 2.4965\n",
            "[Epoch 69/100] Loss_D: 0.6840, Loss_G: 2.5670\n",
            "[Epoch 70/100] Loss_D: 0.6397, Loss_G: 2.6853\n",
            "[Epoch 71/100] Loss_D: 0.6390, Loss_G: 2.7457\n",
            "[Epoch 72/100] Loss_D: 0.6101, Loss_G: 2.8649\n",
            "[Epoch 73/100] Loss_D: 0.5795, Loss_G: 2.9759\n",
            "[Epoch 74/100] Loss_D: 0.5386, Loss_G: 3.1363\n",
            "[Epoch 75/100] Loss_D: 0.5494, Loss_G: 3.1377\n",
            "[Epoch 76/100] Loss_D: 0.5046, Loss_G: 3.2717\n",
            "[Epoch 77/100] Loss_D: 0.4847, Loss_G: 3.3654\n",
            "[Epoch 78/100] Loss_D: 0.4746, Loss_G: 3.4252\n",
            "[Epoch 79/100] Loss_D: 0.4357, Loss_G: 3.5783\n",
            "[Epoch 80/100] Loss_D: 0.4149, Loss_G: 3.6947\n",
            "[Epoch 81/100] Loss_D: 0.4261, Loss_G: 3.7440\n",
            "[Epoch 82/100] Loss_D: 0.3999, Loss_G: 3.7715\n",
            "[Epoch 83/100] Loss_D: 0.4058, Loss_G: 3.8514\n",
            "[Epoch 84/100] Loss_D: 0.3802, Loss_G: 3.8843\n",
            "[Epoch 85/100] Loss_D: 0.3653, Loss_G: 4.0086\n",
            "[Epoch 86/100] Loss_D: 0.3626, Loss_G: 4.0920\n",
            "[Epoch 87/100] Loss_D: 0.3799, Loss_G: 4.1947\n",
            "[Epoch 88/100] Loss_D: 0.3334, Loss_G: 4.2268\n",
            "[Epoch 89/100] Loss_D: 0.3432, Loss_G: 4.2761\n",
            "[Epoch 90/100] Loss_D: 0.3148, Loss_G: 4.3933\n",
            "[Epoch 91/100] Loss_D: 0.3278, Loss_G: 4.3896\n",
            "[Epoch 92/100] Loss_D: 0.3023, Loss_G: 4.5446\n",
            "[Epoch 93/100] Loss_D: 0.2759, Loss_G: 4.5683\n",
            "[Epoch 94/100] Loss_D: 0.3305, Loss_G: 4.5436\n",
            "[Epoch 95/100] Loss_D: 0.2797, Loss_G: 4.6935\n",
            "[Epoch 96/100] Loss_D: 0.2756, Loss_G: 4.7287\n",
            "[Epoch 97/100] Loss_D: 0.2778, Loss_G: 4.8314\n",
            "[Epoch 98/100] Loss_D: 0.2917, Loss_G: 4.8510\n",
            "[Epoch 99/100] Loss_D: 0.2596, Loss_G: 4.8665\n",
            "[Epoch 100/100] Loss_D: 0.2509, Loss_G: 4.8972\n",
            "Training completed in 137 min 30 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load logs\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KmYUVcutp65",
        "outputId": "e2ff6f29-51f6-446c-ddd0-04a46ac43237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E4_L1_GenW/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E4_L1_GenW/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith(\".png\")]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits):(k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYXJwzu2tp3J",
        "outputId": "6ae512ad-8085-49d3-c135-55343a3d21b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [01:09<00:00,  1.13it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 54.2408\n",
            "Inception Score: 3.5390 ± 0.1078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E4_L2_GenW"
      ],
      "metadata": {
        "id": "R4jnqJJvuIjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E4_L2_GenW: L2 Regularization on Generator Weights\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E4_L2_GenW\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L2\",\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.001,\n",
        "    \"regularization_placement\": \"generator_weights\",\n",
        "})"
      ],
      "metadata": {
        "id": "-VGjwc0ttp08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "wkVyuccitpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_l8q6V1tput",
        "outputId": "ac4ec6d6-2dcf-4036-b72c-f433ee6c3018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E4_L2_GenW/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "l5R5PvW7tprT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(f\"Generator and Discriminator initialized for {experiment_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osWAnSxFtpoZ",
        "outputId": "5dfd5c16-9e82-476a-d715-dbf8415fbbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E4_L2_GenW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "JahVHvkTtplv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gGT6FMRtpja",
        "outputId": "7e5801fa-1762-47e3-b9a3-6ed443e3d1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "\n",
        "        # Add L2 regularization on generator weights\n",
        "        # Penalize squared values of ConvTranspose2d kernel weights in G\n",
        "        if config[\"regularization_type\"] == \"L2\" and config[\"regularization_placement\"] == \"generator_weights\":\n",
        "            l2_reg = torch.tensor(0., device=device)\n",
        "            for module in G.modules():\n",
        "                if isinstance(module, nn.ConvTranspose2d):\n",
        "                    l2_reg += torch.sum(module.weight ** 2)\n",
        "            loss_G += config[\"regularization_lambda_L2\"] * l2_reg\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging losses\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save sample images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save running loss log\n",
        "    metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save training time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CVvLLXBtpg4",
        "outputId": "7309a286-28ee-4f9f-f209-8987a41faec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.8319, Loss_G: 2.4628\n",
            "[Epoch 2/100] Loss_D: 1.2141, Loss_G: 1.8363\n",
            "[Epoch 3/100] Loss_D: 1.2362, Loss_G: 1.6979\n",
            "[Epoch 4/100] Loss_D: 1.2732, Loss_G: 1.6217\n",
            "[Epoch 5/100] Loss_D: 1.3254, Loss_G: 1.3639\n",
            "[Epoch 6/100] Loss_D: 1.3751, Loss_G: 1.0811\n",
            "[Epoch 7/100] Loss_D: 1.3762, Loss_G: 0.9881\n",
            "[Epoch 8/100] Loss_D: 1.3807, Loss_G: 0.9305\n",
            "[Epoch 9/100] Loss_D: 1.3854, Loss_G: 0.8805\n",
            "[Epoch 10/100] Loss_D: 1.3877, Loss_G: 0.8343\n",
            "[Epoch 11/100] Loss_D: 1.3882, Loss_G: 0.7971\n",
            "[Epoch 12/100] Loss_D: 1.3880, Loss_G: 0.7680\n",
            "[Epoch 13/100] Loss_D: 1.3877, Loss_G: 0.7466\n",
            "[Epoch 14/100] Loss_D: 1.3874, Loss_G: 0.7311\n",
            "[Epoch 15/100] Loss_D: 1.3872, Loss_G: 0.7206\n",
            "[Epoch 16/100] Loss_D: 1.3869, Loss_G: 0.7156\n",
            "[Epoch 17/100] Loss_D: 1.3875, Loss_G: 0.7151\n",
            "[Epoch 18/100] Loss_D: 1.3874, Loss_G: 0.7106\n",
            "[Epoch 19/100] Loss_D: 1.3872, Loss_G: 0.7088\n",
            "[Epoch 20/100] Loss_D: 1.3870, Loss_G: 0.7062\n",
            "[Epoch 21/100] Loss_D: 1.3869, Loss_G: 0.7071\n",
            "[Epoch 22/100] Loss_D: 1.3869, Loss_G: 0.7109\n",
            "[Epoch 23/100] Loss_D: 1.3869, Loss_G: 0.7055\n",
            "[Epoch 24/100] Loss_D: 1.3863, Loss_G: 0.7065\n",
            "[Epoch 25/100] Loss_D: 1.2913, Loss_G: 1.2596\n",
            "[Epoch 26/100] Loss_D: 1.2618, Loss_G: 1.5375\n",
            "[Epoch 27/100] Loss_D: 1.2233, Loss_G: 1.6526\n",
            "[Epoch 28/100] Loss_D: 1.2759, Loss_G: 1.4254\n",
            "[Epoch 29/100] Loss_D: 1.2929, Loss_G: 1.3427\n",
            "[Epoch 30/100] Loss_D: 1.2701, Loss_G: 1.3405\n",
            "[Epoch 31/100] Loss_D: 1.2823, Loss_G: 1.2905\n",
            "[Epoch 32/100] Loss_D: 1.2809, Loss_G: 1.3089\n",
            "[Epoch 33/100] Loss_D: 1.2903, Loss_G: 1.2599\n",
            "[Epoch 34/100] Loss_D: 1.2894, Loss_G: 1.2423\n",
            "[Epoch 35/100] Loss_D: 1.3005, Loss_G: 1.2092\n",
            "[Epoch 36/100] Loss_D: 1.2947, Loss_G: 1.1936\n",
            "[Epoch 37/100] Loss_D: 1.2963, Loss_G: 1.1886\n",
            "[Epoch 38/100] Loss_D: 1.2957, Loss_G: 1.1902\n",
            "[Epoch 39/100] Loss_D: 1.2881, Loss_G: 1.2055\n",
            "[Epoch 40/100] Loss_D: 1.2852, Loss_G: 1.2286\n",
            "[Epoch 41/100] Loss_D: 1.2654, Loss_G: 1.2323\n",
            "[Epoch 42/100] Loss_D: 1.2486, Loss_G: 1.2655\n",
            "[Epoch 43/100] Loss_D: 1.2425, Loss_G: 1.3056\n",
            "[Epoch 44/100] Loss_D: 1.2196, Loss_G: 1.3305\n",
            "[Epoch 45/100] Loss_D: 1.1961, Loss_G: 1.3669\n",
            "[Epoch 46/100] Loss_D: 1.1857, Loss_G: 1.4001\n",
            "[Epoch 47/100] Loss_D: 1.1595, Loss_G: 1.4499\n",
            "[Epoch 48/100] Loss_D: 1.1191, Loss_G: 1.5220\n",
            "[Epoch 49/100] Loss_D: 1.1013, Loss_G: 1.5369\n",
            "[Epoch 50/100] Loss_D: 1.0575, Loss_G: 1.6100\n",
            "[Epoch 51/100] Loss_D: 1.0353, Loss_G: 1.6538\n",
            "[Epoch 52/100] Loss_D: 1.0067, Loss_G: 1.7351\n",
            "[Epoch 53/100] Loss_D: 0.9759, Loss_G: 1.7813\n",
            "[Epoch 54/100] Loss_D: 0.9577, Loss_G: 1.8468\n",
            "[Epoch 55/100] Loss_D: 0.9245, Loss_G: 1.8822\n",
            "[Epoch 56/100] Loss_D: 0.9040, Loss_G: 1.9563\n",
            "[Epoch 57/100] Loss_D: 0.8735, Loss_G: 2.0628\n",
            "[Epoch 58/100] Loss_D: 0.8433, Loss_G: 2.1172\n",
            "[Epoch 59/100] Loss_D: 0.8067, Loss_G: 2.1760\n",
            "[Epoch 60/100] Loss_D: 0.7785, Loss_G: 2.2760\n",
            "[Epoch 61/100] Loss_D: 0.7751, Loss_G: 2.3651\n",
            "[Epoch 62/100] Loss_D: 0.7238, Loss_G: 2.4046\n",
            "[Epoch 63/100] Loss_D: 0.7121, Loss_G: 2.4836\n",
            "[Epoch 64/100] Loss_D: 0.6832, Loss_G: 2.5685\n",
            "[Epoch 65/100] Loss_D: 0.6564, Loss_G: 2.6406\n",
            "[Epoch 66/100] Loss_D: 0.6207, Loss_G: 2.6873\n",
            "[Epoch 67/100] Loss_D: 0.6250, Loss_G: 2.7870\n",
            "[Epoch 68/100] Loss_D: 0.5694, Loss_G: 2.8946\n",
            "[Epoch 69/100] Loss_D: 0.5576, Loss_G: 2.9453\n",
            "[Epoch 70/100] Loss_D: 0.5472, Loss_G: 3.0594\n",
            "[Epoch 71/100] Loss_D: 0.5086, Loss_G: 3.1395\n",
            "[Epoch 72/100] Loss_D: 0.4851, Loss_G: 3.2414\n",
            "[Epoch 73/100] Loss_D: 0.5122, Loss_G: 3.3445\n",
            "[Epoch 74/100] Loss_D: 0.4685, Loss_G: 3.3767\n",
            "[Epoch 75/100] Loss_D: 0.4509, Loss_G: 3.4560\n",
            "[Epoch 76/100] Loss_D: 0.4440, Loss_G: 3.5088\n",
            "[Epoch 77/100] Loss_D: 0.4247, Loss_G: 3.5781\n",
            "[Epoch 78/100] Loss_D: 0.3964, Loss_G: 3.6856\n",
            "[Epoch 79/100] Loss_D: 0.4153, Loss_G: 3.7596\n",
            "[Epoch 80/100] Loss_D: 0.3704, Loss_G: 3.8238\n",
            "[Epoch 81/100] Loss_D: 0.3678, Loss_G: 3.9282\n",
            "[Epoch 82/100] Loss_D: 0.3633, Loss_G: 3.9557\n",
            "[Epoch 83/100] Loss_D: 0.3838, Loss_G: 4.0035\n",
            "[Epoch 84/100] Loss_D: 0.3286, Loss_G: 4.0485\n",
            "[Epoch 85/100] Loss_D: 0.3288, Loss_G: 4.2780\n",
            "[Epoch 86/100] Loss_D: 0.3360, Loss_G: 4.1994\n",
            "[Epoch 87/100] Loss_D: 0.3132, Loss_G: 4.3477\n",
            "[Epoch 88/100] Loss_D: 0.3104, Loss_G: 4.4342\n",
            "[Epoch 89/100] Loss_D: 0.3046, Loss_G: 4.4970\n",
            "[Epoch 90/100] Loss_D: 0.2644, Loss_G: 4.5477\n",
            "[Epoch 91/100] Loss_D: 0.3094, Loss_G: 4.5059\n",
            "[Epoch 92/100] Loss_D: 0.2819, Loss_G: 4.6644\n",
            "[Epoch 93/100] Loss_D: 0.2747, Loss_G: 4.7184\n",
            "[Epoch 94/100] Loss_D: 0.3060, Loss_G: 4.6329\n",
            "[Epoch 95/100] Loss_D: 0.2349, Loss_G: 4.8971\n",
            "[Epoch 96/100] Loss_D: 0.2720, Loss_G: 4.8718\n",
            "[Epoch 97/100] Loss_D: 0.2298, Loss_G: 5.0903\n",
            "[Epoch 98/100] Loss_D: 0.2676, Loss_G: 4.9511\n",
            "[Epoch 99/100] Loss_D: 0.2523, Loss_G: 4.9849\n",
            "[Epoch 100/100] Loss_D: 0.2540, Loss_G: 4.9669\n",
            "Training completed in 132 min 28 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load logs\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fiq75ppdtpeR",
        "outputId": "80541d16-b93c-45f5-b92d-127c05fcf595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E4_L2_GenW/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E4_L2_GenW/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Save images, calculate FID & Inception Score\n",
        "\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics['frechet_inception_distance']\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGaEJSuXtpb3",
        "outputId": "ca7c4de3-0020-45e9-fd09-27894dee96e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:54<00:00,  1.42it/s]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 263MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 256MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 49.5931\n",
            "Inception Score: 3.4440 ± 0.0575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E5_L1_DiscW"
      ],
      "metadata": {
        "id": "zra1o9OEueXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E5_L1_DiscW: L1 Regularization on Discriminator Weights\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E5_L1_DiscW\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L1\",\n",
        "    \"regularization_lambda_L1\": 0.0001,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"discriminator_weights_early\",\n",
        "})"
      ],
      "metadata": {
        "id": "qNS6ICFLtpY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "liwlBO8StpWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6feZTLQitpUC",
        "outputId": "cdfa4a9f-d3e0-41d3-dabf-71697110608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E5_L1_DiscW/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "3sRP29wftpRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(f\"Generator and Discriminator initialized for {experiment_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTZohBs6tpPY",
        "outputId": "0cd9ff11-8ee4-4f0a-d418-f620d52f2a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E5_L1_DiscW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "YHHy2k1ZtpNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPv3NqL1tpKn",
        "outputId": "508dee11-040d-4ff8-f872-5f896d7e12be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "\n",
        "        # Add L1 regularization on early discriminator conv layers\n",
        "        # Penalize absolute values of weights in the first two Conv2d layers of D\n",
        "        if config[\"regularization_type\"] == \"L1\" and config[\"regularization_placement\"] == \"discriminator_weights_early\":\n",
        "            l1_reg = torch.tensor(0., device=device)\n",
        "            conv_count = 0\n",
        "            for module in D.modules():\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    l1_reg += torch.sum(torch.abs(module.weight))\n",
        "                    conv_count += 1\n",
        "                    if conv_count == 2:\n",
        "                        break\n",
        "            loss_D += config[\"regularization_lambda_L1\"] * l1_reg\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save image\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            fake_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(fake_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save loss CSV\n",
        "    metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Training Time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z6Po8CLtpIS",
        "outputId": "17041a51-b68d-4e4e-cf8b-019cc21e7c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.7723, Loss_G: 2.3897\n",
            "[Epoch 2/100] Loss_D: 1.0435, Loss_G: 2.4706\n",
            "[Epoch 3/100] Loss_D: 1.1534, Loss_G: 1.9697\n",
            "[Epoch 4/100] Loss_D: 1.1888, Loss_G: 1.9638\n",
            "[Epoch 5/100] Loss_D: 1.2420, Loss_G: 1.7169\n",
            "[Epoch 6/100] Loss_D: 1.2945, Loss_G: 1.5254\n",
            "[Epoch 7/100] Loss_D: 1.2809, Loss_G: 1.4888\n",
            "[Epoch 8/100] Loss_D: 1.2293, Loss_G: 1.6433\n",
            "[Epoch 9/100] Loss_D: 1.1880, Loss_G: 1.6641\n",
            "[Epoch 10/100] Loss_D: 1.1429, Loss_G: 1.7930\n",
            "[Epoch 11/100] Loss_D: 1.1497, Loss_G: 1.8602\n",
            "[Epoch 12/100] Loss_D: 1.1317, Loss_G: 1.7890\n",
            "[Epoch 13/100] Loss_D: 1.1330, Loss_G: 1.8340\n",
            "[Epoch 14/100] Loss_D: 1.1021, Loss_G: 1.8173\n",
            "[Epoch 15/100] Loss_D: 1.1093, Loss_G: 1.8425\n",
            "[Epoch 16/100] Loss_D: 1.0805, Loss_G: 1.8883\n",
            "[Epoch 17/100] Loss_D: 1.0576, Loss_G: 1.9824\n",
            "[Epoch 18/100] Loss_D: 1.0037, Loss_G: 2.0467\n",
            "[Epoch 19/100] Loss_D: 1.0086, Loss_G: 2.0495\n",
            "[Epoch 20/100] Loss_D: 1.0121, Loss_G: 2.1041\n",
            "[Epoch 21/100] Loss_D: 0.9671, Loss_G: 2.2257\n",
            "[Epoch 22/100] Loss_D: 0.9218, Loss_G: 2.1713\n",
            "[Epoch 23/100] Loss_D: 0.9194, Loss_G: 2.1996\n",
            "[Epoch 24/100] Loss_D: 0.8961, Loss_G: 2.3022\n",
            "[Epoch 25/100] Loss_D: 0.9062, Loss_G: 2.3734\n",
            "[Epoch 26/100] Loss_D: 0.8745, Loss_G: 2.3571\n",
            "[Epoch 27/100] Loss_D: 0.8339, Loss_G: 2.4633\n",
            "[Epoch 28/100] Loss_D: 0.8458, Loss_G: 2.5163\n",
            "[Epoch 29/100] Loss_D: 0.8284, Loss_G: 2.5689\n",
            "[Epoch 30/100] Loss_D: 0.7827, Loss_G: 2.5866\n",
            "[Epoch 31/100] Loss_D: 0.7893, Loss_G: 2.5574\n",
            "[Epoch 32/100] Loss_D: 0.7927, Loss_G: 2.7204\n",
            "[Epoch 33/100] Loss_D: 0.7691, Loss_G: 2.6945\n",
            "[Epoch 34/100] Loss_D: 0.7192, Loss_G: 2.7839\n",
            "[Epoch 35/100] Loss_D: 0.7354, Loss_G: 2.7677\n",
            "[Epoch 36/100] Loss_D: 0.7385, Loss_G: 2.8299\n",
            "[Epoch 37/100] Loss_D: 0.7070, Loss_G: 2.9815\n",
            "[Epoch 38/100] Loss_D: 0.7065, Loss_G: 2.9577\n",
            "[Epoch 39/100] Loss_D: 0.6820, Loss_G: 2.9848\n",
            "[Epoch 40/100] Loss_D: 0.6799, Loss_G: 2.9865\n",
            "[Epoch 41/100] Loss_D: 0.6831, Loss_G: 3.0987\n",
            "[Epoch 42/100] Loss_D: 0.6517, Loss_G: 3.0905\n",
            "[Epoch 43/100] Loss_D: 0.7054, Loss_G: 3.0959\n",
            "[Epoch 44/100] Loss_D: 0.6630, Loss_G: 3.1776\n",
            "[Epoch 45/100] Loss_D: 0.6322, Loss_G: 3.2782\n",
            "[Epoch 46/100] Loss_D: 0.6540, Loss_G: 3.2477\n",
            "[Epoch 47/100] Loss_D: 0.6220, Loss_G: 3.3376\n",
            "[Epoch 48/100] Loss_D: 0.6508, Loss_G: 3.2498\n",
            "[Epoch 49/100] Loss_D: 0.5820, Loss_G: 3.4298\n",
            "[Epoch 50/100] Loss_D: 0.5970, Loss_G: 3.4055\n",
            "[Epoch 51/100] Loss_D: 0.6184, Loss_G: 3.3536\n",
            "[Epoch 52/100] Loss_D: 0.5945, Loss_G: 3.4758\n",
            "[Epoch 53/100] Loss_D: 0.5414, Loss_G: 3.6154\n",
            "[Epoch 54/100] Loss_D: 0.5828, Loss_G: 3.5833\n",
            "[Epoch 55/100] Loss_D: 0.5993, Loss_G: 3.6210\n",
            "[Epoch 56/100] Loss_D: 0.5599, Loss_G: 3.6306\n",
            "[Epoch 57/100] Loss_D: 0.5789, Loss_G: 3.5604\n",
            "[Epoch 58/100] Loss_D: 0.5878, Loss_G: 3.7185\n",
            "[Epoch 59/100] Loss_D: 0.4666, Loss_G: 3.6414\n",
            "[Epoch 60/100] Loss_D: 0.6059, Loss_G: 3.6882\n",
            "[Epoch 61/100] Loss_D: 0.5629, Loss_G: 3.7107\n",
            "[Epoch 62/100] Loss_D: 0.5057, Loss_G: 3.8445\n",
            "[Epoch 63/100] Loss_D: 0.5439, Loss_G: 3.7816\n",
            "[Epoch 64/100] Loss_D: 0.5625, Loss_G: 3.7601\n",
            "[Epoch 65/100] Loss_D: 0.5671, Loss_G: 3.6651\n",
            "[Epoch 66/100] Loss_D: 0.4947, Loss_G: 3.8912\n",
            "[Epoch 67/100] Loss_D: 0.5492, Loss_G: 3.8968\n",
            "[Epoch 68/100] Loss_D: 0.5010, Loss_G: 3.8143\n",
            "[Epoch 69/100] Loss_D: 0.5363, Loss_G: 3.9080\n",
            "[Epoch 70/100] Loss_D: 0.5540, Loss_G: 3.8719\n",
            "[Epoch 71/100] Loss_D: 0.4993, Loss_G: 4.0388\n",
            "[Epoch 72/100] Loss_D: 0.4422, Loss_G: 4.0675\n",
            "[Epoch 73/100] Loss_D: 0.5423, Loss_G: 3.9468\n",
            "[Epoch 74/100] Loss_D: 0.5200, Loss_G: 4.0078\n",
            "[Epoch 75/100] Loss_D: 0.4973, Loss_G: 4.0885\n",
            "[Epoch 76/100] Loss_D: 0.4630, Loss_G: 4.0525\n",
            "[Epoch 77/100] Loss_D: 0.5554, Loss_G: 4.1804\n",
            "[Epoch 78/100] Loss_D: 0.4620, Loss_G: 4.0145\n",
            "[Epoch 79/100] Loss_D: 0.4640, Loss_G: 4.1525\n",
            "[Epoch 80/100] Loss_D: 0.4614, Loss_G: 4.4291\n",
            "[Epoch 81/100] Loss_D: 0.4829, Loss_G: 4.3471\n",
            "[Epoch 82/100] Loss_D: 0.5064, Loss_G: 4.2587\n",
            "[Epoch 83/100] Loss_D: 0.4632, Loss_G: 4.2342\n",
            "[Epoch 84/100] Loss_D: 0.4484, Loss_G: 4.5170\n",
            "[Epoch 85/100] Loss_D: 0.5059, Loss_G: 4.1942\n",
            "[Epoch 86/100] Loss_D: 0.4666, Loss_G: 4.2264\n",
            "[Epoch 87/100] Loss_D: 0.4217, Loss_G: 4.3468\n",
            "[Epoch 88/100] Loss_D: 0.4988, Loss_G: 4.4243\n",
            "[Epoch 89/100] Loss_D: 0.4291, Loss_G: 4.3719\n",
            "[Epoch 90/100] Loss_D: 0.4456, Loss_G: 4.3641\n",
            "[Epoch 91/100] Loss_D: 0.4890, Loss_G: 4.4877\n",
            "[Epoch 92/100] Loss_D: 0.4438, Loss_G: 4.4877\n",
            "[Epoch 93/100] Loss_D: 0.4526, Loss_G: 4.3763\n",
            "[Epoch 94/100] Loss_D: 0.4830, Loss_G: 4.5545\n",
            "[Epoch 95/100] Loss_D: 0.4563, Loss_G: 4.5383\n",
            "[Epoch 96/100] Loss_D: 0.3856, Loss_G: 4.5120\n",
            "[Epoch 97/100] Loss_D: 0.4294, Loss_G: 4.7210\n",
            "[Epoch 98/100] Loss_D: 0.4611, Loss_G: 4.3861\n",
            "[Epoch 99/100] Loss_D: 0.4625, Loss_G: 4.4219\n",
            "[Epoch 100/100] Loss_D: 0.4629, Loss_G: 4.3795\n",
            "Training completed in 133 min 29 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4qHagbptpGF",
        "outputId": "78e2e2e4-6f3c-4504-cf70-93b182ee6fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E5_L1_DiscW/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E5_L1_DiscW/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: FID and Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval_data folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
        "    ])\n",
        "    eval_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images: return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID Calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics['frechet_inception_distance']\n",
        "\n",
        "# IS Calculation\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gP2W621tpDm",
        "outputId": "535072c0-550d-4a7e-df26-acd774424083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:56<00:00,  1.39it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 44.5129\n",
            "Inception Score: 3.4408 ± 0.0959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E5_L2_DiscW"
      ],
      "metadata": {
        "id": "UOIptN4QuyCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E5_L2_DiscW: L2 Regularization on Discriminator Weights\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E5_L2_DiscW\"\n",
        "\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L2\",\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.001,\n",
        "    \"regularization_placement\": \"discriminator_weights_late\",\n",
        "})"
      ],
      "metadata": {
        "id": "8ySJE3m2tpBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "l1aLx8BIto_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbDtgl-Lto84",
        "outputId": "65329c6e-36a3-4d66-ce4c-9b6fa8c18383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E5_L2_DiscW/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise\n",
        "num_fixed_samples = 64\n",
        "latent_dim = config[\"latent_dim\"]\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(num_fixed_samples, latent_dim, 1, 1, device=device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))"
      ],
      "metadata": {
        "id": "xAKiyR45to6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(f\"Generator and Discriminator initialized for {experiment_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm__hJtlto4n",
        "outputId": "2f008d17-30e7-43fb-da4e-08402d3f125a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E5_L2_DiscW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "oIOTym_8to2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    G.load_state_dict(checkpoint[\"G_state_dict\"])\n",
        "    D.load_state_dict(checkpoint[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuIKK4HKtozw",
        "outputId": "7d4e3c78-3f44-4a44-9bec-bb196b723915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load previous loss logs (for continuation in Colab runtime)\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(batch_size, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        output_real = D(real_imgs).view(-1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise).detach()\n",
        "        output_fake = D(fake_imgs).view(-1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "\n",
        "        # Add L2 regularization on late discriminator convolutional layers\n",
        "        # Penalize squared weights of the last two Conv2d layers in D\n",
        "        if config[\"regularization_type\"] == \"L2\" and config[\"regularization_placement\"] == \"discriminator_weights_late\":\n",
        "            l2_reg = torch.tensor(0., device=device)\n",
        "            conv_modules = [m for m in D.modules() if isinstance(m, nn.Conv2d)]\n",
        "            for module in conv_modules[-2:]:\n",
        "                l2_reg += torch.sum(module.weight ** 2)\n",
        "            loss_D += config[\"regularization_lambda_L2\"] * l2_reg\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_imgs = G(noise)\n",
        "        output = D(fake_imgs).view(-1)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save images and checkpoints\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            vutils.save_image(sample_imgs, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Training Time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlLgQq3Ctox4",
        "outputId": "a5c27e3f-868f-4a5a-ee4b-f3d7a4a6baec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 0.9503, Loss_G: 2.1478\n",
            "[Epoch 2/100] Loss_D: 0.9845, Loss_G: 2.4480\n",
            "[Epoch 3/100] Loss_D: 1.1510, Loss_G: 1.9117\n",
            "[Epoch 4/100] Loss_D: 1.1120, Loss_G: 1.9066\n",
            "[Epoch 5/100] Loss_D: 1.1603, Loss_G: 1.7017\n",
            "[Epoch 6/100] Loss_D: 1.1582, Loss_G: 1.5697\n",
            "[Epoch 7/100] Loss_D: 1.1679, Loss_G: 1.5574\n",
            "[Epoch 8/100] Loss_D: 1.1659, Loss_G: 1.5979\n",
            "[Epoch 9/100] Loss_D: 1.1148, Loss_G: 1.6681\n",
            "[Epoch 10/100] Loss_D: 1.0869, Loss_G: 1.7194\n",
            "[Epoch 11/100] Loss_D: 1.0472, Loss_G: 1.8457\n",
            "[Epoch 12/100] Loss_D: 1.0542, Loss_G: 1.7787\n",
            "[Epoch 13/100] Loss_D: 1.0428, Loss_G: 1.8123\n",
            "[Epoch 14/100] Loss_D: 1.0187, Loss_G: 1.8573\n",
            "[Epoch 15/100] Loss_D: 1.0058, Loss_G: 1.8834\n",
            "[Epoch 16/100] Loss_D: 0.9985, Loss_G: 1.9152\n",
            "[Epoch 17/100] Loss_D: 0.9709, Loss_G: 1.9501\n",
            "[Epoch 18/100] Loss_D: 0.9411, Loss_G: 2.0212\n",
            "[Epoch 19/100] Loss_D: 0.9147, Loss_G: 2.0977\n",
            "[Epoch 20/100] Loss_D: 0.9246, Loss_G: 2.1472\n",
            "[Epoch 21/100] Loss_D: 0.8675, Loss_G: 2.1746\n",
            "[Epoch 22/100] Loss_D: 0.8642, Loss_G: 2.2838\n",
            "[Epoch 23/100] Loss_D: 0.8335, Loss_G: 2.3627\n",
            "[Epoch 24/100] Loss_D: 0.8516, Loss_G: 2.3678\n",
            "[Epoch 25/100] Loss_D: 0.8111, Loss_G: 2.4938\n",
            "[Epoch 26/100] Loss_D: 0.8171, Loss_G: 2.4653\n",
            "[Epoch 27/100] Loss_D: 0.7872, Loss_G: 2.5165\n",
            "[Epoch 28/100] Loss_D: 0.7836, Loss_G: 2.5710\n",
            "[Epoch 29/100] Loss_D: 0.7661, Loss_G: 2.6619\n",
            "[Epoch 30/100] Loss_D: 0.7237, Loss_G: 2.6419\n",
            "[Epoch 31/100] Loss_D: 0.7331, Loss_G: 2.7791\n",
            "[Epoch 32/100] Loss_D: 0.7181, Loss_G: 2.7949\n",
            "[Epoch 33/100] Loss_D: 0.7307, Loss_G: 2.8660\n",
            "[Epoch 34/100] Loss_D: 0.7198, Loss_G: 2.8892\n",
            "[Epoch 35/100] Loss_D: 0.6816, Loss_G: 2.9423\n",
            "[Epoch 36/100] Loss_D: 0.7117, Loss_G: 2.9724\n",
            "[Epoch 37/100] Loss_D: 0.6609, Loss_G: 3.0622\n",
            "[Epoch 38/100] Loss_D: 0.6767, Loss_G: 3.0527\n",
            "[Epoch 39/100] Loss_D: 0.6828, Loss_G: 3.1327\n",
            "[Epoch 40/100] Loss_D: 0.6459, Loss_G: 3.1690\n",
            "[Epoch 41/100] Loss_D: 0.6219, Loss_G: 3.2399\n",
            "[Epoch 42/100] Loss_D: 0.6352, Loss_G: 3.3180\n",
            "[Epoch 43/100] Loss_D: 0.6579, Loss_G: 3.2952\n",
            "[Epoch 44/100] Loss_D: 0.6114, Loss_G: 3.3577\n",
            "[Epoch 45/100] Loss_D: 0.6148, Loss_G: 3.3943\n",
            "[Epoch 46/100] Loss_D: 0.5971, Loss_G: 3.4927\n",
            "[Epoch 47/100] Loss_D: 0.5970, Loss_G: 3.4921\n",
            "[Epoch 48/100] Loss_D: 0.5741, Loss_G: 3.5959\n",
            "[Epoch 49/100] Loss_D: 0.5752, Loss_G: 3.7168\n",
            "[Epoch 50/100] Loss_D: 0.6316, Loss_G: 3.5302\n",
            "[Epoch 51/100] Loss_D: 0.5531, Loss_G: 3.6627\n",
            "[Epoch 52/100] Loss_D: 0.5944, Loss_G: 3.5671\n",
            "[Epoch 53/100] Loss_D: 0.5508, Loss_G: 3.7655\n",
            "[Epoch 54/100] Loss_D: 0.5557, Loss_G: 3.6928\n",
            "[Epoch 55/100] Loss_D: 0.6024, Loss_G: 3.7004\n",
            "[Epoch 56/100] Loss_D: 0.5171, Loss_G: 3.9766\n",
            "[Epoch 57/100] Loss_D: 0.5520, Loss_G: 4.0353\n",
            "[Epoch 58/100] Loss_D: 0.5530, Loss_G: 3.8810\n",
            "[Epoch 59/100] Loss_D: 0.5338, Loss_G: 3.9224\n",
            "[Epoch 60/100] Loss_D: 0.5117, Loss_G: 3.9926\n",
            "[Epoch 61/100] Loss_D: 0.5672, Loss_G: 4.0527\n",
            "[Epoch 62/100] Loss_D: 0.4984, Loss_G: 3.9986\n",
            "[Epoch 63/100] Loss_D: 0.5236, Loss_G: 3.9947\n",
            "[Epoch 64/100] Loss_D: 0.5552, Loss_G: 3.9627\n",
            "[Epoch 65/100] Loss_D: 0.5181, Loss_G: 4.0385\n",
            "[Epoch 66/100] Loss_D: 0.5018, Loss_G: 4.0436\n",
            "[Epoch 67/100] Loss_D: 0.4280, Loss_G: 4.2073\n",
            "[Epoch 68/100] Loss_D: 0.5281, Loss_G: 4.2073\n",
            "[Epoch 69/100] Loss_D: 0.4611, Loss_G: 4.1603\n",
            "[Epoch 70/100] Loss_D: 0.5095, Loss_G: 4.2978\n",
            "[Epoch 71/100] Loss_D: 0.4696, Loss_G: 4.1756\n",
            "[Epoch 72/100] Loss_D: 0.5064, Loss_G: 4.3479\n",
            "[Epoch 73/100] Loss_D: 0.4861, Loss_G: 4.1350\n",
            "[Epoch 74/100] Loss_D: 0.4873, Loss_G: 4.2924\n",
            "[Epoch 75/100] Loss_D: 0.4127, Loss_G: 4.4768\n",
            "[Epoch 76/100] Loss_D: 0.4617, Loss_G: 4.6073\n",
            "[Epoch 77/100] Loss_D: 0.4982, Loss_G: 4.2990\n",
            "[Epoch 78/100] Loss_D: 0.4715, Loss_G: 4.2933\n",
            "[Epoch 79/100] Loss_D: 0.4314, Loss_G: 4.4555\n",
            "[Epoch 80/100] Loss_D: 0.4612, Loss_G: 4.3956\n",
            "[Epoch 81/100] Loss_D: 0.4996, Loss_G: 4.3686\n",
            "[Epoch 82/100] Loss_D: 0.4529, Loss_G: 4.3172\n",
            "[Epoch 83/100] Loss_D: 0.3868, Loss_G: 4.5959\n",
            "[Epoch 84/100] Loss_D: 0.3569, Loss_G: 4.9045\n",
            "[Epoch 85/100] Loss_D: 0.4695, Loss_G: 4.7588\n",
            "[Epoch 86/100] Loss_D: 0.4661, Loss_G: 4.6027\n",
            "[Epoch 87/100] Loss_D: 0.3948, Loss_G: 4.6063\n",
            "[Epoch 88/100] Loss_D: 0.4467, Loss_G: 4.5617\n",
            "[Epoch 89/100] Loss_D: 0.3805, Loss_G: 4.7994\n",
            "[Epoch 90/100] Loss_D: 0.4083, Loss_G: 4.7264\n",
            "[Epoch 91/100] Loss_D: 0.4223, Loss_G: 4.6761\n",
            "[Epoch 92/100] Loss_D: 0.4089, Loss_G: 4.8762\n",
            "[Epoch 93/100] Loss_D: 0.4197, Loss_G: 4.7184\n",
            "[Epoch 94/100] Loss_D: 0.3944, Loss_G: 4.8964\n",
            "[Epoch 95/100] Loss_D: 0.4481, Loss_G: 4.8267\n",
            "[Epoch 96/100] Loss_D: 0.4392, Loss_G: 4.4928\n",
            "[Epoch 97/100] Loss_D: 0.3884, Loss_G: 4.7401\n",
            "[Epoch 98/100] Loss_D: 0.3796, Loss_G: 4.7776\n",
            "[Epoch 99/100] Loss_D: 0.4646, Loss_G: 4.7256\n",
            "[Epoch 100/100] Loss_D: 0.3457, Loss_G: 4.7749\n",
            "Training completed in 133 min 53 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load loss logs\n",
        "if os.path.exists(metrics_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyM483Ktovo",
        "outputId": "834b79d5-5ba0-436e-9946-9dcdb1d979d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E5_L2_DiscW/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E5_L2_DiscW/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: FID & Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    eval_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    i = 0\n",
        "    for imgs, _ in eval_loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            fake_imgs = generator(noise).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID Calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg7hlDzBtotO",
        "outputId": "4365ba49-665e-4ec8-ea1a-8afa1b368a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:58<00:00,  1.34it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 47.8164\n",
            "Inception Score: 3.5081 ± 0.0778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E6_CGAN_Baseline"
      ],
      "metadata": {
        "id": "NB7_lq6GWomS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E6_CGAN_Baseline: CGAN with No Regularization\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E6_CGAN_Baseline\"\n",
        "\n",
        "config = {\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"model\": \"CGAN\",\n",
        "    \"regularization_type\": \"None\",\n",
        "    \"regularization_lambda_L1\": 0.0,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"none\",\n",
        "    \"dataset\": \"CIFAR-10\",\n",
        "    \"image_size\": 32,\n",
        "    \"latent_dim\": 100,\n",
        "    \"num_classes\": 10,\n",
        "    \"num_epochs\": 100,\n",
        "    \"batch_size\": 128,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"beta1\": 0.5,\n",
        "    \"fid_samples\": 10000,\n",
        "    \"save_images_every\": 10,\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"pytorch_version\": torch.__version__,\n",
        "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"None\",\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\").type\n",
        "}"
      ],
      "metadata": {
        "id": "BVwJ2clr1KrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folders\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for sub in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, sub), exist_ok=True)"
      ],
      "metadata": {
        "id": "hH1og9TY1KuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "with open(os.path.join(exp_path, \"config_manual.txt\"), \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {exp_path}/config_manual.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YQvWdUN1Kwd",
        "outputId": "c037d7b5-e1b3-4c9e-a7c8-d52727fc22f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E6_CGAN_Baseline/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare fixed noise and labels\n",
        "device = torch.device(config[\"device\"])\n",
        "fixed_noise = torch.randn(64, config[\"latent_dim\"], 1, 1, device=device)\n",
        "fixed_labels = torch.arange(0, 10).repeat(7)[:64].to(device)\n",
        "torch.save(fixed_noise, os.path.join(exp_path, \"fixed_noise.pt\"))\n",
        "torch.save(fixed_labels, os.path.join(exp_path, \"fixed_labels.pt\"))"
      ],
      "metadata": {
        "id": "0vjYT5OQ1Ky0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional Generator and Discriminator\n",
        "class CGANGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes, feature_maps=64):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim + num_classes, feature_maps * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_maps, 3, 3, 1, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "        x = torch.cat([noise, label_embedding], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "class CGANDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, feature_maps=64):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3 + num_classes, feature_maps, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1),\n",
        "            nn.BatchNorm2d(feature_maps * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_maps * 8, 1, 2, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "        label_embedding = label_embedding.expand(labels.size(0), config[\"num_classes\"], config[\"image_size\"], config[\"image_size\"])\n",
        "        x = torch.cat([img, label_embedding], dim=1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "xLXwWr3O1U6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = CGANGenerator(latent_dim=config[\"latent_dim\"], num_classes=config[\"num_classes\"]).to(device)\n",
        "D = CGANDiscriminator(num_classes=config[\"num_classes\"]).to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(\"CGAN Generator and Discriminator initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OgrHcaa1U9V",
        "outputId": "e96334fb-5b92-45d0-ae89-205e8218ab10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CGAN Generator and Discriminator initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers & Loss\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "U7pvZ0Mv1VAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume checkpoint if exists\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    ckpt = torch.load(checkpoint_path)\n",
        "    G.load_state_dict(ckpt[\"G_state_dict\"])\n",
        "    D.load_state_dict(ckpt[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(ckpt[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(ckpt[\"optimizer_D\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    print(f\"Resumed from epoch {ckpt['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOh1ERJj1VCE",
        "outputId": "5e691b60-2a29-4fab-9e1e-02342d9e5877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previous loss logs\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log, loss_G_log = [], []\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "# Training loop\n",
        "training_start = time.time()\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "    for real_imgs, real_labels in dataloader:\n",
        "        real_imgs, real_labels = real_imgs.to(device), real_labels.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        real_targets = torch.ones(batch_size, device=device)\n",
        "        fake_targets = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_labels = torch.randint(0, config[\"num_classes\"], (batch_size,), device=device)\n",
        "        fake_imgs = G(noise, fake_labels).detach()\n",
        "        output_real = D(real_imgs, real_labels).view(-1)\n",
        "        output_fake = D(fake_imgs, fake_labels).view(-1)\n",
        "        loss_D = criterion(output_real, real_targets) + criterion(output_fake, fake_targets)\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_labels = torch.randint(0, config[\"num_classes\"], (batch_size,), device=device)\n",
        "        gen_imgs = G(noise, fake_labels)\n",
        "        output = D(gen_imgs, fake_labels).view(-1)\n",
        "        loss_G = criterion(output, real_targets)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Save images\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            sample_imgs = G(fixed_noise, fixed_labels).detach().cpu()\n",
        "            save_image(sample_imgs, os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\"), normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict()\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Log loss\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f} | Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save loss log\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Save training time\n",
        "duration = time.time() - training_start\n",
        "mins, secs = divmod(int(duration), 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training complete in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl_SUQ_k1VEf",
        "outputId": "d6b47788-f9f8-4004-80d3-2f2ff0de758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 1.1217 | Loss_G: 1.1649\n",
            "[Epoch 2/100] Loss_D: 1.2535 | Loss_G: 1.0771\n",
            "[Epoch 3/100] Loss_D: 1.2481 | Loss_G: 1.1583\n",
            "[Epoch 4/100] Loss_D: 1.2250 | Loss_G: 1.2524\n",
            "[Epoch 5/100] Loss_D: 1.2664 | Loss_G: 1.1215\n",
            "[Epoch 6/100] Loss_D: 1.3030 | Loss_G: 1.0184\n",
            "[Epoch 7/100] Loss_D: 1.2920 | Loss_G: 1.0208\n",
            "[Epoch 8/100] Loss_D: 1.3003 | Loss_G: 0.9857\n",
            "[Epoch 9/100] Loss_D: 1.3081 | Loss_G: 0.9506\n",
            "[Epoch 10/100] Loss_D: 1.3032 | Loss_G: 0.9487\n",
            "[Epoch 11/100] Loss_D: 1.3121 | Loss_G: 0.9415\n",
            "[Epoch 12/100] Loss_D: 1.3275 | Loss_G: 0.8913\n",
            "[Epoch 13/100] Loss_D: 1.3210 | Loss_G: 0.8802\n",
            "[Epoch 14/100] Loss_D: 1.3114 | Loss_G: 0.9116\n",
            "[Epoch 15/100] Loss_D: 1.3162 | Loss_G: 0.9030\n",
            "[Epoch 16/100] Loss_D: 1.3211 | Loss_G: 0.8813\n",
            "[Epoch 17/100] Loss_D: 1.3090 | Loss_G: 0.9111\n",
            "[Epoch 18/100] Loss_D: 1.3069 | Loss_G: 0.9208\n",
            "[Epoch 19/100] Loss_D: 1.2832 | Loss_G: 0.9333\n",
            "[Epoch 20/100] Loss_D: 1.2699 | Loss_G: 0.9740\n",
            "[Epoch 21/100] Loss_D: 1.2524 | Loss_G: 1.0169\n",
            "[Epoch 22/100] Loss_D: 1.2233 | Loss_G: 1.0527\n",
            "[Epoch 23/100] Loss_D: 1.2023 | Loss_G: 1.0850\n",
            "[Epoch 24/100] Loss_D: 1.1915 | Loss_G: 1.1170\n",
            "[Epoch 25/100] Loss_D: 1.1545 | Loss_G: 1.1647\n",
            "[Epoch 26/100] Loss_D: 1.1298 | Loss_G: 1.2194\n",
            "[Epoch 27/100] Loss_D: 1.1192 | Loss_G: 1.2724\n",
            "[Epoch 28/100] Loss_D: 1.0848 | Loss_G: 1.2700\n",
            "[Epoch 29/100] Loss_D: 1.0598 | Loss_G: 1.3403\n",
            "[Epoch 30/100] Loss_D: 1.0563 | Loss_G: 1.3687\n",
            "[Epoch 31/100] Loss_D: 1.0201 | Loss_G: 1.4126\n",
            "[Epoch 32/100] Loss_D: 1.0109 | Loss_G: 1.4068\n",
            "[Epoch 33/100] Loss_D: 0.9839 | Loss_G: 1.4588\n",
            "[Epoch 34/100] Loss_D: 0.9741 | Loss_G: 1.5124\n",
            "[Epoch 35/100] Loss_D: 0.9462 | Loss_G: 1.5389\n",
            "[Epoch 36/100] Loss_D: 0.9372 | Loss_G: 1.5602\n",
            "[Epoch 37/100] Loss_D: 0.9120 | Loss_G: 1.6095\n",
            "[Epoch 38/100] Loss_D: 0.9004 | Loss_G: 1.7032\n",
            "[Epoch 39/100] Loss_D: 0.8642 | Loss_G: 1.7290\n",
            "[Epoch 40/100] Loss_D: 0.8365 | Loss_G: 1.7852\n",
            "[Epoch 41/100] Loss_D: 0.8325 | Loss_G: 1.7984\n",
            "[Epoch 42/100] Loss_D: 0.8107 | Loss_G: 1.8305\n",
            "[Epoch 43/100] Loss_D: 0.7832 | Loss_G: 1.8926\n",
            "[Epoch 44/100] Loss_D: 0.7705 | Loss_G: 1.9472\n",
            "[Epoch 45/100] Loss_D: 0.7748 | Loss_G: 1.9350\n",
            "[Epoch 46/100] Loss_D: 0.7324 | Loss_G: 2.0305\n",
            "[Epoch 47/100] Loss_D: 0.7131 | Loss_G: 2.0754\n",
            "[Epoch 48/100] Loss_D: 0.7087 | Loss_G: 2.0966\n",
            "[Epoch 49/100] Loss_D: 0.6678 | Loss_G: 2.1212\n",
            "[Epoch 50/100] Loss_D: 0.6811 | Loss_G: 2.2565\n",
            "[Epoch 51/100] Loss_D: 0.6317 | Loss_G: 2.2541\n",
            "[Epoch 52/100] Loss_D: 0.6569 | Loss_G: 2.2866\n",
            "[Epoch 53/100] Loss_D: 0.6365 | Loss_G: 2.2771\n",
            "[Epoch 54/100] Loss_D: 0.6380 | Loss_G: 2.3515\n",
            "[Epoch 55/100] Loss_D: 0.6032 | Loss_G: 2.3513\n",
            "[Epoch 56/100] Loss_D: 0.5994 | Loss_G: 2.4768\n",
            "[Epoch 57/100] Loss_D: 0.5628 | Loss_G: 2.4968\n",
            "[Epoch 58/100] Loss_D: 0.5596 | Loss_G: 2.5678\n",
            "[Epoch 59/100] Loss_D: 0.5723 | Loss_G: 2.5626\n",
            "[Epoch 60/100] Loss_D: 0.5353 | Loss_G: 2.5661\n",
            "[Epoch 61/100] Loss_D: 0.5238 | Loss_G: 2.7258\n",
            "[Epoch 62/100] Loss_D: 0.5307 | Loss_G: 2.7285\n",
            "[Epoch 63/100] Loss_D: 0.5046 | Loss_G: 2.6932\n",
            "[Epoch 64/100] Loss_D: 0.5120 | Loss_G: 2.7229\n",
            "[Epoch 65/100] Loss_D: 0.4825 | Loss_G: 2.7573\n",
            "[Epoch 66/100] Loss_D: 0.4784 | Loss_G: 2.8188\n",
            "[Epoch 67/100] Loss_D: 0.4525 | Loss_G: 2.9790\n",
            "[Epoch 68/100] Loss_D: 0.4615 | Loss_G: 2.9293\n",
            "[Epoch 69/100] Loss_D: 0.4136 | Loss_G: 3.0171\n",
            "[Epoch 70/100] Loss_D: 0.4301 | Loss_G: 3.0684\n",
            "[Epoch 71/100] Loss_D: 0.4128 | Loss_G: 3.1648\n",
            "[Epoch 72/100] Loss_D: 0.4420 | Loss_G: 3.1497\n",
            "[Epoch 73/100] Loss_D: 0.4145 | Loss_G: 3.2205\n",
            "[Epoch 74/100] Loss_D: 0.4199 | Loss_G: 3.1425\n",
            "[Epoch 75/100] Loss_D: 0.3644 | Loss_G: 3.3123\n",
            "[Epoch 76/100] Loss_D: 0.3968 | Loss_G: 3.2276\n",
            "[Epoch 77/100] Loss_D: 0.3718 | Loss_G: 3.3366\n",
            "[Epoch 78/100] Loss_D: 0.3877 | Loss_G: 3.3482\n",
            "[Epoch 79/100] Loss_D: 0.3614 | Loss_G: 3.3420\n",
            "[Epoch 80/100] Loss_D: 0.3716 | Loss_G: 3.4409\n",
            "[Epoch 81/100] Loss_D: 0.3354 | Loss_G: 3.4114\n",
            "[Epoch 82/100] Loss_D: 0.3692 | Loss_G: 3.5903\n",
            "[Epoch 83/100] Loss_D: 0.3082 | Loss_G: 3.5204\n",
            "[Epoch 84/100] Loss_D: 0.3438 | Loss_G: 3.6726\n",
            "[Epoch 85/100] Loss_D: 0.3150 | Loss_G: 3.5706\n",
            "[Epoch 86/100] Loss_D: 0.2839 | Loss_G: 3.7351\n",
            "[Epoch 87/100] Loss_D: 0.3339 | Loss_G: 3.8854\n",
            "[Epoch 88/100] Loss_D: 0.3255 | Loss_G: 3.7418\n",
            "[Epoch 89/100] Loss_D: 0.3161 | Loss_G: 3.8300\n",
            "[Epoch 90/100] Loss_D: 0.2777 | Loss_G: 3.8521\n",
            "[Epoch 91/100] Loss_D: 0.3415 | Loss_G: 3.9817\n",
            "[Epoch 92/100] Loss_D: 0.2854 | Loss_G: 3.8060\n",
            "[Epoch 93/100] Loss_D: 0.2917 | Loss_G: 3.8258\n",
            "[Epoch 94/100] Loss_D: 0.3011 | Loss_G: 3.8494\n",
            "[Epoch 95/100] Loss_D: 0.2601 | Loss_G: 3.9792\n",
            "[Epoch 96/100] Loss_D: 0.2791 | Loss_G: 3.9061\n",
            "[Epoch 97/100] Loss_D: 0.2950 | Loss_G: 4.0154\n",
            "[Epoch 98/100] Loss_D: 0.2629 | Loss_G: 4.2098\n",
            "[Epoch 99/100] Loss_D: 0.2703 | Loss_G: 3.9733\n",
            "[Epoch 100/100] Loss_D: 0.2539 | Loss_G: 4.2085\n",
            "Training complete in 140 min 58 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and save loss curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load loss logs\n",
        "if os.path.exists(metrics_path):\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(f\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h59P1aGI1VGv",
        "outputId": "43f8fe03-27ea-48c7-8875-18c900498166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E6_CGAN_Baseline/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E6_CGAN_Baseline/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Settings\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    i = 0\n",
        "    total_batches = num_eval_images // dataloader.batch_size + 1\n",
        "    for real_imgs, _ in tqdm(dataloader, desc=\"Saving real images\", total=total_batches):\n",
        "        for img in real_imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    total_batches = num_eval_images // eval_batch_size + 1\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(total_batches), desc=\"Generating fake images\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            labels = torch.randint(0, config[\"num_classes\"], (curr_batch,), device=device)\n",
        "            fake_imgs = generator(noise, labels).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID Calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics[\"frechet_inception_distance\"]\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "with open(os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\"), \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONPYgp4Y1VJP",
        "outputId": "457d034f-defe-4338-a2c1-a8de1733aaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving real images:  99%|█████████▊| 78/79 [00:53<00:00,  1.46it/s]\n",
            "Generating fake images:  99%|█████████▊| 78/79 [00:57<00:00,  1.36it/s]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:01<00:00, 50.5MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 205MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 79.7459\n",
            "Inception Score: 2.7588 ± 0.0539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E7_CGAN_L1_DiscW"
      ],
      "metadata": {
        "id": "4tfkZYkH6cPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E7_CGAN_L1_DiscW: L1 Regularization on Discriminator Weights (CGAN)\n",
        "\n",
        "# Experiment Configuration\n",
        "experiment_id = \"E7_CGAN_L1_DiscW\"\n",
        "config = config.copy()\n",
        "config.update({\n",
        "    \"experiment_id\": experiment_id,\n",
        "    \"regularization_type\": \"L1\",\n",
        "    \"regularization_lambda_L1\": 0.0001,\n",
        "    \"regularization_lambda_L2\": 0.0,\n",
        "    \"regularization_placement\": \"discriminator_weights_early\",\n",
        "})"
      ],
      "metadata": {
        "id": "o2VRgAYm1K1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare experiment folder\n",
        "exp_path = os.path.join(ROOT_DIR, experiment_id)\n",
        "for subfolder in [\"images\", \"metrics\", \"eval_data/real\", \"eval_data/fake\"]:\n",
        "    os.makedirs(os.path.join(exp_path, subfolder), exist_ok=True)"
      ],
      "metadata": {
        "id": "YUNNT3_X6aTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save config\n",
        "config_path = os.path.join(exp_path, \"config_manual.txt\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    for k, v in config.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(f\"Configuration saved: {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt6dXaoe6aXH",
        "outputId": "fdfd66b1-dc6f-4665-f401-d2abf4ab006f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved: /content/drive/MyDrive/GAN_Research/E7_CGAN_L1_DiscW/config_manual.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload fixed noise and labels from baseline\n",
        "fixed_noise = torch.load(os.path.join(ROOT_DIR, \"E6_CGAN_Baseline\", \"fixed_noise.pt\"))\n",
        "fixed_labels = torch.load(os.path.join(ROOT_DIR, \"E6_CGAN_Baseline\", \"fixed_labels.pt\"))"
      ],
      "metadata": {
        "id": "0zZLBZuO6aaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "G = CGANGenerator(latent_dim=config[\"latent_dim\"], num_classes=config[\"num_classes\"]).to(device)\n",
        "D = CGANDiscriminator(num_classes=config[\"num_classes\"]).to(device)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "print(f\"Generator and Discriminator initialized for {experiment_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EOmi3uw4ts9",
        "outputId": "20ff5dbe-c6e4-47fd-e210-78271032af0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator initialized for E7_CGAN_L1_DiscW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "VNhgT8S14tqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpointing\n",
        "checkpoint_path = os.path.join(exp_path, \"checkpoint.pt\")\n",
        "start_epoch = 1\n",
        "if os.path.exists(checkpoint_path):\n",
        "    ckpt = torch.load(checkpoint_path)\n",
        "    G.load_state_dict(ckpt[\"G_state_dict\"])\n",
        "    D.load_state_dict(ckpt[\"D_state_dict\"])\n",
        "    optimizer_G.load_state_dict(ckpt[\"optimizer_G\"])\n",
        "    optimizer_D.load_state_dict(ckpt[\"optimizer_D\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    print(f\"Resumed from checkpoint epoch {ckpt['epoch']}\")\n",
        "else:\n",
        "    print(\"Starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzH4E2p24tnp",
        "outputId": "d0d375a7-7c40-4000-a71e-d748f9cf9d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previous loss logs\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "if os.path.exists(metrics_path):\n",
        "    df_prev = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df_prev[\"loss_D\"])\n",
        "    loss_G_log = list(df_prev[\"loss_G\"])\n",
        "    print(f\"Loaded previous loss logs for {len(loss_D_log)} epochs.\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "# Training Loop\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
        "    loss_D_epoch = 0.0\n",
        "    loss_G_epoch = 0.0\n",
        "\n",
        "    for real_imgs, real_labels in dataloader:\n",
        "        real_imgs, real_labels = real_imgs.to(device), real_labels.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "        real_targets = torch.ones(batch_size, device=device)\n",
        "        fake_targets = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_labels = torch.randint(0, config[\"num_classes\"], (batch_size,), device=device)\n",
        "        fake_imgs = G(noise, fake_labels).detach()\n",
        "\n",
        "        output_real = D(real_imgs, real_labels).view(-1)\n",
        "        output_fake = D(fake_imgs, fake_labels).view(-1)\n",
        "        loss_D = criterion(output_real, real_targets) + criterion(output_fake, fake_targets)\n",
        "\n",
        "        # Add L1 regularization on early discriminator convolutional layers\n",
        "        # Penalize absolute values of weights in the first two Conv2d layers of D\n",
        "        if config[\"regularization_type\"] == \"L1\" and config[\"regularization_placement\"] == \"discriminator_weights_early\":\n",
        "            l1_reg = torch.tensor(0., device=device)\n",
        "            conv_count = 0\n",
        "            for module in D.modules():\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    l1_reg += torch.sum(torch.abs(module.weight))\n",
        "                    conv_count += 1\n",
        "                    if conv_count == 2:\n",
        "                        break\n",
        "            loss_D += config[\"regularization_lambda_L1\"] * l1_reg\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        noise = torch.randn(batch_size, config[\"latent_dim\"], 1, 1, device=device)\n",
        "        fake_labels = torch.randint(0, config[\"num_classes\"], (batch_size,), device=device)\n",
        "        gen_imgs = G(noise, fake_labels)\n",
        "        output = D(gen_imgs, fake_labels).view(-1)\n",
        "        loss_G = criterion(output, real_targets)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        loss_D_epoch += loss_D.item()\n",
        "        loss_G_epoch += loss_G.item()\n",
        "\n",
        "    # Logging\n",
        "    avg_D = loss_D_epoch / len(dataloader)\n",
        "    avg_G = loss_G_epoch / len(dataloader)\n",
        "    loss_D_log.append(avg_D)\n",
        "    loss_G_log.append(avg_G)\n",
        "    print(f\"[Epoch {epoch}/{config['num_epochs']}] Loss_D: {avg_D:.4f}, Loss_G: {avg_G:.4f}\")\n",
        "\n",
        "    # Save sample image\n",
        "    if epoch % config[\"save_images_every\"] == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            samples = G(fixed_noise, fixed_labels).detach().cpu()\n",
        "            img_path = os.path.join(exp_path, \"images\", f\"epoch_{epoch:03}.png\")\n",
        "            save_image(samples, img_path, normalize=True, nrow=8)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % 10 == 0 or epoch == config[\"num_epochs\"]:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G_state_dict\": G.state_dict(),\n",
        "            \"D_state_dict\": D.state_dict(),\n",
        "            \"optimizer_G\": optimizer_G.state_dict(),\n",
        "            \"optimizer_D\": optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    # Save loss CSV\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        f.write(\"epoch,loss_D,loss_G\\n\")\n",
        "        for i in range(len(loss_D_log)):\n",
        "            f.write(f\"{i+1},{loss_D_log[i]},{loss_G_log[i]}\\n\")\n",
        "\n",
        "# Training Time\n",
        "total_sec = time.time() - training_start_time\n",
        "mins, secs = int(total_sec // 60), int(total_sec % 60)\n",
        "with open(os.path.join(exp_path, \"training_time.txt\"), \"w\") as f:\n",
        "    f.write(f\"Total training time: {mins} min {secs} sec\\n\")\n",
        "print(f\"Training completed in {mins} min {secs} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZMnEpD-4tii",
        "outputId": "3359e478-69ef-4b72-e81e-a48aa2852c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous loss logs found. Starting fresh.\n",
            "[Epoch 1/100] Loss_D: 1.3052, Loss_G: 1.1755\n",
            "[Epoch 2/100] Loss_D: 1.3716, Loss_G: 1.1904\n",
            "[Epoch 3/100] Loss_D: 1.3814, Loss_G: 1.2336\n",
            "[Epoch 4/100] Loss_D: 1.4020, Loss_G: 1.2338\n",
            "[Epoch 5/100] Loss_D: 1.4046, Loss_G: 1.2355\n",
            "[Epoch 6/100] Loss_D: 1.4190, Loss_G: 1.1531\n",
            "[Epoch 7/100] Loss_D: 1.4150, Loss_G: 1.1076\n",
            "[Epoch 8/100] Loss_D: 1.4284, Loss_G: 1.0872\n",
            "[Epoch 9/100] Loss_D: 1.4207, Loss_G: 1.1047\n",
            "[Epoch 10/100] Loss_D: 1.4307, Loss_G: 1.1070\n",
            "[Epoch 11/100] Loss_D: 1.4320, Loss_G: 1.0807\n",
            "[Epoch 12/100] Loss_D: 1.4239, Loss_G: 1.0294\n",
            "[Epoch 13/100] Loss_D: 1.4376, Loss_G: 1.0352\n",
            "[Epoch 14/100] Loss_D: 1.4150, Loss_G: 1.0405\n",
            "[Epoch 15/100] Loss_D: 1.4178, Loss_G: 1.0939\n",
            "[Epoch 16/100] Loss_D: 1.4064, Loss_G: 1.0743\n",
            "[Epoch 17/100] Loss_D: 1.3914, Loss_G: 1.0917\n",
            "[Epoch 18/100] Loss_D: 1.3647, Loss_G: 1.1619\n",
            "[Epoch 19/100] Loss_D: 1.3448, Loss_G: 1.1853\n",
            "[Epoch 20/100] Loss_D: 1.3333, Loss_G: 1.2229\n",
            "[Epoch 21/100] Loss_D: 1.3037, Loss_G: 1.2577\n",
            "[Epoch 22/100] Loss_D: 1.2797, Loss_G: 1.3526\n",
            "[Epoch 23/100] Loss_D: 1.2497, Loss_G: 1.3688\n",
            "[Epoch 24/100] Loss_D: 1.2278, Loss_G: 1.4089\n",
            "[Epoch 25/100] Loss_D: 1.2243, Loss_G: 1.4486\n",
            "[Epoch 26/100] Loss_D: 1.2028, Loss_G: 1.4643\n",
            "[Epoch 27/100] Loss_D: 1.1873, Loss_G: 1.4906\n",
            "[Epoch 28/100] Loss_D: 1.1831, Loss_G: 1.5395\n",
            "[Epoch 29/100] Loss_D: 1.1789, Loss_G: 1.5443\n",
            "[Epoch 30/100] Loss_D: 1.1566, Loss_G: 1.5874\n",
            "[Epoch 31/100] Loss_D: 1.1454, Loss_G: 1.5728\n",
            "[Epoch 32/100] Loss_D: 1.1005, Loss_G: 1.6482\n",
            "[Epoch 33/100] Loss_D: 1.1053, Loss_G: 1.6615\n",
            "[Epoch 34/100] Loss_D: 1.0823, Loss_G: 1.7321\n",
            "[Epoch 35/100] Loss_D: 1.0651, Loss_G: 1.8258\n",
            "[Epoch 36/100] Loss_D: 1.0476, Loss_G: 1.7926\n",
            "[Epoch 37/100] Loss_D: 0.9991, Loss_G: 1.9158\n",
            "[Epoch 38/100] Loss_D: 0.9997, Loss_G: 1.9768\n",
            "[Epoch 39/100] Loss_D: 0.9704, Loss_G: 1.9856\n",
            "[Epoch 40/100] Loss_D: 0.9550, Loss_G: 2.1060\n",
            "[Epoch 41/100] Loss_D: 0.9453, Loss_G: 2.0783\n",
            "[Epoch 42/100] Loss_D: 0.9235, Loss_G: 2.1658\n",
            "[Epoch 43/100] Loss_D: 0.8936, Loss_G: 2.1473\n",
            "[Epoch 44/100] Loss_D: 0.8900, Loss_G: 2.2596\n",
            "[Epoch 45/100] Loss_D: 0.8756, Loss_G: 2.2949\n",
            "[Epoch 46/100] Loss_D: 0.8509, Loss_G: 2.3866\n",
            "[Epoch 47/100] Loss_D: 0.8474, Loss_G: 2.3757\n",
            "[Epoch 48/100] Loss_D: 0.8552, Loss_G: 2.4333\n",
            "[Epoch 49/100] Loss_D: 0.8090, Loss_G: 2.5277\n",
            "[Epoch 50/100] Loss_D: 0.8378, Loss_G: 2.4493\n",
            "[Epoch 51/100] Loss_D: 0.7734, Loss_G: 2.5091\n",
            "[Epoch 52/100] Loss_D: 0.8156, Loss_G: 2.6015\n",
            "[Epoch 53/100] Loss_D: 0.7658, Loss_G: 2.6974\n",
            "[Epoch 54/100] Loss_D: 0.7591, Loss_G: 2.6787\n",
            "[Epoch 55/100] Loss_D: 0.7642, Loss_G: 2.7683\n",
            "[Epoch 56/100] Loss_D: 0.7542, Loss_G: 2.8589\n",
            "[Epoch 57/100] Loss_D: 0.7356, Loss_G: 2.8840\n",
            "[Epoch 58/100] Loss_D: 0.7095, Loss_G: 2.9310\n",
            "[Epoch 59/100] Loss_D: 0.7473, Loss_G: 2.8793\n",
            "[Epoch 60/100] Loss_D: 0.7256, Loss_G: 2.9351\n",
            "[Epoch 61/100] Loss_D: 0.6861, Loss_G: 2.8949\n",
            "[Epoch 62/100] Loss_D: 0.7099, Loss_G: 3.0078\n",
            "[Epoch 63/100] Loss_D: 0.7095, Loss_G: 2.9859\n",
            "[Epoch 64/100] Loss_D: 0.7084, Loss_G: 2.9832\n",
            "[Epoch 65/100] Loss_D: 0.6664, Loss_G: 3.0373\n",
            "[Epoch 66/100] Loss_D: 0.6375, Loss_G: 3.1893\n",
            "[Epoch 67/100] Loss_D: 0.6718, Loss_G: 3.1419\n",
            "[Epoch 68/100] Loss_D: 0.6452, Loss_G: 3.1483\n",
            "[Epoch 69/100] Loss_D: 0.6329, Loss_G: 3.3371\n",
            "[Epoch 70/100] Loss_D: 0.6573, Loss_G: 3.2485\n",
            "[Epoch 71/100] Loss_D: 0.6227, Loss_G: 3.3398\n",
            "[Epoch 72/100] Loss_D: 0.6093, Loss_G: 3.3201\n",
            "[Epoch 73/100] Loss_D: 0.6746, Loss_G: 3.4441\n",
            "[Epoch 74/100] Loss_D: 0.6267, Loss_G: 3.3185\n",
            "[Epoch 75/100] Loss_D: 0.5779, Loss_G: 3.5424\n",
            "[Epoch 76/100] Loss_D: 0.6319, Loss_G: 3.4101\n",
            "[Epoch 77/100] Loss_D: 0.6355, Loss_G: 3.4630\n",
            "[Epoch 78/100] Loss_D: 0.5747, Loss_G: 3.4727\n",
            "[Epoch 79/100] Loss_D: 0.6006, Loss_G: 3.4932\n",
            "[Epoch 80/100] Loss_D: 0.6242, Loss_G: 3.4637\n",
            "[Epoch 81/100] Loss_D: 0.5500, Loss_G: 3.7069\n",
            "[Epoch 82/100] Loss_D: 0.5769, Loss_G: 3.6342\n",
            "[Epoch 83/100] Loss_D: 0.5659, Loss_G: 3.6777\n",
            "[Epoch 84/100] Loss_D: 0.5566, Loss_G: 3.8434\n",
            "[Epoch 85/100] Loss_D: 0.5653, Loss_G: 3.8399\n",
            "[Epoch 86/100] Loss_D: 0.5806, Loss_G: 3.8073\n",
            "[Epoch 87/100] Loss_D: 0.5295, Loss_G: 3.8076\n",
            "[Epoch 88/100] Loss_D: 0.5429, Loss_G: 3.8717\n",
            "[Epoch 89/100] Loss_D: 0.5684, Loss_G: 3.9661\n",
            "[Epoch 90/100] Loss_D: 0.5192, Loss_G: 4.0586\n",
            "[Epoch 91/100] Loss_D: 0.5315, Loss_G: 4.0126\n",
            "[Epoch 92/100] Loss_D: 0.5328, Loss_G: 4.0590\n",
            "[Epoch 93/100] Loss_D: 0.5197, Loss_G: 4.0180\n",
            "[Epoch 94/100] Loss_D: 0.5524, Loss_G: 3.9440\n",
            "[Epoch 95/100] Loss_D: 0.5033, Loss_G: 4.1580\n",
            "[Epoch 96/100] Loss_D: 0.5303, Loss_G: 4.2032\n",
            "[Epoch 97/100] Loss_D: 0.5041, Loss_G: 4.1393\n",
            "[Epoch 98/100] Loss_D: 0.5844, Loss_G: 4.0619\n",
            "[Epoch 99/100] Loss_D: 0.4957, Loss_G: 4.0118\n",
            "[Epoch 100/100] Loss_D: 0.4492, Loss_G: 4.3525\n",
            "Training completed in 139 min 55 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and Save Loss Curve\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"losses.csv\")\n",
        "loss_D_log = []\n",
        "loss_G_log = []\n",
        "\n",
        "# Load loss logs\n",
        "if os.path.exists(metrics_path):\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    loss_D_log = list(df[\"loss_D\"])\n",
        "    loss_G_log = list(df[\"loss_G\"])\n",
        "    print(f\"Loaded loss logs for {len(loss_D_log)} epochs from {metrics_path}\")\n",
        "else:\n",
        "    print(\"No previous loss logs found. Starting fresh.\")\n",
        "\n",
        "epochs = list(range(1, len(loss_D_log) + 1))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_D_log, label=\"Discriminator Loss\")\n",
        "plt.plot(epochs, loss_G_log, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(f\"Training Loss Curve ({experiment_id})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_path = os.path.join(exp_path, \"metrics\", \"loss_curve.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(f\"Loss curve saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8CSI0XT4tf4",
        "outputId": "1644e040-da77-4ac4-b593-6f0075df218a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded loss logs for 100 epochs from /content/drive/MyDrive/GAN_Research/E7_CGAN_L1_DiscW/metrics/losses.csv\n",
            "Loss curve saved: /content/drive/MyDrive/GAN_Research/E7_CGAN_L1_DiscW/metrics/loss_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: FID and Inception Score\n",
        "eval_batch_size = 128\n",
        "num_eval_images = config[\"fid_samples\"]\n",
        "eval_image_size = config[\"image_size\"]\n",
        "real_dir = os.path.join(exp_path, \"eval_data\", \"real\")\n",
        "fake_dir = os.path.join(exp_path, \"eval_data\", \"fake\")\n",
        "\n",
        "# Clean eval folders\n",
        "for dir_ in [real_dir, fake_dir]:\n",
        "    for f in os.listdir(dir_):\n",
        "        os.remove(os.path.join(dir_, f))\n",
        "\n",
        "# Save real images\n",
        "def save_real_images():\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize(eval_image_size),\n",
        "        transforms.CenterCrop(eval_image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n",
        "    ])\n",
        "    dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "    loader = DataLoader(dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "\n",
        "    i = 0\n",
        "    for imgs, _ in loader:\n",
        "        for img in imgs:\n",
        "            save_image(img, f\"{real_dir}/{i}.png\", normalize=True)\n",
        "            i += 1\n",
        "            if i >= num_eval_images:\n",
        "                return\n",
        "\n",
        "# Save fake images\n",
        "def save_fake_images(generator):\n",
        "    generator.eval()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(0, num_eval_images, eval_batch_size), desc=\"Generating fake\"):\n",
        "            curr_batch = min(eval_batch_size, num_eval_images - i)\n",
        "            noise = torch.randn(curr_batch, config[\"latent_dim\"], 1, 1, device=device)\n",
        "            labels = torch.randint(0, config[\"num_classes\"], (curr_batch,), device=device)\n",
        "            fake_imgs = generator(noise, labels).cpu()\n",
        "            for img in fake_imgs:\n",
        "                save_image(img, f\"{fake_dir}/{i}.png\", normalize=True)\n",
        "                i += 1\n",
        "                if i >= num_eval_images:\n",
        "                    return\n",
        "\n",
        "save_real_images()\n",
        "save_fake_images(G)\n",
        "\n",
        "# FID Calculation\n",
        "metrics = calculate_metrics(\n",
        "    input1=real_dir,\n",
        "    input2=fake_dir,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False,\n",
        "    fid=True,\n",
        "    kid=False,\n",
        "    verbose=False\n",
        ")\n",
        "fid = metrics['frechet_inception_distance']\n",
        "\n",
        "# Inception Score\n",
        "class PNGImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.paths = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def compute_inception_score(image_folder, splits=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = PNGImageFolder(image_folder, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for imgs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(imgs)\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    split_scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (len(preds)//splits): (k+1)*(len(preds)//splits)]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = [entropy(p, py) for p in part]\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "\n",
        "    return float(np.mean(split_scores)), float(np.std(split_scores))\n",
        "\n",
        "mean_is, std_is = compute_inception_score(fake_dir)\n",
        "\n",
        "metrics_path = os.path.join(exp_path, \"metrics\", \"eval_metrics.txt\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    f.write(f\"FID: {fid:.4f}\\n\")\n",
        "    f.write(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nFID: {fid:.4f}\")\n",
        "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDlLn52I4tdg",
        "outputId": "01e61e3a-dd4b-4336-88eb-5c15cac76b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating fake:  99%|█████████▊| 78/79 [00:59<00:00,  1.31it/s]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:02<00:00, 32.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 238MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FID: 73.6611\n",
            "Inception Score: 3.0467 ± 0.0596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results Visualization: FID and IS Scores"
      ],
      "metadata": {
        "id": "RkbqZs1IdGOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FID data\n",
        "fid_data = {\n",
        "    'Experiment': ['E5_L1_DiscW', 'E2_L2_GenOut', 'E1_Baseline', 'E5_L2_DiscW',\n",
        "                   'E3_L2_DiscIn', 'E4_L2_GenW', 'E2_L1_GenOut', 'E3_L1_DiscIn',\n",
        "                   'E4_L1_GenW', 'E7_CGAN_L1_DiscW', 'E6_CGAN_Baseline'],\n",
        "    'FID Score': [44.51, 45.57, 47.63, 47.82, 49.56, 49.59, 50.28, 50.93,\n",
        "                  54.24, 73.66, 79.75]\n",
        "}\n",
        "\n",
        "df_fid = pd.DataFrame(fid_data).sort_values('FID Score')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(df_fid['Experiment'], df_fid['FID Score'], color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('FID Scores by Experiment (Lower is Better)', fontsize=14)\n",
        "plt.ylabel('FID Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "PGOvhjuMgyPF",
        "outputId": "3a06c9cf-b649-46d2-a7b9-9bbe0aea26c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA0vRJREFUeJzs3Xd4VFX+x/HPJCGFkJ4QSqgRAaUXJSJNUMRVQFBEYRcQUFeERbFgAYVFAREsPymrIC7rgmVVWFZlBQSVqsEggtIERcAAISQhgSQkc35/sLlm0hNyMwO+X8+T58l8750753vOmfKdW8ZhjDECAAAAAACVzsvdDQAAAAAA4FJF0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AcDuHw6Hu3bu7uxmXnGeeeUYOh0Pr1693d1Pcpn///mrevLlyc3Pd3RSPMnz4cDkcDv3000/ubspFLTk5WSEhIXr00Ufd3RQAHoyiG8Al5aeffpLD4SjxLyUlxVq/YcOG8vf3d9nG+vXrC92nRo0aqlevnvr06aMZM2bo6NGj5W5bRkaGnnvuObVr1041atSQn5+fYmJi1KVLFz3++OP68ccfLzR9lFP37t1LnS+/54LVE1zIFzKff/65VqxYoaefflre3t5WPO/LiLfffruSWok8RT2nqlWrpnr16umuu+7Sd999d8GPUdqXSQ0bNlTDhg0v+HHKIjw8XOPGjdMrr7yin3/+uUoeE8DFx8fdDQAAO8TGxmro0KFFLitYZBenffv2uvnmmyVJZ86cUWJiojZt2qRVq1ZpypQpev755zV27Ngybev06dO69tprtWPHDl122WUaOnSoIiIilJSUpK+++kozZsxQbGysYmNjy5YgKtWECRNUo0aNIpdV1Yd3OzzwwAMaPHiw6tev7+6muMWkSZPUoEEDDRo0yN1N8TjTp0/XxIkTVbduXVu2n/85lZ6eru3bt+vtt9/W8uXL9cUXX6hDhw62PK47jB8/XjNnztS0adP0+uuvu7s5ADwQRTeAS9Jll12mZ5555oK20aFDhyK3sWLFCo0cOVLjxo1TYGCg7r777lK39dJLL2nHjh0aNWqUXnvtNTkcDpflBw8eVFZW1gW1FxX38MMPq1atWu5uRqWLjIxUZGSku5vhFrt27dKXX36pJ598Ul5eHNhXUO3atVW7dm3btl/Uc2rWrFl69NFH9corr2jJkiW2PXZVi4iIUJ8+fbRs2TLNnj1bwcHB7m4SAA/DuxAAlFO/fv30r3/9S5L02GOPKSMjo9T7bN68WZI0ZsyYQgW3JDVq1EjNmjUrFD9+/LgmTJigpk2bKiAgQOHh4br66qv1wgsvFFp35cqV6tGjh0JCQhQQEKDWrVtrzpw5ysnJcVkv7xD84cOH64cfftCtt96qiIiIQud3rlixQj179lRYWJj8/f3VokULvfDCC4XOjXU6nVq4cKGuuuoqhYeHKyAgQDExMbrlllvKfWj24cOHdeeddyoyMlLVq1dX586dtWbNGpd1hg4dKofDoa+++qrIbUyePFkOh0PLli0r12OXZv/+/QoKClJMTIxOnjxZ6rL8/bxr1y794Q9/UGhoqGrUqKEbbrhB27ZtK/JxTp8+raefflpXXnmlAgICFBoaqt69e2vDhg2F1s07lDczM1NPPfWUYmNjVa1aNevLoqIOwy04/jfffLNCQ0MVFhamO++8U0lJSZLOz9mePXsqODhYYWFhGjVqVLFz/YsvvtAtt9yiyMhI+fn5qUmTJnrqqad05swZl/XyTt145plnFB8fr+uvv15BQUEKCQnRrbfe6jL/8taVzh8mnv9w5TfffLPYccqzePFiSdLtt99e6rqlKctzy+l0KiIiQi1atHC5b3Jysry8vORwOArN5bzzqgselrxjxw4NHjxYtWvXlq+vrxo0aKCxY8cWmndlfS4Xpbhzut9//31169ZNNWvWlL+/v+rUqaNevXrp/fffL2NvFe/GG2+UJGuO5WeM0RtvvKHOnTsrODhY1atXV4cOHfTGG2+4rNe9e3dNmTJFktSjRw9rTjRs2NDqj59//lk///yzy5wp+AVqRebspk2bdMMNNyg0NLTQ6/igQYOUkZGh995770K7CcAliKIbACqge/fu6tKli5KSkvTZZ5+Vun5ERIQkae/evWV+jD179qhNmzaaM2eOatasqXHjxumuu+5S9erV9dxzz7msO2fOHPXt21c7duzQXXfdpTFjxujs2bOaMGGCbr/9dhljCm1///796tSpk06cOKHhw4dr2LBh8vX1lSQ9/vjj6t+/v/bs2aMBAwbo/vvvV0BAgB555BENHjzYZTuPP/64Ro8ereTkZN11110aP368rrvuOu3atatQkVGSU6dOqXPnztq3b59GjRqlO++8U99++61uvPFGLV++3Frv3nvvlSQtXLiw0DZyc3O1ePFiRUREaMCAAWV+7LK47LLL9H//9386cuSIRo0aZcXPnTunO++8U2fOnNE//vEPa6zzHDhwQJ07d9bZs2f15z//WX379tW6devUtWtXbd261WXd5ORkxcXFaerUqQoLC9N9992ngQMHatu2berRo4dLP+Q3cOBAvfnmm+rRo4f+8pe/qFGjRqXmc/DgQV1zzTXKysrSqFGj1Lp1a7399tvq37+/NmzYoJ49e6pGjRq65557FBsbq0WLFhV5OsX8+fPVvXt3bdy4UX/4wx80btw4xcTE6Nlnn9X111+v7OzsQvf5+uuv1bVrV/n6+uree+9Vhw4dtHz5cvXq1UuZmZmSzh/W//TTT0uSGjRooKefftr6a9OmTan5rV27VoGBgYWK4PIq63PLy8tL3bp1065du3T8+HHr/p9//rm1zrp161y2vW7dOjVq1EgNGjSwYv/+97911VVX6d///re6d++u8ePHq2XLlnr11VcVFxenU6dOFWpjSc/l8pg/f75uu+027du3T7feeqseeugh3XjjjUpMTNSHH35Y7u0V9Omnn0qS2rVr5xI3xmjIkCEaOXKkTpw4obvuusv6kmfkyJF6+OGHrXWHDx+ubt26SZKGDRtmzYnx48crNDRUTz/9tEJCQhQSEuIyZ/JfF6Aic3bTpk3Wl1z33HOP7rjjDpflcXFxks7POwAoxADAJeTgwYNGkomNjTVPP/10ob/Nmze7rN+gQQPj5+fnElu3bp2RZO69994SH2vSpElGkpk0aVKp7VqxYoWRZIKCgsyECRPMf//7X5OUlFTifTp06GAkmddee63Qsl9++cX6f//+/cbHx8fUrFnTHDp0yIpnZmaaa6+91kgyS5YsseJ5fSTJTJ48udC2P/30UyPJ9O7d26Snp1txp9Np7rvvPiPJ/Otf/7Li4eHhpk6dOiYjI6PQtk6ePFlijnny2nPXXXcZp9Npxb/99lvj6+troqKizJkzZ6z4FVdcYYKCglzaZ4wx//nPf4wkM378+DI9brdu3YwkM2HChCLny/Tp0wvdZ/DgwUaSmTdvnjHGmEceecRIMo8//rjLevn7eeLEiS7LVq1aZSSZli1busTvuusuI8m8/vrrLvFjx46ZevXqmaioKHP27NlC7W/Tpk2Rff30008bSWbdunVFtuull16y4k6n09x0001GkgkNDTXLly+3lmVnZ5tWrVoZHx8fk5iYaMV37dplfHx8TOvWrQvN5+nTpxtJ5oUXXrBiec8tSebtt992Wf+Pf/yjkWSWLVvmEpdkunXrVii3kpw+fdp4eXmZzp07F7k8r18KPlZB5X1uvfLKK0aSeeedd6zY2LFjTWBgoOnUqZOJi4uz4j/++KORZO6++24rlpSUZIKDg03dunXNTz/95NKWZcuWGUnmgQcesGKlPZdLMmzYMCPJHDx40Iq1a9fO+Pr6mmPHjhVav7TXqzxFPacefvhhc8MNNxgvLy/Ts2dPc+rUKZf7vPbaa0aSGTFihMnOzrbiWVlZ5pZbbjGSTHx8vBUval7n16BBA9OgQYMil13InH3jjTdKzD0sLMzUr1+/xHUA/D5RdAO4pOT/EFrU34svvuiy/oUU3fPnzzeSzJ///OcytW327NmmRo0aLu2JjY01Y8aMMXv37nVZd+vWrUaS6dq1a6nbnTp1qpFkZs6cWWjZxo0bjSRz3XXXWbG8PqpVq5bJysoqdJ++ffsaSebnn38utCwlJcU4HA4zcOBAKxYeHm4aNmxoMjMzS21rcSQZb2/vQoWGMcaMHDmyUKH/8ssvG0lm4cKFLuv279/fSDK7du0q0+PmFQjF/YWEhBS6T0pKimnYsKEJCAgwr7zyinE4HOaqq64y586dc1kvr59DQ0PN6dOnC22nZ8+eLsXEiRMnjLe3t8tY5ZdX0K1cubJQ+1esWFHkfUoqumNjY12+4DDGmCVLlhhJpkePHoW2lTfPPvvsMys2btw4I8l88cUXhdbPzc01UVFRpn379lYs77lV1LzOW/bQQw+5xCtSdO/Zs8dIMgMGDChyeVmL7vI+t7777rtCrx0tWrQwvXv3NpMnTzY+Pj7WXFi4cGGhon3OnDmFYvm1a9fOREZGWrdLey6XpLiiOzAw0CQnJ5drW/mV9Jxq2LBhoeesMca0atXKBAYGunyxlmfHjh1WEZ/nQoruis7Zdu3alZK5Mc2aNTM+Pj6FnlcAwIXUAFySevfurVWrVrm7GS4eeughjR49WqtWrdKmTZsUHx+vrVu3au7cuVq0aJHeeecd9e3bV5Ks85VvuOGGUrebkJAgSUX+rFJcXJz8/f21ffv2Qstat25d5CGoW7ZsUWBgYKFzKfMEBARo9+7d1u3Bgwdr3rx5atGihQYPHqwePXooLi5OAQEBpbY9v/r167scZpunS5cuWrRokRISEjRw4EBJ0p/+9CdNnDhRr7/+ukaOHClJOnbsmP7zn//ommuu0RVXXFGux/7111/LfCG1kJAQ/fOf/1TXrl01btw4BQUFaenSpfLxKfottW3btkVeGb1Lly5au3atEhIS1L59e3399dfKzc1VVlZWkRfw27dvnyRp9+7d1lX181x11VVlant+rVq1KnReat6FtYo6fDtvWf6fy9uyZYsk6b///W+Rh9VWq1bNZa7kad++faFYTEyMJLn8pF9F5Z37HBoaekHbKe9z68orr1RUVJR1GPmJEye0a9cu/fGPf9RVV12lqVOn6ssvv1SfPn2sdXr06GHdP68/t27dWuRPCGZmZiopKUlJSUkuF8gr7rlcXoMHD9ajjz6qFi1a6K677lKPHj107bXXVujCYPmfU2fPntX+/fs1depUjRo1St9//71mz54t6fwvQ3z33XeqU6eOZs6cWWg7586dk6Qi51FFVHTOduzYsdRth4eHKycnRykpKQoLC7vwxgK4ZFB0A0AF5RUfUVFRZb5PUFCQbr/9duviTqmpqXriiSc0b948jRw5UkeOHJGvr69SU1MlqUw/55OWliZJio6OLrTM4XAoOjpaR44cKbSsqPWl8+cV5+TkWBcrKkr+C2q9/PLLatSokRYvXqxp06Zp2rRp8vf316BBgzR79uwyXz27uPbkxfP6RDpfTA0aNEh///vftXPnTrVo0UJvvvmmcnJyNHr06DI93oVo166dGjRooAMHDqhPnz4l/tRbWfNKTk6WJG3cuFEbN24sdntFXcysuMcoSVGFVN4XByUtyyuCpN/a/Oyzz1baYxe8UF9F5H3hk3d+eEWV97mV95vi7733no4ePaqNGzfKGKPrrrtOLVu2lL+/v9atW6c+ffpo/fr1uuyyy6wvG6Tf+nPu3LkltisjI8PleVWR8S/Kww8/rIiICM2fP1+zZ8/WCy+8IB8fH/3hD3/Qiy++WKZrBRQlICBALVu21NKlSxUfH6+XX35Z48aNU4MGDXTq1CkZY3TkyJEyv+ZciIrO2bL08dmzZyVJ1atXL3/DAFzSuJAaAFRQ3hWhy7IHpDghISF69dVX1aBBAyUlJem7776T9NseuqKK5YLyCphjx44VWmaM0bFjx4oscoq6inre9iIiImTOn4JU5N/Bgwet9X18fPTwww9r165dOnLkiJYuXaouXbpoyZIlGjJkSKntz1NU+/PHQ0JCXOL33XefJFm/i7to0SIFBwdXyW8yP/LIIzpw4IAiIiL07rvv6uOPPy523bLmlTdGEyZMKLHv8y4ull9xY2m3vDanpaWV2OaqlvdFWF6BVVEVeW7l7blet26d1q9fr5CQELVt21Z+fn6Ki4vTunXrtG/fPh05csRlL3f+x/vuu+9K7M+CR4RU1vg7HA7dfffd+vrrr3XixAl9+OGHGjBggFasWKGbb775gr8QqVatmtq1a6fc3FzrKIK8nNu3b19izgUvQldRFZ2zZenj5ORkBQUFyc/Pr1LaCuDSQdENABXw+eef68svv1TNmjV13XXXXdC2HA6HAgMDXWJ5hwvnXe23JG3btpWkIn+ea+vWrcrMzCzT1Z7zXH311Tp58qR1OHN51KlTR3feeadWrVqlyy67TGvWrLH2/pTm0KFDhX46SZK+/PJLSb/lmadTp05q1aqV3nrrLX366afat2+fhgwZYvtepo8++kivvvqqunXrpvj4eIWFhWnEiBHFFtcJCQlKT08vFC+YV8eOHeVwOKyfl7sYXH311ZJ+O2TXDl5eXuUu9urUqaOIiAjt2bPngh67Is+tvEL6s88+07p169StWzd5e3tLkq677jolJCRYVwIveNh6Xn96whyIiIhQ//799c477+i6667T999/r/3791/wdvOuvu50OiWdP/qnefPm+uGHH8p8akFefxY3L7y9vYtdZteczcjI0OHDh9WyZctK3S6ASwNFNwCU08qVK61zi2fOnFmmIu9vf/ubvv766yKXLV++XD/88INCQ0Otnzfq2LGjOnbsqC+++MLak5tf/j3gd911l3x8fDRnzhyX822zs7P12GOPSTr/MztlNW7cOEnS3XffXeh3gSUpMTFRP/zwgyQpKytLmzZtKrRORkaG0tPTVa1aNXl5le2tJjc3V0888YTLXqYdO3boH//4h6KionTTTTcVus+9996r5ORkjRgxQpJsP7Q8MTFRI0aMUFhYmN566y01bNhQr732mo4fP65hw4YVuYcsJSWl0KGseeeTtmjRwjq/uVatWho0aJA2bdqkWbNmFbmtrVu3FvodYXe6//775ePjo7Fjx+rQoUOFlqekpFh7NCsqPDxchw8fLtd9HA6HunTpooMHD+rEiRMVfuyKPLeaNWumWrVqaeXKlfrhhx9cvpTr0aOHcnNz9cILL1i38xsxYoSCgoL05JNPateuXYXac+bMGVu/4Fi/fn2heXfu3DnriAF/f/8L2v7XX3+tL7/8UtWqVbN+Yks6/5pz5swZjR49usjDyA8ePOjye+Lh4eGSpF9++aXIxwkPD1dSUlKRpxfYNWe3bdum3Nxc6+fMACA/zukGgGLEx8dbF7TKzMzUr7/+qk2bNmn//v0KCAjQ3Llzy1zMfvLJJ7rvvvt02WWXqXPnzqpTp44yMjKUkJCgL7/8Ul5eXpo3b57LYYn//Oc/1b17d91zzz36xz/+obi4OGVmZmrXrl1KSEiwCuLY2FjNnDlTEyZMUKtWrTRo0CAFBgZq5cqV2rNnj/r166ehQ4eWOe8bb7xRkyZN0l//+ldddtlluvHGG9WgQQOdPHlS+/fv15dffqlp06apefPmOnv2rDp37qzLL79c7du3V/369ZWenq7//Oc/SkxM1MMPP1zmQy1btWqlDRs2qGPHjurVq5dOnDihd955Rzk5OXrttdeKvDDb0KFD9eijj+ro0aNq3759ob3hZfXCCy8UebGzvP7o1KmTjDH605/+pBMnTuhf//qXdS7ubbfdppEjR2rRokWaM2eOJkyY4HL/Ll26aP78+dq6das6deqkn376Se+9954CAgIK/db4vHnztGfPHj366KPWmIeGhuqXX35RfHy89u3bp19//dVjzhlt0aKF5s2bpz//+c9q2rSpbrrpJsXGxur06dM6cOCAPv/8cw0fPlwLFiyo8GNcd911evfdd9W/f3+1bdtW3t7e6tu3r1q1alXi/W699VYtX75cq1ev1l133VXkOvPnzy/2goujRo3StddeW6HnVo8ePbRs2TLr/zxXXXWVAgMDdeLECTVt2tS6OF2eqKgoLVu2TLfffrtat26tG2+8Uc2aNVNWVpZ++uknff7557rmmmtsu0hk//79FRwcrE6dOqlBgwY6d+6cVq9ere+//1633XZbkRc6LE7+51RmZqb27dunlStXKicnR88995xL7vfee6+2bNmiv//979q4caN69eqlOnXq6NixY9q9e7e2bt2qpUuXqmHDhpLO96nD4dATTzyhXbt2KSQkRKGhoXrggQcknZ8z8fHx6tOnj7p06SJfX1917dpVXbt2tW3Orl692upDACikkq+GDgBulfcTOr179y7T+iX9ZFj+v+rVq5uYmBjTu3dvM2PGDHP06NFytWv37t3m+eefN9dff71p1KiR8ff3N/7+/iY2NtYMGzbM5Tdo80tMTDR/+ctfTOPGjY2vr68JDw83V199tZkzZ06hdVesWGG6detmgoKCjJ+fn2nZsqWZPXt2sT9lNWzYsBLbvHr1anPLLbeYqKgoU61aNVOrVi0TFxdn/vrXv1q/WZydnW1mzpxpbrjhBhMTE2N8fX1NdHS06dq1q1m6dGmZfzpH//tZqF9++cXccccdJjw83Pj7+5u4uDjz6aeflnjfoUOHGklmwYIFZXqs/Er7yTDl+5m5WbNmGUlm1KhRhbaTnp5uLr/8cuPr62u++eYbY4xrP+/cudPcdNNNJjg42AQGBppevXoVO+Znzpwxzz//vGnfvr0JDAw0AQEBplGjRqZ///5myZIlLuOZ1/7ilPSTYUWNf97cf/rppwstW7x4sZFkFi9eXGjZV199ZQYPHmzq1KljqlWrZiIjI027du3MxIkTzQ8//FCm7RfXrl9//dUMGjTIREZGGi8vr2LbUNDZs2dNeHi46dOnT6Flef1S0l/+xyjrcytP3u9OR0ZGFnoO3HDDDaX+JOHu3bvNyJEjTYMGDYyvr68JCwszLVu2NOPGjTNfffVVqX1WFkX9ZNi8efNM3759TYMGDYy/v7+JiIgwV111lZk/f77L72eXpKjnlJeXl4mKijJ9+vQx//nPf4q97zvvvGN69eplwsLCTLVq1UzdunVN9+7dzezZs82JEydc1n3zzTdNy5YtjZ+fn5Hk8hNhp0+fNqNHjza1a9c23t7eRc65ypiz+TVq1Mi0adOmTH0E4PfHYYwbrnACAEAladmypQ4ePKijR49W6KeN7PLTTz+pUaNGGjZsmN588013N+d3adKkSZoxY4b2799frr20QHmsWbNG119/vf7+97/rT3/6k7ubA8ADcU43AOCi9cknn2jnzp0aMmSIRxXc8AyPPvqowsPDy/3zUEB5TJkyRW3atCnXaTwAfl84pxsAcNGZP3++fvnlFy1cuFD+/v6aOHGiu5sEDxQUFKR//OMfio+PV25urnXVa6CyJCcnq2fPnrrlllvKfNFIAL8/HF4OALjoNGzYUIcPH1bTpk01c+ZM3Xzzze5uUiEcXg4AACSKbgAAAAAAbMNxMAAAAAAA2ISiGwAAAAAAm1zyF1JzOp06evSogoKC5HA43N0cAAAAAMAlwBij06dPq06dOiVeTPGSL7qPHj2qevXqubsZAAAAAIBL0C+//KKYmJhil1/yRXdQUJCk8x3Bb7gCAAAAACpDWlqa6tWrZ9Wcxbnki+68Q8qDg4MpugEAAAAAlaq005i5kBoAAAAAADah6AYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAAAAAJeYGjVquPxVq1ZNrVq1spb/+OOP6tOnj8LCwlS3bl09//zzxW7r0KFDhbbn4+Ojvn37Wut0795dfn5+LuscPXrU1hwvFhTdAAAAAHCJSU9Pd/lr3ry5Bg8eLEnKzc1V37591a5dOx0/flyfffaZXn31VS1durTIbdWvX99lW8nJyQoNDbW2l2fmzJku69WpU8f2PC8GFN0AAAAAcAn76quv9P3332v48OGSpD179mjPnj16+umnVa1aNTVt2lQjR47Ua6+9VqbtLV++XE6nUwMGDLCx1ZcOim4AAAAAuIQtWrRIffr0sfY8O51OSZIxxlrH6XRqx44dZd7ekCFD5O/v7xKfNm2awsPD1bZtWy1ZsqSSWn/xo+gGAAAAgEtURkaG3n77bY0aNcqKNW3aVA0bNtTkyZOVlZWlXbt26Y033lBaWlqp2/v555+1Zs0al+1J0vTp0/Xjjz/q2LFjmjFjhsaOHasPP/yw0vO5GFF0AwAAAMAl6r333lP16tX1hz/8wYpVq1ZNK1asUEJCgurWrashQ4ZoxIgRioiIKHV7ixcvVtu2bdW6dWuXeFxcnEJCQlStWjX17t1b9957r955551Kz+di5NaiOzc3V5MmTVKjRo0UEBCg2NhY/fWvf3U5zMEYo8mTJ6t27doKCAhQr169tG/fPje2GgAAAAAuDgsXLtSwYcPk4+PjEr/yyiv16aefKikpSdu3b1dWVpa6detW4racTqcWL15caC93Uby82L+bx609MXPmTM2fP1+vvvqqfvjhB82cOVPPP/+8/u///s9a5/nnn9crr7yiBQsWaOvWrQoMDFTv3r2VmZnpxpYDAAAAgGfbs2ePNm3apJEjRxZatmPHDmVkZCg7O1sffPCB3njjDT311FMlbm/16tVKSkrSnXfe6RJPSUnRxx9/rDNnzig3N1dr167VggULNHDgwErN52LlU/oq9tm0aZP69etnHerQsGFDLVu2TF999ZWk83u5X3rpJT311FPq16+fJGnJkiWKjo7W8uXLC12iHgAAAABw3qJFi9SlSxc1adKk0LJ3331X8+fPV2Zmplq3bq3ly5e7/I53nz591KVLFz3xxBMu27vtttsUEhLisq1z585pypQpVn3WsGFDzZkzR7fffrtNmV1cHCb/sdxV7LnnntNrr72mTz/9VJdffrm+/fZb3XDDDZozZ46GDBmiAwcOKDY2VgkJCWrTpo11v27duqlNmzZ6+eWXC20zKytLWVlZ1u20tDTVq1dPJ0+eVHBwsKTzhzp4eXnJ6XRaV+7LH8/NzXU5xL24uLe3txwOh3Jyclza4O3tLen84fNlifv4+MgY4xJ3OBzy9vYu1Mbi4uRETuRETuRETuRETuRETuRETuRUdTmlpaUpIiJCqampVq1ZFLfu6Z44caLS0tLUrFkzeXt7Kzc3V88++6yGDBkiSUpMTJQkRUdHu9wvOjraWlbQ9OnTNWXKlELxhIQEBQYGSpKioqIUGxurgwcP6sSJE9Y6MTExiomJ0d69e5WammrFGzdurJo1a2rnzp06e/asFW/WrJlCQ0OVkJDgMuCtWrWSr6+v4uPjXdrQoUMHZWdnu1yK39vbWx07dlRqaqp2795txQMCAtS6dWslJSXpwIEDVjwkJETNmzfX0aNHdfjwYStOTuRETuRETuRETuRETuRETuRETlWXU0ZGhsrCrXu63377bT3yyCOaNWuWrrzySm3fvl3jx4/XnDlzNGzYMG3atEmdO3fW0aNHVbt2bet+gwYNksPhKPJqeOzpJidyIidyIidyIidyIidyIidyIie7cyrrnm63Ft316tXTxIkTNWbMGCs2bdo0vfXWW9q9e3eFDi8vKC0tTSEhIaV2BAAAAAB4mhkJSe5ugltMbBvp7iaUqqy1pluvXn7mzBl5ebk2Ie9bCElq1KiRatWqpbVr11rL09LStHXrVsXFxVVpWwEAAAAAKC+3ntN9yy236Nlnn1X9+vV15ZVXKiEhQXPmzNHdd98t6fyhAOPHj9e0adPUpEkTNWrUSJMmTVKdOnXUv39/dzYdAAAAAIBSubXo/r//+z9NmjRJ999/v44fP646dero3nvv1eTJk611Hn30UWVkZOiee+5RSkqKrr32Wq1atUr+/v5ubDkAAAAAAKVz6zndVYFzugEAAABcrDin23NdFOd0AwAAAABwKaPoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAADAY9WoUcPlr1q1amrVqpW1fOzYsapXr56Cg4NVt25djR8/XtnZ2SVuc+HChWratKkCAwPVsGFDrVixwlpmjNH06dPVsGFDBQYG6vLLL9fWrVttyw+XPopuAAAAAB4rPT3d5a958+YaPHiwtfz+++/X7t27lZaWpm+//Vbffvutnn/++WK399prr2n27Nl6++23lZ6erq1bt6ply5bW8ieffFIfffSR1qxZo/T0dK1evVr169e3NUdc2nzc3QAAAAAAKIuvvvpK33//vYYPH27Fmjdvbv1vjJGXl5f27dtX5P1zc3M1efJkLVmyRG3btpUkRUdHW8uTk5M1Z84c7dixQ5dddpkkqUGDBjZkgt8T9nQDAAAAuCgsWrRIffr0UZ06dVziM2bMUI0aNVSzZk19++23Gjt2bJH337Nnj44dO6ZvvvlGDRs2VExMjEaPHq20tDRJ0pYtW+Tn56dly5apTp06atiwoR577LFSD1cHSkLRDQAAAMDjZWRk6O2339aoUaMKLZs4caLS09P1/fff67777lOtWrWK3EZycrIkac2aNYqPj9f27dt18OBBPfjgg9bytLQ07du3T3v37tUXX3yhTz75RDNnzrQvMVzyKLoBAAAAeLz33ntP1atX1x/+8Idi12nevLlat27tcvh5fjVq1JAkPf7444qMjFRkZKQef/xxrVy50mX5lClTVKNGDdWvX19/+ctfrOVARVB0AwAAAPB4Cxcu1LBhw+TjU/Jlqc6dO1fsOd1NmzaVv79/sfdt3br1BbURKApFNwAAAACPtmfPHm3atEkjR450iaenp2vx4sVKSUmRMUbfffedpk2bpt69exe5nYCAAA0dOlQzZ87UqVOnlJKSopkzZ6pfv36SpEaNGqlXr16aOnWqzpw5o6NHj+r//u//rOVARVB0AwAAAPBoixYtUpcuXdSkSROXuMPh0NKlSxUbG6ugoCD169dPf/jDH/TSSy9Z6/Tp00fPPfecdfull15SnTp11KhRIzVt2lQNGjTQnDlzrOX//Oc/lZqaqujoaHXs2FG9e/fWo48+anuOuHQ5jDHG3Y2wU1pamkJCQpSamqrg4GB3NwcAAAAAymxGQpK7m+AWE9tGursJpSprrcmebgAAAAAAbELRDQAAAACATUq+9B8AAAAAVAIOk8bvFXu6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJu4tehu2LChHA5Hob8xY8ZIkjIzMzVmzBhFRESoRo0aGjhwoI4dO+bOJgMAAAAAUGZuLbq//vpr/frrr9bf6tWrJUm33367JOnBBx/UypUr9d577+nzzz/X0aNHNWDAAHc2GQAAAACAMvNx54NHRUW53J4xY4ZiY2PVrVs3paamatGiRVq6dKmuu+46SdLixYvVvHlzbdmyRZ06dXJHkwEAAAAAKDOPOac7Oztbb731lu6++245HA5t27ZN586dU69evax1mjVrpvr162vz5s1ubCkAAAAAAGXj1j3d+S1fvlwpKSkaPny4JCkxMVG+vr4KDQ11WS86OlqJiYnFbicrK0tZWVnW7bS0NElSTk6OcnJyJEleXl7y8vKS0+mU0+m01s2L5+bmyhhTatzb21sOh8Pabv64JOXm5pYp7uPjI2OMS9zhcMjb27tQG4uLkxM5kRM5kRM5kRM5kRM5eXJOkiTjlCNfW4zDITm8io07jFNyiXtJDkfxcadrG43j/D5Gh3GWLe7lLRnjGnc4zq9fbLzknArWIOUdJ0/MqWDcjnHKycnx+OdTwedQcTym6F60aJH69OmjOnXqXNB2pk+frilTphSKJyQkKDAwUNL5w9pjY2N18OBBnThxwlonJiZGMTEx2rt3r1JTU61448aNVbNmTe3cuVNnz5614s2aNVNoaKgSEhJcBrxVq1by9fVVfHy8Sxs6dOig7Oxs7dixw4p5e3urY8eOSk1N1e7du614QECAWrduraSkJB04cMCKh4SEqHnz5jp69KgOHz5sxcmJnMiJnMiJnMiJnMiJnDw5JylKwWdOKjjjt7ZnBITqVFAdhaUnKvBsihVPC4xSWmCUIlJ/kX92hhU/FVRbGQFhij51UD45v+1oSwqtr0zfGqqTvE+OfAVUYniscr18VDdpj0tORyKbytuZo1rJP1ox4+WlI5HN5H8uQ5Eph6x4jo+fEsNjFZiZorDTv1rxTN9AJYU2KDWn+HhfSRUfJ0/MKY+d4xQf7+vxz6eMjN9yLonD5C/f3eTnn39W48aN9cEHH6hfv36SpM8++0w9e/bUqVOnXPZ2N2jQQOPHj9eDDz5Y5LaK2tNdr149nTx5UsHBwZIu/m8JL8VvPsmJnMiJnMiJnMiJnMjp0s5p1o5THrEHtcS4DXuFJ7SOkFTxcZq5zfXXmzwhp4JxO8ZpQusIj38+paWlKSIiQqmpqVatWRSPKLqfeeYZ/e1vf9Mvv/wiH5/zO99TU1MVFRWlZcuWaeDAgZKkPXv2qFmzZtq8eXOZL6SWlpamkJCQUjsCAAAAgH1mJCS5uwluMbFt5AXdn37zXGWtNd1+eLnT6dTixYs1bNgwq+CWzh8GMHLkSD300EMKDw9XcHCwxo4dq7i4OK5cDgAAAAC4KLi96F6zZo0OHTqku+++u9CyF198UV5eXho4cKCysrLUu3dvzZs3zw2tBAAAAACg/NxedN9www0q7gh3f39/zZ07V3Pnzq3iVgEAAAAAcOE85ne6AQAAAAC41FB0AwAAAABgE4puAAAAAABsQtENAAAAAIBNKLoBAAAAALAJRTcAAAAAADah6AYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAAAAAIBNKLoBAAAAALAJRTcAAAAAADah6AYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAAAAAIBNKLoBAAAAALAJRTcAAAAAADah6AYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAAAAAIBNKLoBAAAAALAJRTcAAAAAADah6AYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAAAAAIBNKLoBAAAAALAJRTcAAAAAADah6AYAAAAAwCYU3QAAAAAA2MTtRfeRI0c0dOhQRUREKCAgQC1btlR8fLy13BijyZMnq3bt2goICFCvXr20b98+N7YYAAAAAICycWvRferUKXXu3FnVqlXTJ598ou+//16zZ89WWFiYtc7zzz+vV155RQsWLNDWrVsVGBio3r17KzMz040tBwAAAACgdD7ufPCZM2eqXr16Wrx4sRVr1KiR9b8xRi+99JKeeuop9evXT5K0ZMkSRUdHa/ny5Ro8eHCVtxkAAAAAgLJy657uf//73+rQoYNuv/121axZU23bttXrr79uLT948KASExPVq1cvKxYSEqKrr75amzdvdkeTAQAAAAAoM7fu6T5w4IDmz5+vhx56SE888YS+/vprjRs3Tr6+vho2bJgSExMlSdHR0S73i46OtpYVlJWVpaysLOt2WlqaJCknJ0c5OTmSJC8vL3l5ecnpdMrpdFrr5sVzc3NljCk17u3tLYfDYW03f1yScnNzyxT38fGRMcYl7nA45O3tXaiNxcXJiZzIiZzIiZzIiZzIiZw8OSdJknHKka8txuGQHF7Fxh3GKbnEvSSHo/i407WNxnF+H6PDOMsW9/KWjHGNOxzn1y82XnJOBWuQ8o6TJ+ZUMG7HOOXk5Hj886ngc6g4bi26nU6nOnTooOeee06S1LZtW+3cuVMLFizQsGHDKrTN6dOna8qUKYXiCQkJCgwMlCRFRUUpNjZWBw8e1IkTJ6x1YmJiFBMTo7179yo1NdWKN27cWDVr1tTOnTt19uxZK96sWTOFhoYqISHBZcBbtWolX19flwvCSVKHDh2UnZ2tHTt2WDFvb2917NhRqamp2r17txUPCAhQ69atlZSUpAMHDljxkJAQNW/eXEePHtXhw4etODmREzmREzmREzmREzmRkyfnJEUp+MxJBWf81vaMgFCdCqqjsPREBZ5NseJpgVFKC4xSROov8s/OsOKngmorIyBM0acOyifntx1tSaH1lelbQ3WS98mRr4BKDI9VrpeP6ibtccnpSGRTeTtzVCv5RytmvLx0JLKZ/M9lKDLlkBXP8fFTYnisAjNTFHb6Vyue6RuopNAGpeYUH+8rqeLj5Ik55bFznOLjfT3++ZSR8VvOJXGY/OV7FWvQoIGuv/56LVy40IrNnz9f06ZN05EjR3TgwAHFxsYqISFBbdq0sdbp1q2b2rRpo5dffrnQNova012vXj2dPHlSwcHBki7+bwkvxW8+yYmcyImcyImcyImcyOnSzmnWjlMesQe1xLgNe4UntI6QVPFxmrntmMflVDBuxzhNaB3h8c+ntLQ0RUREKDU11ao1i+LWPd2dO3fWnj2u39Ds3btXDRo0kHT+omq1atXS2rVrraI7LS1NW7du1Z///Ocit+nn5yc/P79CcR8fH/n4uKab13EF5Q1uWeMFt1uRuMPhKDJeXBvLGycnciouTk7kJJFTcW0sb5ycyEkip+LaWN44OV2aOcnhJeMoYuPFxM8XaeWIexWdq3GUI+5wlDNeck4F+6e84+SJORV+zMofp/z95qnPp+KeKwW5teh+8MEHdc011+i5557ToEGD9NVXX+m1117Ta6+9Jul8R44fP17Tpk1TkyZN1KhRI02aNEl16tRR//793dl0AAAAAABK5daiu2PHjvrwww/1+OOPa+rUqWrUqJFeeuklDRkyxFrn0UcfVUZGhu655x6lpKTo2muv1apVq+Tv7+/GlgMAAAAAUDq3ntNdFdLS0hQSElLqcfYAAAAA7DMjIcndTXCLiW0jL+j+9JvnKmut6dbf6QYAAAAA4FJG0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAKrI8OHD5evrqxo1alh/mzdvLrTe2bNnddlllyk0NLTYbR0/flxDhgxRTEyMgoOD1bZtW/373/8uct2dO3fK19dX/fv3r6RMAJQVRTcAAABQhe6//36lp6dbf3FxcYXWmTx5sho0aFDidtLT09W2bVtt2bJFKSkpmjp1qu688059//33Lus5nU6NHj1anTt3rtQ8AJQNRTcAAADgQbZt26ZVq1bpscceK3G9xo0b6+GHH1ZMTIy8vLx0yy23qGnTptqyZYvLeq+88oqaN2+ubt262dlsAMWg6AYAAACq0JIlSxQeHq4rr7xSs2fPltPptJbl5ORo9OjRmjt3rnx9fcu13ePHj+uHH35Qq1atrNjPP/+sl19+WbNmzaq09gMoH4puAAAAoIqMGzdOe/bs0YkTJ7Ro0SK9/PLLevnll63ls2bNUtu2bdW1a9dybTc7O1uDBw/WoEGD1KFDByt+7733aurUqYqIiKi0HACUD0U3AAAAUEXatWunqKgoeXt7q1OnTpo4caLeeecdSdL+/fu1YMGCcu+Vzs7O1m233abq1avr9ddft+JvvfWWcnJy9Mc//rFScwBQPj7ubgAAAADwe+Xl9ds+sA0bNujYsWO6/PLLJUnnzp3T6dOnFRkZqY8++khXX311oftnZ2fr9ttvV3Z2tlasWOFySPqaNWu0detWRUZGSpLOnDmj3Nxc1apVS4mJiTZnBiAPe7oBAACAKvLuu+8qLS1NxhjFx8drxowZGjhwoCRp0KBB2r9/v7Zv367t27dr4cKFCgoK0vbt29W2bdtC2zp37pwGDRqkjIwMLV++XH5+fi7LX3zxRf3www/W9u677z716NFD27Ztq5JcAZzHnm4AAACgirz66qu65557lJOTo7p16+r+++/XhAkTJEnVq1dX9erVrXWjoqLkcDgUExNjxfr06aMuXbroiSee0KZNm7RixQr5+/tbe7Ml6YknntATTzyhsLAwhYWFWfHg4GD5+/urbt26VZApgDwOY4xxdyPslJaWppCQEKWmpio4ONjdzQEAAAB+l2YkJLm7CW4xsW1k6SuVgH7zXGWtNTm8HAAAAAAAm3B4OQAAAFAOv9c9j9LFsfcR8DTs6QYAAAAAwCYU3QAAAAAA2ISiGwAAAAAAm1B0AwAAAABgE4puAAAAAABsQtENAACAchs+fLh8fX1Vo0YN62/z5s3W8nPnzumBBx5QWFiYwsPDNXbsWOXk5BS7vR9//FF9+vRRWFiY6tatq+eff95l+W233abatWsrODhYjRo10rRp02zLDQAqE0U3AAAAKuT+++9Xenq69RcXF2ctmzZtmjZs2KDvv/9eu3bt0pdffqnnnnuuyO3k5uaqb9++ateunY4fP67PPvtMr776qpYuXWqt8/TTT+unn35SWlqaPv/8cy1dulRvvfWW7TkCwIWi6AYAAECle+ONN/TUU0+pdu3aql27tp588kktWrSoyHX37NmjPXv26Omnn1a1atXUtGlTjRw5Uq+99pq1TsuWLeXn5ydJcjgc8vLy0r59+6okFwC4EBTdAADgd60yD5POysrS6NGj1ahRIwUFBalZs2Z64403XNb5/vvv1bNnT4WFhalWrVq65557dObMGVtztMuSJUsUHh6uK6+8UrNnz5bT6ZQknTp1SocPH1abNm2sddu0aaNDhw4pNTW10Hby7meMcYnt2LHDZb37779f1atXV/369ZWenq7hw4dXflIAUMkougEAwO9eZR0mnZOTo9q1a2vNmjVKS0vTm2++qQkTJujTTz+11rnrrrvUtGlTHTt2TN99952+/fZb/fWvf7U9x8o2btw47dmzRydOnNCiRYv08ssv6+WXX5YkpaenS5JCQ0Ot9fP+P336dKFtNW3aVA0bNtTkyZOVlZWlXbt26Y033lBaWprLevPmzVN6erq+/vpr/elPf1JYWJg9yQFAJaLoBgAAKEF5DpMODAzU1KlTFRsbK4fDoU6dOqlHjx7asGGDtc6BAwc0dOhQ+fr6KioqSn379tV3331XVelUmnbt2ikqKkre3t7q1KmTJk6cqHfeeUeSVKNGDUly2aud939QUFChbVWrVk0rVqxQQkKC6tatqyFDhmjEiBGKiIgotK6Xl5c6dOigoKAgPfzww3akBgCViqIbAAD87lXWYdIFZWZm6quvvlKrVq2s2MMPP6wlS5bo7NmzSkxM1Icffqhbbrml0nOqal5ev32sDAsLU0xMjLZv327Ftm/frnr16ikkJKTI+1955ZX69NNPlZSUpO3btysrK0vdunUr9vHOnTvHOd0ALgoU3QAAj3f27FlddtllLoeqbtu2Tddee62Cg4PVuHFjLVmypMRtNGzYUAEBAdY5u/m3JZ0/l3T69Olq2LChAgMDdfnll2vr1q02ZFM1qqLPVq9erXbt2ikoKEhXXHGFVq1aZUMm9qvMw6TzM8Zo1KhRatKkiQYMGGDF+/Tpow0bNigoKEi1a9dWvXr1dPfdd1duUlXg3XffVVpamowxio+P14wZMzRw4EBr+YgRI/Tss88qMTFRiYmJeu655zRq1Khit7djxw5lZGQoOztbH3zwgXWEgST9/PPPev/995Weni6n06lNmzbplVdeUe/evW3PEwAuFEU3AMDjTZ48WQ0aNLBup6Sk6KabbtLQoUN16tQpLVu2TGPHjnU5hLcoy5Yts87ZTUlJcVn25JNP6qOPPtKaNWuUnp6u1atXq379+nakUyXs7rMDBw7o1ltv1dSpU5Wamqrnn39eAwcO1IEDB+xKyTaVeZh0HmOM7r//fu3Zs0fLly+39gKfOnVKvXr10ujRo3XmzBklJycrMDBQQ4cOtSs927z66quqX7++goKCNGTIEN1///2aMGGCtXzSpEmKi4tT8+bN1bx5c3Xu3FlPPPGEtfy+++7TfffdZ91+9913Vb9+fYWFhemFF17Q8uXLXY4QeOmllxQTE6PQ0FDdfffdGjt2rCZOnFg1yQLABfBxdwMAACjJtm3btGrVKs2ePVuDBg2SJG3atEl+fn7WB/arr75aAwYM0MKFC3XttdeW+zGSk5M1Z84c7dixQ5dddpkkuRSsF5uq6LNVq1apXbt2uvnmmyVJN998s6666iotWbJEzzzzTKXl4g7FHSYdGxsrqfTDpI0xGjNmjLZu3aq1a9e6rPfjjz/q7NmzGjdunBwOh3x9fXXvvfeqT58+9iZlgy+++KLE5dWqVdPcuXM1d+7cIpcvWLDA5fa0adM0bdq0Itdt0KCBvvzyy4o1FADcjD3dAACPlZOTo9GjR2vu3Lny9fW14k6n0+WnhfJiBX9eqKB7771XkZGRiouL08cff2zFt2zZIj8/Py1btkx16tRRw4YN9dhjjyk7O7tyE6oCVdVnFd2eJ6rsw6QfeOABbdy4UatXry50de1mzZqpRo0amjdvnnJycnT69Gm9/vrratu2rW35AQDciz3dAACPNWvWLLVt21Zdu3bV+vXrrXhcXJwyMjL06quv6t5779VXX32lDz/8UDVr1ix2W//4xz/Uvn17eXt76/3339fAgQP1xRdfqGPHjkpOTlZaWpr27dunvXv3Kjk5WTfffLNq1KihSZMmVUGmlaeq+uz666/Xww8/rOXLl+vmm2/Wf/7zH23cuFHdu3e3P8lK9uqrr+qee+5RTk6O6tatW+Rh0idPnlTz5s0lSUOHDi10mLR0fs/tzz//rHnz5snPz8/laImhQ4dqwYIFqlGjhlauXKnHHntMTz75pLy9vdW5c2f9/e9/r6JsC5uRkOS2x3aniW0j3d0EAL8TDlPwa+oq9Mwzz2jKlCkusaZNm2r37t2Szl/xc8KECXr77beVlZWl3r17a968eYqOji7zY6SlpSkkJESpqakKDg6u1PYDAOyzf/9+9ezZUwkJCQoPD9f69evVv39/67zijRs36pFHHtGePXt0xRVXqF27dtqyZUuZL342aNAgNW7cWDNmzNDy5ct16623av/+/dYhxIsWLdLf/vY3ffXVV3alWOmqss8kacWKFXrmmWf0888/q3PnzgoJCdG5c+es86FxcaDoLr/fa59J9FtFXOgXPPSb5yprren2Pd1XXnml1qxZY9328fmtSQ8++KA++ugjvffeewoJCdEDDzygAQMGaOPGje5oKgCgCm3YsEHHjh3T5ZdfLun8zwOdPn1akZGR+uijj9S5c2dt2rTJWv+OO+4o8eeFCsp/3m7r1q0rr+FuVJV9Jkn9+vVTv379rNtXX321hg0bdoFZAABwaXF70e3j46NatWoViqempmrRokVaunSprrvuOknS4sWL1bx5c23ZskWdOnWq6qYCAKrQoEGD1KtXL+v25s2bNWrUKG3fvl01a9ZUQkKCrrjiCjmdTr311ltav369EhISitzWoUOH9NNPP+nqq6+Wl5eXPvzwQ61YsULr1q2TJDVq1Ei9evXS1KlTNX/+fKWkpOj//u//dPvtt1dJrpWlKvtMkuLj49WmTRudPXtWL774opKTkym6AQAowO1F9759+1SnTh35+/srLi5O06dPV/369bVt2zadO3fO5cNDs2bNVL9+fW3evLnYojsrK0tZWVnW7bS0NEnnLyyTk5Mj6fw39V5eXnI6nXI6nda6efHc3FyXi8MUF/f29pbD4bC2mz8uSbm5uWWK+/j4yBjjEnc4HPL29i7UxuLi5ERO5EROl1pOvr6+qlWrlhWPiIiQw+Gwvqh9+eWXtXz5cuXk5CguLk6ffvqpatasKafTKS8vL1155ZV67LHHdNdddyklJUXjxo3T/v375ePjoyZNmmjZsmXq0KGDjDFyOBz6+9//rj//+c+Kjo5WcHCwhgwZokceeaTMuXrCOOX1Wd44hYeHu/TZK6+8og8//LBQn+Vts0WLFqX2WceOHSWdv2jaxIkT9dVXX8nhcKhXr15at26dAgICXPqsKufe7G9PWnHjOL9X3mGcLusbL2/JGNe4w3F+/WLjTjnytcU4HFIJcYdxSi5xL8nhKD7udJ1Lxba9mPhj7aMvaO45nLkel1NVjNOFzL3zfeZ5OVXFOOXvh/K+7p3fmOfl5BK3YZwK1iDlnXuemFPBuB3jlJOT49HvuXltLAu3ntP9ySefKD09XU2bNtWvv/6qKVOm6MiRI9q5c6dWrlypESNGuBTQknTVVVepR48emjlzZpHbLOo8cUlas2aNAgMDJUlRUVGKjY3Vjz/+qBMnTljrxMTEKCYmRj/88IPL73E2btxYNWvW1LfffquzZ89a8WbNmik0NFRff/21y4C3atVKvr6+io+Pd2lDhw4dlJ2d7XJlV29vb3Xs2FEpKSnWueySFBAQoNatW+v48eMuv3kaEhKi5s2b6/Dhwzp8+LAVJydyIqeLI6fMzEyNHDlSJ0+e1KpVq5SYmKi77rpL0m+H7mZmZiouLk6zZs0qMqeDBw/qpZde0t69e1WtWjVdc801Gj9+vPz9/ZWTk6P58+frs88+U1pamurWravRo0drwIABjBM5XfI57U/97WrzRyKbytuZo1rJP1ox4+WlI5HN5J+drsiUQ1Y8x8dPieGxCjx7SmGnf7Ximb6BSgptoOCMEwrO+K3tGQGhOhVUR2GnjyrwbIoVTwuMUlpglCJTfpZ/doYVPxVUWxkBYaqV/KN8cn77XJMUWl+ZvjVUN2m3HPk+8CWGxyrXy0d1k/Yov+Jy+uP1117QOO1Pzfa4nKpinPr5n6jw3Nufmu2ROVXFOF0Wcv5XESryGrEiM8ojc8pj1zjl9VlFX/f+sXqDx+WUx85xuizE1+PfnzIyMtSrV69Sz+l2a9FdUEpKiho0aKA5c+YoICCgQkV3UXu669Wrp5MnT1odcTHu8SktTk7kVNU5nT17Vm3btlVSUpJSUlL0008/qUWLFi7bz8zMVJ8+ffThhx8Wm9OiRYs0Z84cHT58WFFRUZozZ4769u2rrKws3XTTTfrhhx+UmZmpOnXq6C9/+YtGjx5tW04F22jHOD322GPavn27tm3bpqQk1wujeHt7Kzs7W3Xr1tWLL76oO++8s8ic2rdvr2uuuUYvv/yyTp06pVtuuUU9evTQs88+q4MHD2rFihUaNGiQatWqpY8//lhDhgzR1q1b1aJFi0ti7lXFOJHTxZkTe7orNk6zvz3pcTlVxTg90iqswnPvfJ95Xk5VMU4TWkdY8fK+Rszaccojc3KJ2zBOeX1W0de9mduOeVxOBeN2jNOE1hEe//6UlpamiIgIz7+QWn6hoaG6/PLLtX//fl1//fXKzs5WSkqKQkNDrXWOHTtW5Dngefz8/OTn51co7uPj43KRNum3jisob3DLGi+43YrEHQ5HkfHi2ljeODmRU3HxiuY0depUNWjQwCocGzZsqPT0dGu97Oxs1alTR3feeWeR2/f29tZrr72ml156SW+//bbatGmj48ePKyMjQz4+PnI4HHr11VfVvHlz+fj46Pvvv1ePHj3UokULdenSxZacyhqv6Dht27ZNn376qWbPnq1BgwYV2cYVK1bI6XTq9ttvd1meP6eDBw9q/vz58vX1VXR0tPr166fNmzdbhwA//PDD1v369++vpk2b6quvvlKLFi3cMve46mr559jz3ybb2jZPltdvFZl7xqvwfYyjiO04HOWMe8k4injQYuLnP1SWI15Eu8+vX/b4hbzuuTy+B+Vk9zhdyOte/vw8KaeqGKeC86y8c88Tcyrclsodp4L9U96554k5FX7Myh+n/P3mqZ/3intPKvQYZVqriqSnp+vHH39U7dq11b59e1WrVk1r1661lu/Zs0eHDh1SXFycG1sJYNu2bVq1apUee+yxYtdZvny5nE6nBgwYUOTy3NxcTZ48WS+//LLatm0rh8Oh6OhoNW7cWNL5F7SWLVtaL2YOh0MOh0P79++v/ISqQE5OjkaPHq25c+fK19e32PUWLVqkIUOGyN/fv9h1Hn74YS1ZskRnz55VYmKiPvzwQ91yyy1Frnv8+HH98MMPatWq1QXnAAAAgPJza9H98MMP6/PPP9dPP/2kTZs26dZbb5W3t7fuvPNOhYSEaOTIkXrooYe0bt06bdu2TSNGjFBcXBxXLgfcqLKKxz179ujYsWP65ptv1LBhQ8XExGj06NHWxQ/z3HzzzfL399cVV1yh6Oho3XrrrZWaT1WZNWuW2rZtq65duxa7zs8//6w1a9Zo1KhRJW6rT58+2rBhg4KCglS7dm3Vq1dPd999d6H1srOzNXjwYA0aNEgdOnS44BwAAABQfm4tug8fPqw777xTTZs21aBBgxQREaEtW7YoKipKkvTiiy/q5ptv1sCBA9W1a1fVqlVLH3zwgTubDPzuVVbxmJx8/vDZNWvWKD4+Xtu3b9fBgwf14IMPuqz3n//8RxkZGVq/fr0GDhyogICAykmkCu3fv18LFizQrFmzSlxv8eLFatu2bYm/GX3q1Cn16tVLo0eP1pkzZ5ScnKzAwEANHTrUZb3s7Gzddtttql69ul5//fVKyQMAAADl59Zzut9+++0Sl/v7+2vu3LmaO3duFbUIQEnyisfiftc3T1mKxxo1akiSHn/8cUVGRlr/5108LD9vb29169ZN7777rmbNmqWnnnrqArKoehs2bNCxY8d0+eWXS5LOnTun06dPKzIyUh999JGuvvpqOZ1OLV68WI8//niJ2/rxxx919uxZjRs3Tg6HQ76+vrr33nvVp08fa53s7Gzdfvvtys7O1ooVK0o8IgEAAAD28qhzugF4tvzFY2RkpPr166e0tDRFRkZq69atkmQVj6UdIt20adMSz1suyrlz57Rv374Kt99dBg0apP3792v79u3avn27Fi5cqKCgIG3fvl1t27aVJK1evVpJSUlFfumQX7NmzVSjRg3NmzdPOTk5On36tF5//XVrO+fOndOgQYOUkZGh5cuXF3lhSQAAAFQdim4AZVaZxWNAQICGDh2qmTNn6tSpU0pJSdHMmTPVr18/SdL27du1evVqnT17Vjk5Ofroo4/0z3/+U71797Y9z8pWvXp16zcfY2JiFBUVJYfDoZiYGGsv9KJFi3TbbbcpJCSk0P379Omj5557TtL5IwRWrlypZcuWKTIyUg0bNlRKSor+/ve/S5I2bdqkFStWaOPGjYqMjFSNGjVUo0YN6/4AAACoWh71k2EAPFv16tVVvXp163b+4jFPacVjly5d9MQTT0iSXnrpJY0ZM0aNGjWSn5+f+vbtqzlz5kg6f8G2J554Qnv27JHD4VDDhg01Z84c3XXXXTZnab/u3bsrJSXFJfbuu+8Wu/4nn3zicrtz587asGFDket269bN5TclAQAA4F4VLrr379+vH3/8UV27dlVAQICMMXI4ivoxNwCXqgstHgMDA/Xmm28WuW6HDh309ddfX2gTAQAAALcqd9F98uRJ3XHHHfrss8/kcDi0b98+NW7cWCNHjlRYWJhmz55tRzsB25w9e1YtW7ZUUlKSVUB2795dmzdvVrVq1az19u7dqzp16hS5jZLWP3TokK644gqX9TMzM3XTTTfp3//+d+UnBNvMSEhydxPcYmLbSHc3AQAA4KJV7qL7wQcflI+Pjw4dOqTmzZtb8TvuuEMPPfQQRTcuOpMnT1aDBg2UlORaUM2cOVPjx48v83aKW79+/fpKT0+3bmdnZ6tOnToaPHhwRZt8wSgeAQAAgKpR7gupffrpp5o5c6bLOZyS1KRJE/3888+V1jCgKmzbtk2rVq3SY489VmWPuXz5cjmdTg0YMKDKHhMAAACAe5S76M7IyHC5kFKe5ORkfpoGF5WcnByNHj1ac+fOLfJ3jKdNm6bw8HC1bdtWS5YsKXV7ZV1/0aJFGjJkSLl/LgsAAADAxafcRXeXLl1cCgqHwyGn06nnn39ePXr0qNTGAXaaNWuW2rZtq65duxZaNn36dP344486duyYZsyYobFjx+rDDz8sdltlXf/nn3/WmjVrSv0NawAAAACXhnKf0/3888+rZ8+eio+PV3Z2th599FHt2rVLycnJ2rhxox1tBCrd/v37tWDBAiUkJBS5PC4uzvq/d+/euvfee/XOO+/o1ltvvaD1Fy9erLZt26p169aVkAUAAAAAT1fuPd0tWrTQ3r17de2116pfv37KyMjQgAEDlJCQoNjYWDvaiHI4e/asLrvsMoWGhhZaduzYMYWHh6tNmzYlbqNhw4YKCAhQjRo1VKNGjULbuueee9S0aVN5eXnppZdeqrS2V6UNGzbo2LFjuvzyyxUZGal+/fopLS1NkZGR2rp1a6H1vbzK91Qpan2n06nFixezlxsAAAD4HSnXnu5z587pxhtv1IIFC/Tkk0/a1SZcgOKuxC1JDzzwgNq2bauTJ0+Wup1ly5apf//+RS5r3bq17rjjjot6DgwaNEi9evWybm/evFmjRo3S9u3b5evrq48//ljdu3eXn5+f1q9frwULFuj1118vclspKSnatGlTqeuvXr1aSUlJuvPOO23NDQAAAIDnKNfuu2rVqmnHjh12tQUXqKQrca9YsULJycn64x//eMGPM2bMGPXs2fOivhBY9erVFRMTY/1FRUXJ4XAoJiZGDodDU6ZMUa1atRQWFqYHH3xQc+bM0e23327dv0+fPnruuecknf8yqrT1pfMXULvtttsUEhJSpbkCAAAAcJ9yn9M9dOhQLVq0SDNmzLCjPaig/FfidjqdLstSU1P10EMPadWqVWU+7/7ee+/VqFGj1KRJE02aNEk33XSTHc32GN27d1dKSookKSoqqshDzPP75JNPrP/Lsr4kvfvuuxfURgAAAAAXn3IX3Tk5OXrjjTe0Zs0atW/fXoGBgS7L58yZU2mNQ9nlvxL3+vXrXZY9+uijGj58uJo0aVKmovsf//iH2rdvL29vb73//vsaOHCgvvjiC3Xs2NGm1gMAAADApancRffOnTvVrl07SdLevXtdljkcjsppFcqlpCtxf/nll9q4caO++eabMm+vS5cu1v933XWXli9frvfff5+iGwAAAADKqdxF97p16+xoBy5A/itxS+fPMT59+rQiIyPVsWNHHThwQHXq1JEkZWVl6ezZs4qMjNR3332n2rVrl7r98l65u6rNSCh80bjfg4ltI93dBAAAAAClKHfRnd/hw4clSTExMZXSGFRMSVfiDg4OVlpamrXsvffe08KFC/Xf//5XNWvWLLStQ4cO6aefftLVV18tLy8vffjhh1qxYoXLly3Z2dlyOp1yOp3KyclRZmamfHx85ONzQdMJAAAAAC455d6F6XQ6NXXqVIWEhKhBgwZq0KCBQkND9de//rXQBbxQNUq6EndwcLDLsrCwMFWrVk0xMTHy9vaWJF155ZX65z//KUlKT0/XuHHjFBERoaioKL3wwgt699131alTJ+vxbrjhBgUEBOjLL7/UI488ooCAAE2bNs0tuQMAAACAJyv3rsknn3zSunp5586dJZ0/vPmZZ55RZmamnn322UpvJMon/5W4Cxo+fLiGDx/uEtu1a5f1/xVXXKHt27eXuP2CF2oDAAAAABSt3EX33//+dy1cuFB9+/a1Yq1atVLdunV1//33U3QDAAAAAPA/5S66k5OT1axZs0LxZs2aKTk5uVIa9XvFBcEAAAAA4NJS7nO6W7durVdffbVQ/NVXX1Xr1q0rpVEAAAAAAFwKyr2n+/nnn9cf/vAHrVmzRnFxcZLOXy37l19+0ccff1zpDQQAAAAA4GJV7j3d3bp10549e3TrrbcqJSVFKSkpGjBggPbs2aMuXbrY0UYAAAAAAC5KFfph5bp163LBNAAAAAAASlHuPd2LFy/We++9Vyj+3nvv6e9//3ulNAoAAAAAgEtBuYvu6dOnKzKy8NWma9asqeeee65SGgUAAAAAwKWg3EX3oUOH1KhRo0LxBg0a6NChQ5XSKAAAAAAALgXlLrpr1qypHTt2FIp/++23ioiIqJRGAQAAAABwKSh30X3nnXdq3LhxWrdunXJzc5Wbm6vPPvtMf/nLXzR48GA72ggAAAAAwEWp3Fcv/+tf/6qffvpJPXv2lI/P+bs7nU796U9/4pxuAAAAAADyKXfR7evrq3feeUfTpk3T9u3bFRAQoJYtW6pBgwZ2tA8AAAAAgItWhX6nW5KaNGmiJk2aKCcnR5mZmZXZJgAAAAAALgllPqd75cqVevPNN11izz77rGrUqKHQ0FDdcMMNOnXqVGW3DwAAAACAi1aZi+45c+YoIyPDur1p0yZNnjxZkyZN0rvvvqtffvlFf/3rX21pJAAAAAAAF6MyF927du3SNddcY93+17/+peuvv15PPvmkBgwYoNmzZ2vlypW2NBIAAAAAgItRmYvu06dPu/wO94YNG9SzZ0/r9pVXXqmjR49WbusAAAAAALiIlbnorlu3rn744QdJUnp6ur799luXPd8nT55U9erVK7+FAAAAAABcpMpcdN9+++0aP368/vGPf2j06NGqVauWOnXqZC2Pj49X06ZNbWkkAAAAAAAXozIX3ZMnT1bHjh01btw4bd++XW+99Za8vb2t5cuWLdMtt9xS4YbMmDFDDodD48ePt2KZmZkaM2aMIiIiVKNGDQ0cOFDHjh2r8GMAAAAAAFCVyvw73QEBAVqyZEmxy9etW1fhRnz99df629/+platWrnEH3zwQX300Ud67733FBISogceeEADBgzQxo0bK/xYAAAAAABUlTLv6bZLenq6hgwZotdff11hYWFWPDU1VYsWLdKcOXN03XXXqX379lq8eLE2bdqkLVu2uLHFAAAAAACUTZn3dNtlzJgx+sMf/qBevXpp2rRpVnzbtm06d+6cevXqZcWaNWum+vXra/PmzS7nk+eXlZWlrKws63ZaWpokKScnRzk5OZIkLy8veXl5yel0yul0WuvmxXNzc2WMKTXu7e0th8NhbTd/XJJyc3PLFPfx8ZExRg5nvrjDIePwkoyRwziLiDvlyNcW43BIJcQdxim5xL0kh6P4uNO1jcZx/vsZl7aUFPfyLqHtrvHc3Fx5e3sXOx6ljVNeWz0ppyoZJ6nCc8/qM0/LyeZxKjiXHA5HueaejNPjcqqKcco/zyr0ulewjR6QU1WMU05OTrFzrLS556k5/a/xto6T0+ms8Htu/nw9KSe7x0mSjDEuz7/yzD2HM9fjcqqKcbqQz3vn+8zzcqqKccrfD3mfYcs6985vzPNyconbME4Fa5Dyzj1PzKlg3I5xysnJKbV+qujrnlQ5NWHB96TiuLXofvvtt/XNN9/o66+/LrQsMTFRvr6+Cg0NdYlHR0crMTGx2G1Onz5dU6ZMKRRPSEhQYGCgJCkqKkqxsbE6ePCgTpw4Ya0TExOjmJgY7d27V6mpqVa8cePGqlmzpnbu3KmzZ89a8WbNmik0NFQJCQkuA96qVSv5+voqPj7epQ0dOnRQdna2duzYYcW8vb3VsWNHpaamqm7SHiue4+OnxPBYBWamKOz0r1Y80zdQSaENFHzmpIIzfmt7RkCoTgXVUVh6ogLPpljxtMAopQVGKSL1F/lnZ1jxU0G1lREQpuhTB+WT89uXFEmh9ZXpW0N1kvfJkW/yJYbHKtfLx6WNknQksqm8nTmqlfyjFTNeXjoS2Uz+5zIUmXKo1Jz27k1W8+bNdfToUR0+fNiKl3Wc6qZme1xOVTFOUlSF517d1GyPzMnucUpKStKBAweseEhISLnmXnBOiMflVBXjFB//W59V5HXPJzfb43KqinGKj/dVQECAWrduXe6556k5SfaPU1KSo8LvuXnvB56Wk93jJEUrNTVVu3fvtuLlmXt1U7M9LqeqGKcL+bxXNzXbI3OqinGKj/eV5PoZtqxzT4ryyJzy2DVOeX1W0VrDE3PKY+c4xcf7llo/VfR1T6qcmjAj47ecS+Iw+cv3KvTLL7+oQ4cOWr16tXUud/fu3dWmTRu99NJLWrp0qUaMGOGy11qSrrrqKvXo0UMzZ84scrtF7emuV6+eTp48qeDgYEmeu6f7+W+O/xa8CL59KjFejm/UHm4TeUHfPs3+9qTH5VQV4zSxXVSF557VZx6Wk93j9Gjr8Av65vOFHckel1NVjNOEVuFWvCKvezMTkjwup6oYpwmtIyr8rfuMb457ZE7/a7yt4/RI26gKv+fmvbZ5Wk52j9Nj7aMvaI/P7G9PelxOVTFOj7QKq/DnvfN95nk5VcU4TWgdYcXLu7dx1o5THpmTS9yGccrrs4rWGjO3uV5I2hNyKhi3Y5wmtI7w+D3daWlpioiIUGpqqlVrFsVte7q3bdum48ePq127dlYsNzdXX3zxhV599VX997//VXZ2tlJSUlz2dh87dky1atUqdrt+fn7y8/MrFPfx8ZGPj2u61mF8BeS/KntZ4gW3W5G4w+E4/4QovEDGUVTcS8ZRxMaLiZ+f4OWIF9UWqei2FBcvtu2u8bx+LW48Shungm31hJx+i9s7ThWdey594WE52TlO5Z1jheL/e0PwpJyqYpyKmk/lmnvlbvulMffy90V5556n5lS2+IWNU15/VOQ9t6h8PSGnUuOVME4Oh6PIvinL3HN9T/CcnOwepwv5vJc/P0/KqSrGqeA8K+/c88ScCrelcsepYP+Ud+55Yk6FH7Pyxyl/vxVXP1X0da8s8bKMU3HvSQWVq+h2Op1688039cEHH+inn36Sw+FQo0aNdNttt+mPf/zjb+dqlEHPnj313XffucRGjBihZs2a6bHHHlO9evVUrVo1rV27VgMHDpQk7dmzR4cOHVJcXFx5mg0AAAAAgFuUueg2xqhv3776+OOP1bp1a7Vs2VLGGP3www8aPny4PvjgAy1fvrzMDxwUFKQWLVq4xAIDAxUREWHFR44cqYceekjh4eEKDg7W2LFjFRcXV+xF1AAAAAAA8CRlLrrffPNNffHFF1q7dq169Ojhsuyzzz5T//79tWTJEv3pT3+qtMa9+OKL8vLy0sCBA5WVlaXevXtr3rx5lbZ9AAAAAADsVOaie9myZXriiScKFdySdN1112nixIn65z//eUFF9/r1611u+/v7a+7cuZo7d26FtwkAAAAAgLsUcXWDou3YsUM33nhjscv79Omjb7/9tlIaBQAAAADApaDMRXdycrKio6OLXR4dHa1Tp05VSqMAAAAAALgUlLnozs3NLfGS6N7e3oV+OxMAAAAAgN+zcl29fPjw4UX+BrYkZWVlVVqjAAAAAAC4FJS56B42bFip61TmlcsBAAAAALjYlbnoXrx4sZ3tAAAAAADgklPmc7oBAAAAAED5lHlP94ABA8q03gcffFDhxgAAAAAAcCkpc9EdEhJiZzsAAAAAALjkcE43AAAAAAA2KfM53QcOHJAxxs62AAAAAABwSSlz0d2kSROdOHHCun3HHXfo2LFjtjQKAAAAAIBLQZmL7oJ7uT/++GNlZGRUeoMAAAAAALhU8JNhAAAAAADYpMxFt8PhkMPhKBQDAAAAAABFK/PVy40xGj58uPz8/CRJmZmZuu+++xQYGOiyHr/TDQAAAADAeWUuuocNG+Zye+jQoZXeGAAAAAAALiX8TjcAAAAAADbhQmoAAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwiVuL7vnz56tVq1YKDg5WcHCw4uLi9Mknn1jLMzMzNWbMGEVERKhGjRoaOHCgjh075sYWAwAAAABQdm4tumNiYjRjxgxt27ZN8fHxuu6669SvXz/t2rVLkvTggw9q5cqVeu+99/T555/r6NGjGjBggDubDAAAAABAmfm488FvueUWl9vPPvus5s+fry1btigmJkaLFi3S0qVLdd1110mSFi9erObNm2vLli3q1KmTO5oMAAAAAECZecw53bm5uXr77beVkZGhuLg4bdu2TefOnVOvXr2sdZo1a6b69etr8+bNbmwpAAAAAABl49Y93ZL03XffKS4uTpmZmapRo4Y+/PBDXXHFFdq+fbt8fX0VGhrqsn50dLQSExOL3V5WVpaysrKs22lpaZKknJwc5eTkSJK8vLzk5eUlp9Mpp9NprZsXz83NlTGm1Li3t7ccDoe13fxx6fwXCWWJ+/j4yBgjhzNf3OGQcXhJxshhnEXEnXLka4txOKQS4g7jlFziXpLDUXzc6dpG4zj//YxLW0qKe3mX0HbXeG5urry9vYsdj9LGKa+tnpRTlYyTVOG5Z/WZp+Vk8zgVnEsOh6Ncc0/G6XE5VcU45Z9nFXrdK9hGD8ipKsYpJyen2DlW2tzz1Jz+13hbx8npdFb4PTd/vp6Uk93jJEnGGJfnX3nmnsOZ63E5VcU4XcjnvfN95nk5VcU45e+HvM+wZZ175zfmeTm5xG0Yp4I1SHnnnifmVDBuxzjl5OSUWj9V9HVPqpyasOB7UnHcXnQ3bdpU27dvV2pqqv71r39p2LBh+vzzzyu8venTp2vKlCmF4gkJCQoMDJQkRUVFKTY2VgcPHtSJEyesdWJiYhQTE6O9e/cqNTXVijdu3Fg1a9bUzp07dfbsWSverFkzhYaGKiEhwWXAW7VqJV9fX8XHx7u0oUOHDsrOztaOHTusmLe3tzp27KjU1FTVTdpjxXN8/JQYHqvAzBSFnf7Vimf6BioptIGCz5xUcMZvbc8ICNWpoDoKS09U4NkUK54WGKW0wChFpP4i/+wMK34qqLYyAsIUfeqgfHJ++5IiKbS+Mn1rqE7yPjnyTb7E8Fjlevm4tFGSjkQ2lbczR7WSf7RixstLRyKbyf9chiJTDpWa0969yWrevLmOHj2qw4cPW/GyjlPd1GyPy6kqxkmKqvDcq5ua7ZE52T1OSUlJOnDggBUPCQkp19wLzgnxuJyqYpzi43/rs4q87vnkZntcTlUxTvHxvgoICFDr1q3LPfc8NSfJ/nFKSnJU+D037/3A03Kye5ykaKWmpmr37t1WvDxzr25qtsflVBXjdCGf9+qmZntkTlUxTvHxvpJcP8OWde5JUR6ZUx67ximvzypaa3hiTnnsHKf4eN9S66eKvu5JlVMTZmT8lnNJHCZ/+e4BevXqpdjYWN1xxx3q2bOnTp065bK3u0GDBho/frwefPDBIu9f1J7uevXq6eTJkwoODpbkuXu6n//m+G/Bi+DbpxLj5fhG7eE2kRf07dPsb096XE5VMU4T20VVeO5ZfeZhOdk9To+2Dr+gbz5f2JHscTlVxThNaBVuxSvyujczIcnjcqqKcZrQOqLC37rP+Oa4R+b0v8bbOk6PtI2q8Htu3mubp+Vk9zg91j76gvb4zP72pMflVBXj9EirsAp/3jvfZ56XU1WM04TWEVa8vHsbZ+045ZE5ucRtGKe8PqtorTFzm+uvN3lCTgXjdozThNYRHr+nOy0tTREREUpNTbVqzaK4fU93QU6nU1lZWWrfvr2qVaumtWvXauDAgZKkPXv26NChQ4qLiyv2/n5+fvLz8ysU9/HxkY+Pa7rWYXwF5A1uWeMFt1uRuMPhOP+EKLxAxlFU3EvGUcTGi4mfn+DliBfVFqnothQXL7btrvG8fi1uPEobp4Jt9YScfovbO04VnXsufeFhOdk5TuWdY4Xi/3tD8KScqmKcippP5Zp75W77pTH38vdFeeeep+ZUtviFjVNef1TkPbeofD0hp1LjlTBODoejyL4py9xzfU/wnJzsHqcL+byXPz9PyqkqxqngPCvv3PPEnAq3pXLHqWD/lHfueWJOhR+z8scpf78VVz9V9HWvLPGyjFNx70kFubXofvzxx9WnTx/Vr19fp0+f1tKlS7V+/Xr997//VUhIiEaOHKmHHnpI4eHhCg4O1tixYxUXF8eVywEAAAAAFwW3Ft3Hjx/Xn/70J/36668KCQlRq1at9N///lfXX3+9JOnFF1+Ul5eXBg4cqKysLPXu3Vvz5s1zZ5MBAAAAACgztxbdixYtKnG5v7+/5s6dq7lz51ZRiwAAAAAAqDxFnGgBAAAAAAAqA0U3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATSi6AQAAAACwCUU3AAAAAAA2oegGAAAAAMAmFN0AAAAAANiEohsAAAAAAJtQdAMAAAAAYBOKbgAAAAAAbELRDQAAAACATdxadE+fPl0dO3ZUUFCQatasqf79+2vPnj0u62RmZmrMmDGKiIhQjRo1NHDgQB07dsxNLQYAAAAAoOzcWnR//vnnGjNmjLZs2aLVq1fr3LlzuuGGG5SRkWGt8+CDD2rlypV677339Pnnn+vo0aMaMGCAG1sNAAAAAEDZ+LjzwVetWuVy+80331TNmjW1bds2de3aVampqVq0aJGWLl2q6667TpK0ePFiNW/eXFu2bFGnTp3c0WwAAAAAAMrEo87pTk1NlSSFh4dLkrZt26Zz586pV69e1jrNmjVT/fr1tXnzZre0EQAAAACAsnLrnu78nE6nxo8fr86dO6tFixaSpMTERPn6+io0NNRl3ejoaCUmJha5naysLGVlZVm309LSJEk5OTnKycmRJHl5ecnLy0tOp1NOp9NaNy+em5srY0ypcW9vbzkcDmu7+eOSlJubW6a4j4+PjDFyOPPFHQ4Zh5dkjBzGWUTcKUe+thiHQyoh7jBOySXuJTkcxcedrm00jvPfz7i0paS4l3cJbXeN5+bmytvbu9jxKG2c8trqSTlVyThJFZ57Vp95Wk42j1PBueRwOMo192ScHpdTVYxT/nlWode9gm30gJyqYpxycnKKnWOlzT1Pzel/jbd1nJxOZ4Xfc/Pn60k52T1OkmSMcXn+lWfuOZy5HpdTVYzThXzeO99nnpdTVYxT/n7I+wxb1rl3fmOel5NL3IZxKliDlHfueWJOBeN2jFNOTk6p9VNFX/ekyqkJC74nFcdjiu4xY8Zo586d2rBhwwVtZ/r06ZoyZUqheEJCggIDAyVJUVFRio2N1cGDB3XixAlrnZiYGMXExGjv3r3WXndJaty4sWrWrKmdO3fq7NmzVrxZs2YKDQ1VQkKCy4C3atVKvr6+io+Pd2lDhw4dlJ2drR07dlgxb29vdezYUampqaqb9NtF5HJ8/JQYHqvAzBSFnf7Vimf6BioptIGCz5xUcMZvbc8ICNWpoDoKS09U4NkUK54WGKW0wChFpP4i/+zfzpU/FVRbGQFhij51UD45v31JkRRaX5m+NVQneZ8c+SZfYniscr18XNooSUcim8rbmaNayT9aMePlpSORzeR/LkORKYdKzWnv3mQ1b95cR48e1eHDh614Wcepbmq2x+VUFeMkRVV47tVNzfbInOwep6SkJB04cMCKh4SElGvuBeeEeFxOVTFO8fG/9VlFXvd8crM9LqeqGKf4eF8FBASodevW5Z57npqTZP84JSU5Kvyem/d+4Gk52T1OUrRSU1O1e/duK16euVc3NdvjcqqKcbqQz3t1U7M9MqeqGKf4eF9Jrp9hyzr3pCiPzCmPXeOU12cVrTU8Mac8do5TfLxvqfVTRV/3pMqpCfNfi6wkDpO/fHeTBx54QCtWrNAXX3yhRo0aWfHPPvtMPXv21KlTp1z2djdo0EDjx4/Xgw8+WGhbRe3prlevnk6ePKng4GBJnrun+/lvjv8WvAi+fSoxXo5v1B5uE3lB3z7N/vakx+VUFeM0sV1Uheee1WcelpPd4/Ro6/AL+ubzhR3JHpdTVYzThFbhVrwir3szE5I8LqeqGKcJrSMq/K37jG+Oe2RO/2u8reP0SNuoCr/n5r22eVpOdo/TY+2jL2iPz+xvT3pcTlUxTo+0Cqvw573zfeZ5OVXFOE1oHWHFy7u3cdaOUx6Zk0vchnHK67OK1hozt7n+cpMn5FQwbsc4TWgd4fF7utPS0hQREaHU1FSr1iyKW/d0G2M0duxYffjhh1q/fr1LwS1J7du3V7Vq1bR27VoNHDhQkrRnzx4dOnRIcXFxRW7Tz89Pfn5+heI+Pj7y8XFN1zqMr4C8wS1rvOB2KxJ3OBznnxCFF8g4iop7yTiK2Hgx8fMTvBzxotoiFd2W4uLFtt01ntevxY1HaeNUsK2ekNNvcXvHqaJzz6UvPCwnO8epvHOsUPx/bwielFNVjFNR86lcc6/cbb805l7+vijv3PPUnMoWv7BxyuuPirznFpWvJ+RUarwSxsnhcBTZN2WZe67vCZ6Tk93jdCGf9/Ln50k5VcU4FZxn5Z17nphT4bZU7jgV7J/yzj1PzKnwY1b+OOXvt+Lqp4q+7pUlXpZxKu49qSC3Ft1jxozR0qVLtWLFCgUFBVnnaYeEhCggIEAhISEaOXKkHnroIYWHhys4OFhjx45VXFwcVy4HAAAAAHg8txbd8+fPlyR1797dJb548WINHz5ckvTiiy/Ky8tLAwcOVFZWlnr37q158+ZVcUsBAAAAACg/tx9eXhp/f3/NnTtXc+fOrYIWAQAAAABQeYo40QIAAAAAAFQGim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATtxbdX3zxhW655RbVqVNHDodDy5cvd1lujNHkyZNVu3ZtBQQEqFevXtq3b597GgsAAAAAQDm5tejOyMhQ69atNXfu3CKXP//883rllVe0YMECbd26VYGBgerdu7cyMzOruKUAAAAAAJSfjzsfvE+fPurTp0+Ry4wxeumll/TUU0+pX79+kqQlS5YoOjpay5cv1+DBg6uyqQAAAAAAlJtbi+6SHDx4UImJierVq5cVCwkJ0dVXX63NmzcXW3RnZWUpKyvLup2WliZJysnJUU5OjiTJy8tLXl5ecjqdcjqd1rp58dzcXBljSo17e3vL4XBY280fl6Tc3NwyxX18fGSMkcOZL+5wyDi8JGPkMM4i4k458rXFOBxSCXGHcUoucS/J4Sg+7nRto3GcPyjCpS0lxb28S2i7azw3N1fe3t7Fjkdp45TXVk/KqUrGSarw3LP6zNNysnmcCs4lh8NRrrkn4/S4nKpinPLPswq97hVsowfkVBXjlJOTU+wcK23ueWpO/2u8rePkdDor/J6bP19PysnucZLO76zI//wrz9xzOHM9LqeqGKcL+bx3vs88L6eqGKf8/ZD3Gbasc+/8xjwvJ5e4DeNUsAYp79zzxJwKxu0Yp5ycnFLrp4q+7kmVUxMWfE8qjscW3YmJiZKk6Ohol3h0dLS1rCjTp0/XlClTCsUTEhIUGBgoSYqKilJsbKwOHjyoEydOWOvExMQoJiZGe/fuVWpqqhVv3LixatasqZ07d+rs2bNWvFmzZgoNDVVCQoLLgLdq1Uq+vr6Kj493aUOHDh2UnZ2tHTt2WDFvb2917NhRqampqpu0x4rn+PgpMTxWgZkpCjv9qxXP9A1UUmgDBZ85qeCM39qeERCqU0F1FJaeqMCzKVY8LTBKaYFRikj9Rf7ZGVb8VFBtZQSEKfrUQfnk/PYlRVJofWX61lCd5H1y5Jt8ieGxyvXycWmjJB2JbCpvZ45qJf9oxYyXl45ENpP/uQxFphwqNae9e5PVvHlzHT16VIcPH7biZR2nuqnZHpdTVYyTFFXhuVc3Ndsjc7J7nJKSknTgwAErHhISUq65F5wT4nE5VcU4xcf/1mcVed3zyc32uJyqYpzi430VEBCg1q1bl3vueWpOkv3jlJTkqPB7bt77gaflZPc4SdFKTU3V7t27rXh55l7d1GyPy6kqxulCPu/VTc32yJyqYpzi430luX6GLevck6I8Mqc8do1TXp9VtNbwxJzy2DlO8fG+pdZPFX3dkyqnJszI+C3nkjhM/vLdjRwOhz788EP1799fkrRp0yZ17txZR48eVe3ata31Bg0aJIfDoXfeeafI7RS1p7tevXo6efKkgoODJXnunu7nvzmev0M8/tunEuPl+Ebt4TaRF/Tt0+xvT3pcTlUxThPbRVV47ll95mE52T1Oj7YOv6BvPl/YkexxOVXFOE1oFW7FK/K6NzMhyeNyqopxmtA6osLfus/45rhH5vS/xts6To+0jarwe27ea5un5WT3OD3WPvqC9vjM/vakx+VUFeP0SKuwCn/eO99nnpdTVYzThNYRVry8extn7TjlkTm5xG0Yp7w+q2itMXPbMY/LqWDcjnGa0DrC4/d0p6WlKSIiQqmpqVatWRSP3dNdq1YtSdKxY8dciu5jx46pTZs2xd7Pz89Pfn5+heI+Pj7y8XFN1zqMr4C8wS1rvOB2KxJ3OBznnxCFF8g4iop7yTiK2Hgx8fMTvBzxotoiFd2W4uLFtt01ntevxY1HaeNUsK2ekNNvcXvHqaJzz6UvPCwnO8epvHOsUPx/bwielFNVjFNR86lcc6/cbb805l7+vijv3PPUnMoWv7BxyuuPirznFpWvJ+RUarwSxsnhcBTZN2WZe67vCZ6Tk93jdCGf9/Ln50k5VcU4FZxn5Z17nphT4bZU7jgV7J/yzj1PzKnwY1b+OOXvt+Lqp4q+7pUlXpZxKu49qdBjlGktN2jUqJFq1aqltWvXWrG0tDRt3bpVcXFxbmwZAAAAAABl49Y93enp6dq/f791++DBg9q+fbvCw8NVv359jR8/XtOmTVOTJk3UqFEjTZo0SXXq1LEOQQcAAAAAwJO5teiOj49Xjx49rNsPPfSQJGnYsGF688039eijjyojI0P33HOPUlJSdO2112rVqlXy9/d3V5MBAAAAACgztxbd3bt3V0nXcXM4HJo6daqmTp1aha0CAAAAAKByeOw53QAAAAAAXOwougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0AAAAAgE0ougEAAAAAsAlFNwAAAAAANqHoBgAAAADAJhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxyURTdc+fOVcOGDeXv76+rr75aX331lbubBAAAAABAqTy+6H7nnXf00EMP6emnn9Y333yj1q1bq3fv3jp+/Li7mwYAAAAAQIk8vuieM2eORo8erREjRuiKK67QggULVL16db3xxhvubhoAAAAAACXycXcDSpKdna1t27bp8ccft2JeXl7q1auXNm/eXOR9srKylJWVZd1OTU2VJCUnJysnJ8fahpeXl5xOp5xOp8u2vby8lJubK2NMqXFvb285HA5ru/njkpSbm1umuI+Pj4wxykpL+S3ocMg4vCRj5DDOIuJOOfK1xTgcUglxh3FKLnEvyeEoPu50baNxnP9+xqUtJcW9vEtou2v81ClveXt7FzsepY1TXr95Uk5VMU5pab4VnntWn3lYTnaPU0qKj8tccjgc5Zp7madTPS6nqhin5OTfvp+tyOte5uk0j8upKsYpOdmr2DlW2tzLPJ3qkTn9r/G2jlNKik+F33Pzv496Uk52j1Namq+MMS7Pv/LMvay0FI/LqSrG6dQp7wp/3jvfZ56XU1WMU/73hLzPsGWde5nppz0yJ5e4DeOU12cVrTVcagQPyalg3I5xSk72KrV+qujrnlQ5NWFaWtr59udbrygOU9oabnT06FHVrVtXmzZtUlxcnBV/9NFH9fnnn2vr1q2F7vPMM89oypQpVdlMAAAAAMDv1C+//KKYmJhil3v0nu6KePzxx/XQQw9Zt51Op5KTkxURESGHw+HGlnmutLQ01atXT7/88ouCg4Pd3ZyLBv1WfvRZxdBvFUO/lR99VjH0W8XQbxVDv5UffVYx9FvpjDE6ffq06tSpU+J6Hl10R0ZGytvbW8eOHXOJHzt2TLVq1SryPn5+fvLz83OJhYaG2tXES0pwcDBPqAqg38qPPqsY+q1i6Lfyo88qhn6rGPqtYui38qPPKoZ+K1lISEip63j0hdR8fX3Vvn17rV271oo5nU6tXbvW5XBzAAAAAAA8kUfv6Zakhx56SMOGDVOHDh101VVX6aWXXlJGRoZGjBjh7qYBAAAAAFAijy+677jjDp04cUKTJ09WYmKi2rRpo1WrVik6OtrdTbtk+Pn56emnny50WD5KRr+VH31WMfRbxdBv5UefVQz9VjH0W8XQb+VHn1UM/VZ5PPrq5QAAAAAAXMw8+pxuAAAAAAAuZhTdAAAAAADYhKIbAAAAAACbUHQDAAAAAGATim4AAAAAAGxC0Q0U4HQ63d0E/E589dVX2rt3r7ubgd+BDRs2KDs7W5LEj5aUXd77QV6f0Xewy9q1a7V//35JUm5urptbc3HJycmRxOc3eDaK7kvY559/ri1btri7GReVnJwceXmdf1p88803bm4NLlXGGB04cEDXX3+9PvnkE0l8yIJ9Tpw4oV69eumRRx6RJDkcDje36OKQkZFhvR9s2LBBEn0He5w5c0aDBg3SU089JUny9vbmC54yWrVqldq1a2d9fqPf4Kkoui9RR44c0ejRo/XOO+/IGMOLUBm89957euaZZyRJDz74oEaNGqWUlBS3tuliUNw3y8y54jkcDjVu3FhDhgzRtGnTdOzYMXl7e/MtfSmYUxUTHh6uJ554QuvXr9dnn33m7uZcFP71r3/pz3/+s3Jzc/Xggw/qtttuU1JSkrub5fGKeo7yvC2ZMUbVq1fXiy++qI0bN2rp0qWS+IKnrHJzc5WWlqZ58+ZJot/KgiN33IOi+xJVt25d/fGPf9Trr7+unTt3yuFw8IG+FGfOnNFzzz2nrl276o033tCbb76p0NBQdzfLozmdTmtP0I4dO/TZZ5/pxx9/1OnTp5lzJcjbq/3EE0+oYcOG+stf/qKzZ89afYnCnE6n9WEqPT1dWVlZ1jI+OJTM29tb/fr1kzFG77//vpxOJ31WisjISL311ltq37693nzzTa1Zs0aRkZH0WwnyP0eTk5N16tQpSRRBpcnrn65du6pVq1b64IMPlJiY6OZWXTy6du2qq666Su+8845++eUXSRxmXpL8z1OJ98+qxCe8S9hTTz2lTp066Z577lF6ejof6EsxbNgwderUSRs2bNBdd92lVq1aubtJHs0YY82pxx9/XHfeeaeGDh2qu+++W0OGDNHRo0eZcwWsWrVK3333nc6ePStJql27tm6//Xbt3LlT69atk8QbYHHy5tKzzz6r66+/Xn379tULL7wg6fyHVvrN1X//+1+9//771u3WrVvroYce0vz587V27Vr6rAROp1Pdu3fXbbfdph07dqh79+5q2LChJArIkuQ9R59++ml1795d3bt318MPP+zmVnmuDz74QH/729+s2w0bNtS9996r//znP/rvf/8rieKxOIcPH7b+DwoK0vPPP69du3bp5ZdfliQ+exQj/46S+fPn649//KNuu+02/fWvf3Vzy34nDC4ZK1euNMuXLzdHjx41xhjjdDrNO++8Y1q0aGHmzp1rcnNz3dxCz+N0Ol1uT5061UyaNMl4e3ubp556yuTk5BS5Hn7z4osvmqioKPPFF18YY4x54IEHTPXq1c2aNWvc3DLPsnv3buNwOEyTJk3MpEmTzLp164wxxmRlZZmrrrrKdO/e3VqX+fab/K9bL730komIiDBTp041I0aMMOHh4WbMmDHWcvrtfB8cOHDAOBwO43A4zJNPPmm++eYbc+7cOWOMMYMGDTItWrQwP//8s5tb6nkKzp/XX3/dvPbaa6ZatWpm+PDh5siRI2W63+9N/ufoggULTHR0tHnllVfM5MmTTXBwsLn99ttNdna2G1voWZxOpzly5Ij1HJ04caJ5//33TWZmpjHGmL/85S8mLCzM/PDDD25uqWd6//33TVhYmPnLX/7iMq/mzZtnQkNDzYoVK9zYuovDo48+amrXrm2efPJJ8+qrrxqHw2Huuecek5WV5e6mXdIoui8RP//8swkNDTW1atUyf/rTn6wXnaysLDNw4EDTqlUrk5qaaowxFN9FmD9/vvnkk0+svnnjjTeswjv/B6r169e7q4kex+l0mszMTHP77bebV155xRhjzEcffWRq1KhhXnvtNWOMMWfOnDEZGRnubKbHyMjIMNdff72pW7euefnll03Tpk3N7NmzjTHGfPfddyYiIsI888wzbm6l59qwYYP529/+Zj766CNjjDGnT582S5YsMf7+/ub++++31vu9F0B5nnjiCXPNNdeYuLg4M2LECPPYY4+ZrKwss2XLFtOuXTsza9YsCqFivPbaa+aVV14x6enpxhhj1qxZY3x8fMzw4cPNr7/+aq3373//211N9Ejr1q0zb775pnn33Xet2BdffGGioqLMbbfdZn3xg/Nmz55tGjdubG677TYzcuRIM3DgQHPq1Cmzd+9ec91115kxY8ZYn9vwm/fee8+EhISYsLAwc80115h3333XHDlyxKSlpZnevXubIUOGmMTERHc302Nt3brVXHbZZebzzz83xhizatUq4+/vb31ug30oui8RmZmZZvTo0aZp06Zm4cKFJiQkxEyaNMkcO3bMnDp1ytStW9f8+c9/dnczPVabNm1MkyZNzCeffGJ9EH3zzTeNj4+PeeSRR8yOHTtM3759Tc+ePflQX8DNN99sVq1aZRXc8+fPN8YYk52dbRYuXGhWrFhBn/3P/v37TZMmTcz8+fPN+vXrTf369c19991n/va3v5mnn37aXH/99Wbjxo3ubqZHyP8BffPmzcbhcJigoCCzevVqK3727Fnzj3/8w1SvXt088MAD7mimx8l7/Vq9erX585//bN5++23zzjvvmFtuucU0b97cfP3116Zfv36mZcuW5tixY8YYvogt6NZbb7WOEMsretauXWt8fX3N0KFDzapVq8wtt9xirrjiit/1a1tKSor1/65du6w9t0uWLHFZ78svvzRRUVFm0KBBfNFjjHUE3d69e83YsWPNrFmzzJdffmkGDhxoGjdubN577z0zZMgQ0759e7N582ZjDM/RvOdZ3lECDzzwgHnttdfMpEmTzB133GFuueUW8+2335qPPvrIBAUFWTue8voav1m5cqVp3769McaYDz74wNSoUcMsWLDAGHP+Of3JJ5+4s3mXNIruS8jx48dN7dq1zYIFC8yuXbtMjx49zODBg83MmTPNG2+8YVq2bMlhN6boNy+n02l69uxprrjiCvPxxx9bHwyWLVtmfHx8zJVXXmnatm37u/7AUFy/DRw40DRv3tyEhoaav/3tb9ayI0eOmF69ellF+O/Rl19+abZv327dzsrKMi+88IIZPny4ycnJMXv27DFPPvmk6d+/v6lfv75p2LChmTRp0u/+A9ann35qYmNjTXJysjHm/FyaNWuWCQkJMRMnTnRZ9+zZs+att94yDofDvPDCC+5orkf45JNPzIEDB1xio0aNMjfddJN1+/777zc33nijGTNmjHE4HGbIkCFV3UyPU9xzbcSIEaZNmzbm1VdftYrLdevWmfr165s2bdqYTp06We8Hv8fC+/PPPzdRUVFm27ZtxpjzX5K9++67Jjo62gwfPrzQ+hs2bLBOd/i9WrFihdm8ebN1GLkx509p69Onj0lLSzPGGDNlyhTzxz/+0QwePNg4HA5z/fXXu6u5HuX48eMutxcuXGiuvPJK89NPP5lffvnFPPXUU6Z69epm0aJFpn379iY2NrbQfXDeN998Y7p06WJefPFFExQUZBXcxpw/mrNv375m3759bmzhpYui+yL22WefFSqily1bZq6//nrz888/m8TERPP666+b7t27m+joaNOwYUMzZMgQ64Ps713eXp48TqfTdO/e3TRr1sx88skn1rkt+/btMxs3brQ+nP0eD5HL/8H066+/Nrt37zYHDx40xhhz9OhRc8UVV5gWLVqYjIwMc/r0aXPixAnTp08f07lz59/lN81Op9MkJyf/f3t3HpdT9scB/HvbbJkUCoXs2VqEok3Z2jD2QfaQQcYufvbdIDKWUmiMZIsxYxjrZLIky1iyJUsYW+TRptTz+f3RPNfzVKho4X7f/9Bdep2+r3vuOd97zz0H2trasLCwgI+PjxjDc+fOoVWrVggODgYAvHz5EtHR0ejZsycEQYCTk1NxFr1EuHr1KkxMTGBmZoaEhAQAwJMnT7B48WKULVsW8+fPVzk+JSUFBw8elGTdBIDHjx/D0tISmpqa8PX1xZUrV8R9DRo0wA8//CD+vGfPHkybNg2CIKBFixaSTBhzc/PmzRwPVQcOHAhzc3OsXbtWTIoePHiAa9euSbo9AICkpCTY2tqiVq1auHjxIoCsWGzbtg2lS5eGt7d3jnMuXbokyfYAAGQyGdq3bw9BEDB+/HiVT9VatmyJbt26iT+fPHkSa9asgSAIqFu3ruQfwoaHh6NatWoICAhAamqquH3gwIFo1aqV+BnIvn374O7ujmbNmkEQBKxYsaK4ilwivO+6iY2NhYODA7S0tDBz5kxxe2pqKtzc3NCnTx9uFwoJJ91fILlcjuTkZNSrVw8mJibo16+feNN58OABevfujaVLlwLIGmqYmJiIESNGQBAEtGnThisTsibcqF27Ni5cuJBjn5WVFUxMTLB//36VGzzAQ5UmT56MatWqoUqVKmjTpg3CwsIAZA29rFSpEkxMTNC4cWO0bt0azZo1EzuxUouboo7duXMH48ePR7169dCkSRPxG6qQkBCUL18et27dUjlvx44dkp68T/lvvn79OszNzdGoUSOVxHvJkiXQ0dHBggULcv0dUk2Cnj17hpkzZ8LQ0BAODg7YsGEDAOCPP/5A165dVYYMpqen48yZM2KspHitKdu6dSvq1KmDXbt2qVw/mZmZ6NatG6pVq4Z169apDKdW7Jca5WslOTkZbdu2hZGRkZh4Z2RkiIn32LFjc/0dUq2jALBmzRq0bNkSdevWFe9h169fR9u2bREUFKRybExMjNgeSPFaUzhx4gS+//57qKmpoUePHti2bRsA4MWLF+jevTtWrVol9jXu3r2LzZs3o1+/fpK+zpTrqZ+fH0aPHg1PT0/xZdP+/ftRvXp19OrVC+vXr8e2bdvQtm1bNG3aVIyblK+5wsJJ9xdIUREeP36Mn376CfXr14eRkRE2bdqE169f4+jRo9DW1haHfSkcPXpU0h16ZTKZDDVq1EDLli3FxFsRk7///hvq6uowNjYWv6eSoszMTJXr5NSpU6hVqxYiIiIQEhICT09PVK9eHbt37wYAJCQkwNfXF8uXL8e2bdvEa01qDV/2hio9PR3nzp1Dhw4dUKlSJQwbNgyXLl3CjBkzMGjQoFwnypFazADVuCkedt24cQMWFhY5Eu+lS5dCT08vx1BzqcntPr5//34MGzYMGhoaGDJkCHbu3Im+ffti0aJF4jnK50nxWssuMTERDg4OsLKyQlhYmEpMbt++jQoVKqBWrVrivU6qlOtofHw8gKy62q5dO1SvXl1MvDMzM7Ft2zaUK1cOAwcOLIaSlhyKuqYcu8jISMyePRsaGhpwdnaGv78/pk+fDh8fHzx9+hRyuVzlQbXUHlorZP+7jx8/DhsbGzRq1AgDBgxAQkIC5s6di759++L58+e5/g4p3t+Ur7WZM2eiQoUK6N27N+rUqQMjIyP8/fffALK+5+7duzd0dXXFz1Gl+qKkqHDS/YVRrkyK74Li4+PRv39/GBsbo3379oiOjsbcuXPRpk0bsWFUJrXKdP78eezcuRNTp07Fjh07cPbsWQBZHa26deuiWbNmKm+8Dx8+jLFjx+L777+XXKzeZ9OmTfD29sacOXPEbdeuXYOXlxeMjIwQGhqa63lSi59y/Vy1ahUWL16s8m3UsmXL4OTkBF1dXbi4uMDR0ZFnxEfOuC1ZskQcBXDt2rVcE+8ZM2agQ4cOkn2AqByzhIQElaVeXrx4gYMHD8LY2BgdO3aEhYUFypQpozIJnVSdOnUKQUFB8PT0hK+vLw4dOgQg6xOF9u3bw9LSErt37xbje+bMGQwfPhyzZs2S3P1MmfL19uOPP2LChAmIiooCkDXUvH379jkS740bN8LR0VGyb8yU/+4nT56ofNonl8tx+fJl2NnZwdbWFsbGxqhSpcp721KpUY7d77//Li6FGxcXB39/f9SoUQOmpqZYsWIFdHV1VYZJsyxPnz7FoEGDxD5vRkYG3NzcoK+vLy7xmpmZiadPn6qM6pTig4qiwkn3FyR7ozd+/HicO3dO3LZt2zZ069YNgiDA1dUVpqam2Lhxo2QbPAAICgpC7dq1YWZmBmNjY5QqVQq6urpYu3YtgKzEu06dOmjZsiW2bduG69evo1OnTio3cKl1tFxdXVWG7t67dw/u7u4oX748xo0bp3Ls9evXMXLkSNSsWVP8RpkBkyZNQqVKlRAcHJxjbd/o6GgsWbIEpUqVgiAImDRpUjGVsuRRjtvDhw/F7YrEu3HjxmLi/fLlS5UZbaVq/vz5aN68OZydnXN86/748WP4+PiI35IOGDCgmEpZMgQGBqJ69eqws7ODqakp9PT0IAgC/ve//wHISrzbtWsHKysrLFq0CJGRkXB3d1f5Pllq7UF2kydPRsWKFREaGoq4uDhxuyLxrlGjhjh5pHLfQ8r9kFmzZsHExAQtWrTA8OHDVfY9f/4cGzZsgIuLi8rEaVK+pyn/7VOnTkXNmjUxZ84clQnoEhMT0bNnT7Rr1w6VK1eGIAiIiIgojuKWCMHBwUhMTBR/DgwMRPny5WFpaZljvXc3NzdUqVIFJ06cyFEvpXzdFQVOur9Ayo3egwcPVPYlJCRgy5YtMDQ0FBe7l6pt27ahTJkyCA0NFddWPXDgAHr16qUyycbr16/Rpk0bVK9eHVWrVkWLFi0kO0t5YmIiwsLCVN6aAVnDurp16wY9PT0cO3ZMZd+NGzfQp08ffPvtt0VZ1BJrz549MDIyEt8CKWRv3KKiojBhwgR+qvyf98VN4dq1a7C0tESlSpVUOhdS6yQoX0fr1q2Dnp4efvzxR/Tt2xempqbo37+/yvGpqam4desW5s6dK+lrbceOHShbtix27dolfpt9/vx5TJw4ERoaGpgwYQKArO+UBw0ahAYNGsDQ0BA2NjaSnqVc2Z9//pnrZ1eKuCQnJ6NDhw7Q1NTMMV+FlCjX0eDgYFSqVAkBAQHw8fFB3bp10aZNmxzHp6SkYPXq1ZKuo9ktXrwYFStWRFRUVI5RAgo7duxA7969YW1tLdkHYtu3b4epqanKdffs2TO0bdsW6urq4nBy5f2dO3eGIAgqq6uwwsdJ9xfmfY1e9g79tWvXsGzZMknewOVyOZ4/fw47OzusWrVK3KZw8+ZN9O/fHxoaGvjzzz8BZC3ldPbsWZw4cUKy3yIrhm8prqXly5erLCkUERGBHj16wMzMLMeQ6Pv370v6TYYyX19f2NjYICkpKceb2PddU1K71nLzobgpXLlyBQMHDpRs50rZkSNHsGTJEuzduxdA1gOzwMBANGjQQCXxzl4vpXatyeVyyGQyODs7Y8mSJQBU31a/ePEC//vf/yAIgjhB09u3bxEbG4uLFy9KfpZyZZs2bUKTJk1UPltT1FHFg4mUlBR4e3tzHUXWbNobN24Uh4y/ffsWx44dQ/Xq1VUS7+wPuaX60F9ZcnIy3N3d4efnB+DdfUzxr3LbkJiYKP4s1etOcc1ERETgxYsXALI+PVVMDKx4CKYct0mTJkk2XsVFjdgX5d9//yVtbW2qV6+euA0AqampUUZGhritYcOGNGHCBNLQ0FDZLgWCIFBaWhrFxsaSqampuA0AERHVr1+fRo0aRVWrVqU///yTiIi0tLSoRYsWZGdnR+rq6pSZmUkaGhrF9jcUtXnz5pGhoSHdvn2b1NTUKDU1lbS0tOiPP/6gUaNGERGRjY0NjR49mho0aEBjx46lEydOiOfXqFGD1NTUSC6XF9efUCxy+3vv3LlDycnJVK5cORIEgTIzM0kQBJLL5RQeHk63bt3KcY6UrjWi/MftyJEjFBsbS02aNKHNmzeLdVSqTp06RUOGDKElS5aQjo4OERFpa2tT7969afLkyRQVFUWDBg0iIiI1NdVmXmrXmuJaunLlCtWuXZuIVGOip6dHgwcPJjMzM9q1a5d4769duzaZm5uTmpqa5NoDotzr6LNnzygtLY309PSIiCgjI4MEQSAiokOHDlFUVBSVKVOGVq1aJfk6eu3aNerfvz8NGzZM7HtoaGiQvb09BQcH0927d6lt27ZElNX/UKapqVnk5S1uihgppKam0tmzZyktLY2I3tVZRf8kMTFRPFZbW1vs46mrqxddoYvZ7Nmz6bfffiOirGsmMjKS7OzsaM2aNfTq1SuqWLEi7d+/n7S1tenbb7+lmJgYlb7w0qVLJV9Pixon3SVYQRq9c+fO5ThHap0FIqKUlBR6/PgxpaSkiNsUcSIisrKyolatWtHp06eJKOcNX0o3biIiDw8P6tixI7Vp04Zu375NZcqUoX79+tHy5ctp586dNHLkSCIicnBwoO+//54aNmxIvXv3pn/++Ufl92Tv4H/N5HK5+PdGRETQ/fv3iYioT58+FBMTQz/++CMRvbuWEhISaPXq1XT58uXiKXAJUZC4/fTTT3ThwgWV3yO1OqrM2NiYPD09SU1NjUJDQ8Xt2tra1KtXL5oyZQrt27eP5s2bV4ylLDnS09MpPj5e7KgrtwVERLVr1yYbGxu6evVqru2u1K415Tp64MABunHjBhER9ejRgx49ekSTJ08mond9i9evX1NAQABdvHhR5fdIKW7Z+xBGRkb0008/kaGhoUodVVdXJ3t7e9q8eTNFRETQmDFjirqoJY5cLhfrpKLPpq2tTS1btqSYmBiKj49XOf7s2bPk7e1NMplMZXv2ev01e/HiBQUFBZGfnx8dO3aMiLL6tcuXL6e5c+fS2rVrxcT74MGDVK5cOerevTtdv349R5ykVE+LXfG9ZGcfojwk8I8//hAnQoiNjUXZsmUxceJEleNlMhk6d+6MgICAIi1nSSSXy/Ho0SMYGRnB09NT5Vsg4F1s+/btm+P7RymLi4tDhw4dYGBgIA5FSkhIQGBgICpXrgwvLy/x2D///BMzZsyQ7NAk5frp4+MDS0tLBAUFITk5GS9evICPjw9q1KiBGTNm4MGDBzh79izc3NzQrFkzycYM4LgVRPbh4YqhqAkJCVi4cCHq1q2LKVOmqBzz+vVr/P7775KNmTK5XI7Xr1+jZcuW6NixI+7evauyTxGjsWPHolu3bsVUypJDefjplClT0LBhQ/j6+kImkyEzMxNr165FmTJl4OnpidOnT+Po0aNwcXGBqampZIfgK9fRjIwMJCUlif8PCQlB5cqVVT7VUuy7cOGC5OuocuwWLVqEMWPGiJNorl69GmXKlIGvr684L09CQgK6dOkCFxcXyX7SpqijcXFxaN68OZycnHDw4EFx/8qVKyEIAhYsWCBOPPrixQvUrFkzx3XIihYn3SUQN3r5l9s3oNOnT4eamhr8/f3x+vVrleNTUlLQpk0bLFy4MMd5Unb//n0x8b558yaArEYuKCgIBgYGGDlyZI5zpNxpmDFjBipVqoTjx4+rTO718uVL+Pr6Qk9PD5UqVUL9+vXh4ODAa2D+h+OWN9mXUvP09ETz5s2xadMmxMXFITk5GQsWLECjRo3eu2a51GIG5H4/V3REfXx8ckxAmpaWBkdHR/j4+BRVEUu82bNno2LFijh58iRSUlJU9u3duxe1atWCoaEhTExM0LFjR66jAJYsWYJevXqhVq1aWLRoEc6cOQMACAkJgZGRETw8PHL9HVKLGZDzYeKkSZNQtWpVrFu3TmXlinnz5kFfXx+2trZo3749WrZsiaZNm4rXm1QTb8U1ExcXh2bNmsHJyUmcowh4d79buHChmHjLZDJJXmslCSfdJRg3ennzofVqe/TogVKlSmHWrFniLI03b96Em5sbzMzMJPuQAnh/Y/X48WO0bds2R+K9ceNGCIKApUuXFmUxS4x9+/aprGV548YNNG3aFEeOHAGQNVvouXPnMGfOHBw9elTcFh4ejn/++UeyEzJx3D7dlClTYGBggPnz52POnDnQ0dHBoEGDkJaWhqdPn2LhwoVo0qRJrg/FpOZDayOPHj0agiBg4MCBOHz4MBITE3HhwgW4ubmhadOmkr3GAgMD8fTpU/HnuLg4tGrVCr/99hsA4NGjRzhx4gRGjhyJn3/+GUDWaIro6GjExMRwHUXWyJ3KlSsjICAAa9euRb169eDk5ISEhAQkJSUhJCQE1atXh6ura3EXtcTZsWMH9PX1xTXegawE8cmTJwCA8PBwLF68GJ6enioTBEvxesvtBdO9e/dgYWGRa+Ktrq6OqVOnqjzcllqOUJJw0l1CcKP36RTr1Xbs2FF8g52WlgYvLy9oampCS0sLBgYGMDMzg729vWQfUgCqHdNt27Zhzpw5mDVrFg4fPgwga+3Q7G+8X7x4gd9++02S8frpp5/QrFkzlYbuyZMnaNCgAdauXYuoqCgMHjwYTZo0gZmZGQRBwM6dO3P8Hqk9lee4fbqIiAjUqVNHXEotKioKgiBgy5Yt4jEvXrzA1KlT0bdvXx618x/ltZGHDRumst3AwACCIOCbb76Bqakp2rdvL9n2YOvWrWjfvr1KHXv9+jVMTEwwdepUnD59Gt999x2aNWsGe3t7CIKAn376KcfvkXIdvXDhAho1aoSTJ08CyKqzmpqaCA4OFo95+/YtAgMD0aVLF0nHauDAgZg+fbrKttWrV8PNzQ1A1goVS5YsQd26dVGvXj1MmjRJHK6vTGr1FFCtY3fu3MHDhw9x//59AFmJd25vvOfPn4/WrVtzu1BCcNJdAnCjVzAfWq+2SZMmGDhwoLj/6NGj2LFjB9avX4/w8HB+SPGfSZMmoUqVKhg6dChcXV1Ru3ZtzJ8/H0DWgx9nZ2dUq1YN0dHRKudJscFT/M0XL15EYmIiZDIZPD090aRJE2hqamLMmDH47bff8PbtW3To0CFHx0KqOG6f5tixY2jdujWArAdk2traWLt2LYCsdkKxBmtCQsJ7l1qTgg+tjVynTh04ODiI+y9cuIDw8HCEhobiwoULkm8PFH9/eHg47t27BwBYuHAh6tevDy0tLUyYMEHsyHt4eGDEiBHFVtaS6OLFizA1NQWQ9dZWW1sb69atAwAkJSXh119/RVJSEt68eSOeI7X+GgCkpqZi+/btOZZE27RpEwRBgJeXF2rVqoXvvvsOfn5+mDlzJqpXr47Y2NhiKnHJoXxPnz17NiwtLWFiYoLatWvjl19+AfBuqHnbtm1x6NChHOdKsV0oaTjpLiG40Su4D61X27dv3/eeJ8XEUdmvv/6KGjVqiN+dbdmyBaVLlxZv4EDW2t3NmjWDu7t7cRWz2Cm/ATt8+DAEQcD69esBvBsaffbsWfF4uVwOa2trrFy5sljKW1Jw3PLvwYMHuHbtGu7cuSNu+/3331GnTh3s3LkTOjo6WLNmjcq+Pn365JgcTMo+tDaynZ3de8+TWhIkl8vFhwwZGRk4deoUSpcujenTp+Ply5dIT0/H/fv3ceXKFZVzbG1tMXfu3OIqdrG7ffs2oqKixHYTAE6fPo3atWsjMDAQFSpUUHkpcvz4cfTs2RNXr14tjuKWGNnvSxs2bECXLl3E7b6+vujRowcCAwPFPvD9+/dhYWGBS5cuFXl5SyrFZ6cHDx7E3bt30alTJ5QpU0YckaiYXM3U1BSRkZEAsmIv9XahpOCkuxhxo/fpTp48iRo1akBPTw/Hjx8XtycmJiIoKAgmJiYYMGBA8RWwBFu1ahVcXFwAADt37kT58uXFp/OJiYk4d+4cgKwESWod0g/54YcfULp0afj7+6vMtZCcnIxr167BxcUF5ubmkn1r9j4ctw/bvn07nJ2d0a5dO/GzIoV27dpBEAQsW7ZM3Jaamgp3d3f07t2b6+d/oqOjoaOjA3V1dWzbtk3cnpGRgWPHjqFmzZpwcnIqxhKWbAsWLBBXD4iLixO3JyUlISoqSvITtm7btg2Ojo7o0qWL+FBHoXv37hAEAT/++KO4TVFHv/32W0nX0ewJ35s3b7Bs2TJxRKJif3JyMoCsB2CpqalwdnaGk5OTpGOn7PXr12jfvj3CwsIAZM3tpKurK/bbFCMpYmNjMXjwYI5bCcRJdwnDjV7+PHr0CHPnzkWlSpVyvP1PTEzEpk2boKurK/mHFLndfNesWYORI0fi4MGDKsPhgKwkfMaMGXj16tUHf8fX7Nq1a9i9ezdGjRqFlStX4q+//hL3jR8/HhoaGggICBC/N9uwYQM6deqENm3aSPb7UIDjVhBBQUGoVKkSfvnlF1y4cEHcrphY6MiRI7CysoKFhQX279+PwMBAdOzYEY0bNxbbAqnVTyBnZ14mk2HLli2oUaMGunTporIvIyMDx48fh5aWFkaPHl2EpSx5/vnnH2zevBm9evXC1KlTERISIu5btGgRDA0NMXPmTHGW9x07dqB79+5o166dpOuorq4uQkNDxSVcAYgP+2NiYuDg4AADAwOsW7cOixcvRvv27dG4cWPJz7SdkpKCJ0+e4Pbt2+LkaMnJyfD394eZmRk8PDzEupyYmAhfX184OTnBwsJC8rFT9uDBA+jo6ODGjRs4cuSISr8tNTUVs2bNEt94K0itnpZ0nHQXE2708o/Xqy0Y5bj9+eefkMlkAIC///4bgiBAEARs3rxZPCY5ORkdOnTAyJEjJTskKSQkBC1btoSZmRnMzMxQunRp1K5dG5MmTRKPGT9+PDQ1NbFhwwYAWQ+AlK81KT4Y47jl34EDB1CxYkWVzzqArAmHLCws8OuvvwIATpw4gc6dO0NfXx+tW7eGh4eHZNsCgNdGLqhffvkFFhYWaNWqFdq0aQMDAwNUrlwZffr0EY9R7oO8ePECiYmJiIiIkOy370eOHIG+vr5KOwkA/fv3h6GhIfz8/ABk3cuGDh0KU1NTtG3bFl5eXpKeaRsA9u/fj759+6JKlSr45ptvUKlSJfj5+SElJQVpaWlYt24dzM3NMWDAALG/ERgYCG9vb0nHTvn+phgBAGRdc3379kW5cuUQGBgobr937x46dOggTkQq1b5bScdJdzHgRi//eL3aglG+8U6bNg1169bFqlWrxAcW69evh4aGBpYvX47IyEicOXMGHTp0UFlOTWo3b39/f5QrVw7+/v6IiYkBkDWjaq9evWBgYICxY8eKx06aNAmlSpXCihUrVOIkxafyHLf8Ufzdnp6eGDp0qMpSh+3bt0ft2rVhb2+Ptm3bYt++feK+Bw8eID09XTxfam0BwGsjF5S/vz/Kli2LjRs3irMe3717Fz4+Pihbtiy6d+8uHrt06VLUqFEDY8eOVVl2TYp1dNy4cejTp4/KLNpdu3ZFnTp10K9fP1hZWYmJN5C1moAyKdZRICt5rlq1KqZOnYrQ0FCEhoZiwIABEAQBo0ePhkwmQ0pKCtauXYtmzZph8ODBOfobUqynynXsxx9/xJw5c8R5PhYsWAAdHR307dtXjI1MJoOrqyscHR0lGa8vCSfdRYwbvU/D69UWzIwZM1CpUiWcOnVK5VoCsiYw0dPTg4GBAZo1aybpNd83btwIdXV1HDhwIMe+uLg4DBw4EIaGhirf83l5ecHe3l5yDyeUcdwKJjU1FcbGxli0aBGArHu7TCZDly5d8OrVK8TExMDd3R329vbiN8rKdVLKsQN4beT82LRpE9TV1VWWE1KIj4/H7NmzUaFCBZVvkmfPnq0y2ZUUZWZmomnTppg4caL4c2JiIkaOHImXL1/i33//hbe3NywsLMTYcR0FAgICoK6ujl27dqnE4+3bt1i2bBkEQcC8efMAZI1IXLduHQwNDcVt7N3qMhs2bMC///4rbh85ciTq1asHBwcHeHh4oHXr1jAzM+Oh+F8ATrqLEDd6n4bXqy2Y+/fvw8rKSkyInj59iqioKEyaNAkHDx4EkPXg59KlS7h586ZkR1NcunQJenp6+Pbbb8VtilgorqXY2FhUq1ZNZd1f5f1SvOY4bgWXmZmJ+vXrY8yYMTm2K/zzzz+oVq0aVq1aVdTFK9F4beS8u337NmrVqgVra2txW/YHqo8ePYKlpaW4XrKC1OsoAFhZWam8EAFU6+jt27dhZmaG2bNnF3XRSqRdu3ZBEARxwi8g5/UzadIkaGpqihMFJyYmIiwsTHIP+t/n559/hr6+Pi5fvixuk8lk4miLPXv2YPTo0fD09MTSpUslPRT/S6JGrEjExsbS3LlzqUWLFtShQwciIsrMzBT3V6xYkYYNG0Z16tShv/76S9w+a9Ys2rNnDwmCQACKutglSnp6OhkYGFDz5s0pNDSUHB0dac2aNeTh4UGJiYkUERFBenp6NGXKFPrll184Zv/R0NCgmJgYevDgAV24cIEmT55MQ4cOpT/++INcXV1p7969ZGxsTKamplS/fn1SU1MjuVxOGhoaxV30ImVkZER9+/alFy9e0OzZs4mIxFgIgkAZGRlUu3Zt+u677+js2bP05s0bSk9PJyISrzVBEIrxLygeHLeCAUCZmZlUq1YtOn78OF27di3X4ypVqkSNGzem6tWrF3EJSzZBEEhDQ4Nat25NO3fuJGdnZ/Lz86MBAwZQcnIy7du3j9LS0sjDw4P27t0rXpNSZGBgQN7e3pSRkUFDhgwhIiJ1dXWxD5KZmUnVqlWjPn360MWLF0kmk9Hbt2+JiOuoXC4nc3NzOnv2LB0+fFhln4KOjg7p6+tTrVq1iqOYJU5GRgYREd2+fVvlOlIAQD179qQyZcrQ7du3iYhIW1ubunbtqnJdStmzZ8/I3t6emjZtSjdv3iQ/Pz+ytLQkR0dH+t///kddunSh1atX04YNG2jSpEmkoaFBmZmZkuu3fWk46S4i3Ojlz8OHD+n69et09+5dcVtKSgo9ffqUdu3aRV5eXrRkyRIaOXIkERGdOHGC1q5dS/fu3aMKFSpIMmZElGunskqVKuTl5UWTJ08mOzs70tXVpQULFtDVq1fJycmJwsPDc5yjpiatW4NcLic9PT2aM2cOtWjRgg4ePEhz5swhoqxYKBqzjIwMio2NpSZNmlDp0qVJS0tL/B1Su9aIOG6fSlNTkxYsWEC3b9+m2bNnU0xMDAEgNTU1AkCvX7+mYcOGkVwup86dOxd3cYtNbGwsnTt3jiIjI8Vtb968oaSkJAoKCqLhw4fT4sWLycvLi4iIoqKi6JdffqF79+5RqVKlxHOkdl8jykpwtLW1ydPTkwYNGkQXLlzI0QdRV1enjIwMio6OppYtW5KOjg5pamqKv0OqdVQQBFJTU6MpU6ZQcnIyzZ49W2wv1dXVCQC9fPmSBgwYQG/evKF+/foVc4lLht69e1NwcDD5+PjQ/PnzVZJoxcPYhg0bUlpaGiUnJ+c4X11dvSiLWyKlpaXRn3/+SePGjaOuXbtSREQEDRgwgBwdHWnv3r10//79HOdw3L4AxfB2XXKUl0L46aefYGZmhsGDB4v7lWftHTx4sMowTSni9WoLRvlvv3XrFqKiopCamgog6xq8cOGCuAQRkHXd2dvbY/ny5UVd1BJJUU/j4+Mxfvx4WFlZ5RguqJghNCAgQOUcKeO4FZwiDps2bUKZMmXQpk0brFu3DrGxsdi8eTPatWsn+SWHeG3kT/exPggA/Pvvv3B2dhbbA66jWRT9s7/++gs6Ojpo3Lgxpk2bhvPnz8PPzw9OTk5o2rSpZOdB+ZCff/4Z6urqmDFjRo7v3A8cOABra2vcunWrGEtY/D50jxo3bhy6du2K9evXi5OTnj9/Hubm5oiNjS2qIrLPiJPuIsKNXt7werUFo3ytTJ8+HSYmJtDV1YWlpSXWrFmjMptqUlISLly4ADc3N5VZytm7a0c5gZw1a5a439XVFQ4ODtyxyobj9mnkcjmOHj2K+vXro1y5chAEAaampujXr5+kv9XjtZE/n4/1QVxcXGBjY8N19AOio6Ph6OiISpUqQRAENGvWDMOGDZN0Hf0Y5cRbUSfT0tLg7u4u+bl3lO9NGzduxLBhw+Dt7Y2NGzeK2xMTE8X/v3nzBq6urnB2dub72heKk+4ixI3eh/F6tZ9u7ty5qFKlCvbv34/09HS4uLigdu3amDNnjjhreVhYmLi8BMctp+wJpLW1NebOnQsXFxeYmJhwzN6D4/bpEhISEBsbi4iICDx//lzSy4Lx2sifX/Y+iIWFBTw9PeHi4oIGDRpwHf0Axf0tKSkJL1++xKVLl5CUlCTGlGP2forEe86cOcjIyICrqyuaNGki6RclyiZPnoxq1arB09MTQ4YMgZGREebPny/ul8lkWLp0KTp27MizlH/hOOkuYtzo5cTr1X4eV65cQatWrfD7778DyOq0amtrw9HREcbGxpg/fz6SkpLw6tUrHD16VOWzBqZKOYGcOHEiypcvr/L2jGOWO45bwb2vAyW1jhWvjVy4lPsga9asQeXKlSVfR/P6tlX5uPf9n+Xu559/hpaWFrS1tdGoUSNJ9nVzs2nTJtSuXRtnzpwBAGzduhVaWlooXbo0pk6dKh43b948HlXxFeCkuxhwo5cTr1f76Z4/f45t27YhOTkZ4eHh0NfXF7+hdXBwgLGxMSZMmKAyXElqHfr8UFxTz58/x5o1a/ghRR5x3LLcuXMH586dQ2xsrEriyD6O10YumPwmj69fv8bu3bslW0dfvXpV3EWQlM2bN6uMsJPa9abs7du3yMjIwLx588R+7759+1ChQgUsW7YM8+bNgyAIWLBggXgOj6r48gkAr6n0OSGPM2YrjktMTKTDhw9Tly5dxBlEpTjlv1wup4YNG1LHjh3Jz89PZbtixtlLly6Rq6srTZkyhby9vYurqCXCsWPH6PTp01SqVCmytbUla2trIiJKTEyk8uXL0+DBg0lbW5tWrlxJ6urqNGTIEDp16hQ5OTnRmjVrJDkb7d27d+nly5ekq6tLBgYGVK5cuY+eo3z9EZEk6yfHLf+Cg4Np8eLFlJKSQunp6TR48GAaN24cVa5c+YPnKbcfilmlpcra2pqMjIxo165d4jbl6yo2Npa6d+9OXbt2pVmzZhVXMUsEmUxGOjo6+Tone18le5392u3cuZN27NhBkydPphYtWuT5POW4FSTuX4tPvae/fftWZYZ8KXj27Bmlp6eTkZGRuC05OZmePHlCpUuXpo4dO9LgwYNpwoQJFBkZSe3bt6ekpCTy9fWlsWPHElHecwxWMknnDlvIZDIZEeV9aQ3Fklbly5enbt26kbq6uiTXRibi9Wrza8OGDdS7d286evQobd68maZPn043btwgIqLy5csTEVF8fDylpqaKa4mmpqbSqlWrxIRbas/agoODydXVlbp160Y2Nja0YMECev78+UfPU67PUlwDk+OWf9u3b6exY8fS1KlT6ciRIzR+/Hjatm0bxcbGfvA85c7UypUrady4cZKrp0S8NnJ+7dy5kzw9PSkqKqrAv0Mmk0kq4f79999pwIABdPLkSVq7di2dP38+T+cp11E/Pz8aPXo0JSUlFWZRS5zw8HBKTU0V14XOq+z3Mqkl3CEhIeTm5kZ2dnZkYWFB8fHxRERUrlw5qlOnDl2/fp0AiMvOlS5dmrp06UK//vorjR49Wvw9nHB/4Yr4zfpXaceOHejRowfOnj2br/OUh4JJeZiTIg7nzp1D6dKl0bNnT9y6dUvcLpfLIZPJ4OLigrZt20p6aE1AQADU1dWxc+dOAMChQ4dQo0YNlZl95XI5vL29YWZmhl69eqFVq1Zo2LChGDepDSkPDQ2Fjo4ONm/ejFu3bmHp0qUwNjbG6dOnP3iecv309fXFmDFjJDVsleOWf7GxsbC3t8eKFStUttva2mLUqFHvPU85Pv7+/tDW1sbWrVsLrZxfgjt37kBPTw+tW7fGX3/9JW6Xy+V48eIFXFxcYGdnJ+n24LfffkPp0qVRtWpVDBo0COfOncvTecrX26pVq+Dh4aHy2dHXLD4+Ht9++y0mTpyITZs2oXnz5vDw8FCJXW73q9zqqOJTN6kIDQ2FIAiwsLBASkoKgLwNEVeO3f79+3Hq1KlCK2NJtH79epQuXRorV65EaGgoLCws4ObmpnLMmTNnUK5cOfj5+eHRo0dwdXVFv379eEj5V4aT7k/Ejd7nwevVftymTZsgCAJ27Nihsr1Jkybo378/nJ2dVb5v/OGHH9C/f38MGjRIspOWcBJUMBy3grl06RK+++47/PPPPwDedUiHDRuWY4lIBeV72fr16/HNN99g9+7dhV/YEozXRv44Th4LRi6XY8+ePThy5AgAYNeuXWLsoqKicj1H+fqSah09e/YszM3NMXDgQLRo0QItWrRAcnIygA8n3srX25o1a1CxYkVEREQUenlLio0bN0JdXR1//PGHuG358uUYMWIELl68iLt37+LVq1fIzMzE5MmTUbZsWdSqVQsWFhbi/U0qD62lgJPuT8CN3ufH69W+35gxYyAIgsoM7l26dIGhoSHGjBmD/v37QxAETJkyJdfzpRg3ToIKhuNWcMr3f0VnfcGCBRgxYoTKcQ8fPlT5OSAgAN988w127dpV+IX8gvDayLnj5LHgsj+k2blzZ47+W3x8PC5duqRynJTraGhoKAYNGoQrV64gPDwczZo1+2jirdzXXb9+PSpUqJDjpcHX7OLFi9DR0UGvXr1Utjs4OEBfXx9VqlSBtrY2RowYgVevXiExMRGXL1/GwYMHJTu54deOk+5PwI1e4eH1anM3bNgwaGtrY9++fejWrRuaNm2K2NhYcf+4ceOgra2Ne/fu8ZIm/+EkqGA4bvmTvY4p/zxp0iR07txZ3N69e3dMmjRJ3L969WpoaWkhLCysaAr7heC1kT+Mk8dPo/ygUBG7AQMG4M8//4SdnR3atGkj7vfz84OGhoak6+iFCxcAvHs5oki8FaszZGZm4u3btznuhYq+rtSut8ePH2PYsGGws7MTR4317t0bJiYmiIiIwIsXLzB9+nRoaWmpvExRkPr97WvESfcn4kbv8+P1anNSvs6GDBkCQRBgZGSEmzdvqhzn6+sLS0tLyGSyoi5iicNJUMFw3D6/iRMnomfPngAAV1dXGBsbi0MHAYjf+kkJr438+XDyWHDK19Hu3bvRrFkzaGlpwdTUVKWObt26VXJ1VCG3vldmZiaOHTsmJt6pqalIS0uDl5cXLl++LB63du1alC9fXnIvlxTX1b///ovRo0ejVatWMDExQdOmTfHkyROVY/X19TF37tziKCYrYtKZrrKQKJZ0kcvlRETUo0cPmjJlCt24cYP8/Pzo0KFD1LVrV3G6fyKi1atX0/fff0+bN2+m7t27F0u5i9Ldu3fp/PnzdOfOHUpOTv7o8e+bRVVKs6tmp66uLs4UGhQURGPHjqXnz5/TlStXKDU1lYiyrsHDhw9TgwYNxFnMpSz7LJ/KPwOgUqVKERGRu7s7nT9/nhYsWCDuz8zMpJ9//pm6du1aNIUtQThuebNv3z46ffr0B49RtAtVqlSh8uXLU5cuXSgmJoZu3bpFmpqalJGRQUREY8eOpd69exd6mUuCgqz08bH/S52ampo4O3SPHj3Ix8eHrl69Sp06dSKZTEaHDh0Sj61YsSL98ssvXEf/o7yah7u7O71+/ZqaNWtG58+fJ01NTXr79i0REfXt21cydVRBcf/Kre+lpqZG9vb2tGzZMgJAtra25OjoSHv27KFGjRoRUdYyr/7+/hQUFETdunUr0rIXN8V1VbVqVZo2bRo1b96cEhMTycXFhQwMDIgoqz199OgRGRgYkLGxcfEWmBWNYkz4vzr8xDSnzZs3w8TEBDVq1ECVKlXg4+ODZ8+effQ85VjyEJt3lGMxdOhQlC1bFjt37kRKSgrc3NxgYmIiDsGX2siAX3/99aOzoipismzZMgwZMgSdO3dGvXr1xPopxc8XOG75J5fL0aFDB5QvXz5Pq1bMnDkTgiDAzMxMsjEDeKWPgspLHQXexSktLQ1169aFtbW1eJ0p90GkIL91NDU1Fa1bt4ahoaHk5wuIi4sT//++ESXZ+7uCIKBVq1Yq19nbt29x69atwivoF0ARpydPnmD06NGwtrbGokWLxP3u7u6wsrLifq5EcNKdR9zo5R8vOVQ4lG/Onp6e+Oabb1C/fn00atRIsh16ToIKhuNWcG/evEHXrl1RpUoVnDlz5r3HyeVybN26Fc7OzpLuzPNKHwXDyWPB5bWOKtrU48ePS/6+9vfff6N169b4+eefxW0f6n/JZDK0atUKTZo0UbnepPbQ/0NyG2q+dOlSuLm5qTy85sT768dJdx5wo5d/vORQ4VK+OQ8cOBD169eXfGeBk6CC4bgV3Js3b9CpU6f3xu7Ro0cYNGgQ4uLiJD0ZJK/08Wk4eSy4j9XRx48f47vvvlOZH0XKMbt9+zacnJzQvn17hISEiNvfl3j/9ttvcHFxkfz19rEXQ8qJt7e3N8qVK6eyDK5U4yY1nHTnETd6+cNLDhVMft7oKyfeUu7QK+MkqGA4bgX3vtg9efIEbdq0QcWKFcVYSXXEDq/08ek4eSy4j9VRfX19jhXe9cHu3LkDV1dXODo6qiTeyn20hIQE8SWUlNuEqKgo3L59G0DeE+9Hjx7B19eXH15LECfd+cCNXv7wkkN5Fx0dLf4/Px3z7J8s8JAuToIKiuNWcGlpaejcuTMMDAwQGRmJ+Ph42NnZoWHDhjx08D+80sen4+Sx4LiO5o2iDxEbG5tr4g1kXW/W1tbw8PAQt0mxTYiJiYG5uTk8PDxw584dAHlPvBW4vkoLJ935xI3ex/GSQ/mzZcsWCIIAb29vcVteGjDlY44cOYLnz58XSvm+RNzBKhiOW8GlpaWhS5cu0NfXh4mJCQ8dfA9e3urTcB0tOK6jefOhxDshIQH29vYqn7RJma+vL2xtbeHp6ZmnxFv5/ifFBxVSJwD/rZXA8iw9PZ169uxJkZGRtG/fPqpTpw517dqV4uPj6dKlS6SpqUmZmZnicmLsnUmTJtH9+/dpx44d5ObmRteuXROXzyEiWrVqFVWpUkUyS3OcOnWKhgwZQk2aNKFDhw7RoEGDyM/Pj4iylpN437I4yvvWr19Ps2fPpn379lHLli2LrOwlXXp6OvXq1YtOnz5Nenp6pK6uThcvXhSXatLQ0CjuIpZIHLeCS0tLox49elBsbKzYFnDMclK+f4WFhdGCBQvo6tWrZGJiQufOnRPbg5CQEFJXV5dMe5BXXEcLjuto3sjlclJTU6M7d+7QmDFjKC0tjXr27EkhISH0/PlzycdOuY+/evVqCgkJoSZNmtC0adOoVq1aufbflLetWrWKHj9+TIsXLy7ysrPiw0l3AXGj986+ffuocuXK1KpVq/ceo7iBL1++nK5du0bx8fF0/fp1io6OlmTMFNatW0cXL16kCRMm0OXLl2ngwIHk6en5wcRbeZu/vz9NnjyZgoKCqEePHkVe/pKOO1gFw3EjevDgAenp6VG5cuXydV5aWhppamqSmpqa5GJGlLf2gOjdfSw9PZ0aN25MlSpVor///ps0NDTo7du3YuLNcsd1lOtoYVNOvH/44Qfav38/NW7cWFzDXIqxO3/+PFWrVo3KlStH33zzjbjdz8+Ptm7dSqampuTj40O1a9dW6asp/z8gIIDGjRtHgYGB1KdPn2L5O1gxKY7X61+LN2/ewN3dXWVYl9SGKPGSQwWze/dunDx5Ek+ePBE/U8jMzERoaCjKlCmDMWPGqByf23BBxeRCUvnW8dixY7h27Vq+z3vz5o04pEuK1xpQsOUKpRy3HTt2oFKlSggODkZKSkqez8s+p4LUhg/ySh8Fx3U0f7iOFty1a9fw5MmTPB2riNft27cxZcoUSddTxXrkxsbGaNy4MXx9fVUmd9y6dSusra0xdOhQxMTEAMi6vniSYKbASfd/uENfcLzkUP6sW7cOmpqaOHbsWI59GRkZORLvZ8+eYeHChbhx44Z43E8//YQKFSpIJuFes2YNdHR0cP78+XydJ/UO1q+//oqhQ4fCxsYGK1euzPN5Uo2b4u8cNmwYBEFA5cqVsWXLFqSmpqocl9uEhcoxOnPmDF69elW4hS2heKWP/OE6mj9cRz/N5cuXUatWLcycORPPnj3L0znZYynVb7mjoqIgCAJq1aqFwYMHw8bGBpUrV4aZmRkGDBiAkydPYsyYMXBzc8Pw4cMRGxurcr6/v7+kXpSwnDjpBnfoPwdecihv1q9f/9GJgd6+fYvt27ejbNmyGDZsGGxtbVG3bl3xejtz5gyMjY2xffv2oip2sVq/fj00NTXzvT6v1DtYgYGB0NXVxciRIzFixAgIgpCnNe+lHDdFHdu6dSv8/PwwY8YMlCpVCps3b/7gecoxW7NmDerXr4+rV68WallLMl7pI2+4juYf19FPN378eJiammL+/Pl4+vTpR4/nVVHePSQ8e/YstLS0MG7cOFy/fh0PHjzAsmXL0KlTJ9SvXx+1atWCIAgQBAGLFy8Wz/fz80Pp0qV5UkiJk3zSzR36z4eXHPqwgIAAaGlpYc+ePSrbN2zYIM56qSCXyxEQEABBENCyZUuVJ8txcXG4cuVKURS52IWEhEAQBPz6668AgPv37yM0NBQrVqzA/v3733ue1DtYe/fuReXKlVWeqPfo0QNbtmz54FsKqcdNITQ0FK1atQKQ9UatTJkyCA4Ohq2tLbZs2aJyrHLMFEMHd+zYUaTlLYl4pY8P4zr6abiO5p3i709LSxO3TZkyBQ0bNvxo4q0cO19fXyxZsqTwClpCKRJuxb8nTpyApqYm+vbti4SEBPG4S5cu4dixY/Dw8ICHh4fK/W3hwoX5zjPY10fSSTd36D8/Xs4kd8ePH4cgCJgzZ47Kdnd3d7Rs2RIvX75U2f7y5UtYWlrC3NxcvHFLbUhXWloabGxs0LBhQ1y6dAk3b96EqakpLC0tUa9ePQiCgOHDhyM+Pl7lPKl3sDIyMjBs2DDMnz9fJRbW1tZwcHBAo0aNMHr06Bwje6Qct927d+Pw4cMAsuIQExMDW1tbcb+XlxfU1NTQoEEDPHz4UNzO3+p9GLcHueM6mn9cRwvu8uXLSEtLy/Ht+6RJk2BiYvLexFv5egsICEDZsmXzNBLja3H58mWVnzMzM8X+2N9//w0tLS14eHjg3r17Kse9efNG/L+UHyyynCSbdHOHvvDwWpg53bp1C3Z2dujcuTOioqIAAN27d4epqSnu3r0L4N21JZfLsXnzZtja2ko2brt378bFixdx9epVdOzYETY2NtDX18eECRMQFxeH1NRUHD58GJqampg3b554Xm71U0odrN27d+Ps2bN49OiRyoPAzp07w8jICCtWrMDGjRtRuXJl9OnTR9wv5bitW7cOWlpa+Ouvv1S2W1paiiNQLCwsYGhoiDJlyiAkJCRH51XxiRJ/q5cTtwequI7mH9fRgtu1a5c4eW2HDh2wbds2REREiPsXLVqEBg0aYN68eXj8+LG4PbeHFVIaGh0WFgZBEODo6IigoCD8888/OY5RJN4DBw5UedCjIMURnezDJJl0c4e+8PHM7jndunULzs7OcHNzg62tLSwsLHIk3EDWZB3KM15KLW6KieaOHj0KALh48SIcHBwwePBglaFcAPC///0PderUQUJCgkonQYodrPdN0Hfp0iV4e3urfMKwd+9eCIKAW7duqRwrtbjlNseCoi66u7tj7969sLa2Rps2bQAAY8aMgSAIOHjwoHj84cOHUbVqVX74+gHcHmThOpp/XEc/jWLG7erVq8PT0xO1atVC1apVYW9vj9mzZ+Pu3bsYOnQo7OzssGDBApXEG5Du5F/Hjx9H+/bt4enpCS8vL1SsWBELFy7EkSNHVI4LDw9HmTJl4O7unueJ6Zh0SS7p5g79p8vr0zue2T2nW7duoV27dtDR0RE7AMrXVseOHdGyZUvxZ6k9KX3fRHMPHjxAeHi4+LMiLj4+PujYsaPKsVLsYH1sgj7FcDdF3LZt2wZbW1vIZDLxGKnF7X1zLPj7++PJkyeYNm0aBEFAmzZtVDqiy5YtU7mfXbt2DZGRkUVV7BKH24O84Tqaf1xHP4+wsDBoaGhgxYoVuHfvHs6fP4/vv/8erVu3hpGREaytrSEIAtTV1VUmpFu9ejXKlCkjyZdLMTEx6Ny5s/jwJiQkBB4eHmjRogUGDhyIkydPiiNhjx07BgcHB55wjn2UpJJu7tDn39GjRzFv3jyMHTsW+/bty/N5PLP7+92+fRsdO3aEi4uLynXn4uKC+vXrS+7bbYUPdbAePXqU4/jU1FS4uLhg3LhxKtul1sH60AR9t2/fBqBa/9LS0tCpUyf0799fZbuU4vahORZatGgBmUyGw4cPY+7cueJ6ttm/PZZi4sjtQcFwHc0/rqOfJnsstmzZAjU1NUydOlVl+9GjRxEaGgp7e3u4u7uL5yUlJWHcuHEIDQ0tsjKXNLNmzYKJiYn4BvvRo0fQ19dHhQoV0Lp1a7Rs2RKBgYEq53DizT5EMkk3d+jzLzAwEPr6+nB3d4ednR0EQUBISMhHz+OZ3T9OMdTc1dUVERER6Natm0rCLbXOQn4mmktJSUFkZCQ6deoEU1NTSc+IX9C4NW7cWIybFDsJH5pjQXltVeWOqxTjpIzbg4LhOlowXEcLJvva0HK5XKyDW7duhbq6OqZOnaoykzkAJCcni8cpYpp97XOpUNS7p0+fwtnZGceOHUNGRob4XfzTp09x8OBB9OnTBzY2NpLse7CCkUTSzR36/Pvjjz+gr68vrgWdnJyMyZMnw8nJCTKZ7L2NG8/snne3bt2Cm5sbNDU10aBBA8km3ED+JpqLjIyEg4MD7OzsJD0DMpC/uJ09exbOzs5wcnKSfNyAD8+xoByXv//+u5hKWHJwe1BwXEcLjuto/mzfvh1qampwcnLC8ePHcyxFCrxLvKdPn55rX0NqfV0g62FgcHAwFi5cqBKTzMxMDBgwAB06dEDTpk3h4OCAf//9V+Vc5QlwGfsYSSTd3KHPn7dv32L48OHw8vJSuZGEhoaiZs2aOWYFVeCZ3fPv+vXrGDNmjHijl2LCrZDXieaOHTuG2NhYsV5KOWZA/uIWFxcn6e9qs/vYHAvOzs6wtraWdIeK24NPx3W04LiO5l1wcDC+/fZbDBgwAN27d0eTJk2wfv163LhxQ+W4LVu2oFSpUhg1apTkRwcEBwfDzMwMY8aMwYYNG3LsVwwpb9asmcooHeXrja89lleSSLoB7tDn1e7du3HkyBHs3r0b27ZtU9kXHR2NGjVqID4+PsdNhtfC/HRSu9Zyk5cOlpWVVY5hcFKX37hJvaOljOdYeD9uDz4frqMFx3U0b86cOQMrKytER0cjMTER69atg7W1NTp06ICJEyfiwYMHSE5OBpD1yaWdnZ2kE8bg4GCULVsWu3btUhnxumLFCiQlJQEAZDIZhgwZggEDBgDgBJt9Gskk3QB36D9GsRZmeHh4rk/x7t27ByMjI8TFxYn7sk+mI/WZ3dmn4w5WwXDcCo7nWMiJ24PPj+towXEdzZvvv/8e7dq1E1fjuXnzJr755huULl0aLVq0QO/evVWWUwOkmUheuHABdevWxZo1a1S29+rVC4IgoHnz5nj9+jWArOX7lFc9YqygJJV0A9zovY9iZvf3vY2Qy+WIjo5G1apVxRuRk5MTatasKd6wpTazOys83MEqGI5bwfEcC+9we1B4uI4WHNfR91O8JIqKikLbtm3F77nNzc3h7OyMp0+fwt/fH23btpX0G27F371p0yZYWlqqPDScOHEiTExMEBoaCisrK1haWopL9rVq1QrDhw8vljKzr4fkkm6AG73sPrScieLGLZfLcenSJdSsWRMvXryAu7s7GjVqpPKQQkozu7PCxx2sguG4FRzPscDtQVHgOlpwXEezXLhwAWFhYQgKCsoxg3u7du3Qq1cvWFpa5pj8Kz09XdKfLij+di8vL7Ro0QLAu0R8z549eP78OQDg9OnTaNKkCczNzQEABw4ckNzoV/b5STLpBrjRU8jPzO5xcXEwNjZG/fr1Ubt2bTFmUh0dwAofd7AKhuP26aQYM24Pig7X0U8n1Zht2bIFlpaWGDRoEDZu3Jhj//nz51GuXDnY2Nio1FllUk68AWDu3LnQ0dHJMRu5Qnp6OsaPH4+uXbuqbOfEm30KNZKoevXq0bJly8jLy4uuXr1KmpqalJGRQRoaGsVdtCJlaGhItra2dP78eTp37hwREfXo0YPi4uJo+/btpKurSwCIiOj169d0//59KlOmDN28eVOMmaamZnH+CewrZmJiQn5+fqShoSHJ+llQHLdPJ8WYcXtQdLiOfjopxmzz5s00cuRI8vHxocWLF9PgwYPF7ampqUREVLVqVbKysiIrKyvS1dUluVye4/eoqUmr+3/p0iX6448/KDQ0lIiI7O3tqVy5cjRt2jRKSEggIqL09HTx+IyMDLpx4wY1btxY5feoq6sXXaHZV0eAogWVOCk3ejExMeTt7U3q6uokk8koOTmZwsLCyNjYmACQIAgEgI4fP04pKSnk7OzMHQXGGPsKcXvAWMkUGRlJ3333Hfn4+NDw4cPF7X369KHt27dT9+7dKTg4mMqWLSsm56dPnyZzc/PiK3QJsGXLFlq5ciXVrVuXnJycaMSIESSXy2nEiBG0d+9e6tWrFy1cuJB0dHSIiOj+/fs0YsQIev78OUVGRpKGhoZ472PsU3DSzYgoq6P1/fffU1RUFG3YsIF69uxJcrlcfBrq4uJCMpmMTp48SYIgcAeLMca+UtweMFZyKBK+VatWUVhYGO3YsYMMDAyIiGjYsGEUERFBQ4YMobCwMDI0NKTg4GAqV64cNWjQgDw8PGjGjBnF/BcUny1btpCXlxdt2rSJHBwcxLgREWVmZtLAgQPpzz//JF1dXerduzfFxsZSXFwcvXnzhk6fPk2ampqUmZnJb7jZZ8FJNxPFxsbSqFGjSE1NjaZOnUr29vZEROTq6kq3b9+m6Oho0tTU5Cd+jDH2leP2gLGSQfHAq1u3bvTmzRv6448/xHoXEBBA3bt3Jz09PQoNDaXFixdTnTp1KCwsjIKDg8nDw0OyCeP169epW7duNHr0aBo1apS4HQBlZmaShoYGyeVyCg4OpgMHDtCVK1eofv361Lp1a5owYQKP4GGfHSfdTIViaKGamhpNmzaNVqxYQVevXpX0d++MMSZF3B4wVnKMGjWK9u/fTxcuXCA9Pb0c+1NSUsjT05M0NTUpODhY3C7VN7UHDx6k8ePH0759+6hu3bo59r99+1ZlDgqZTCYOMSeSbtxY4ZHWTArso+rVq0d+fn4kCAI5OjpSdHQ0d7AYY0yCuD1grPjcuHGDIiIiKDw8nIiIrK2tKTk5mdauXUvJyclElDUfkQIAkslk1KhRI5XfI9XE8caNG/Tq1SuqVq1arvs1NTXp3r17tGXLFiIi0tbWVtkv1bixwsNJN8uBZ3ZnjDFGxO0BY8Xhl19+ocGDB9OCBQvo3r17RETUt29fMjc3p2XLltG6desoMTFRHCL977//Uvfu3en58+c0YcKE4i18CaGrq0tPnjyhhw8fEpHqAwqirIcUW7ZsodjYWCLiJJsVPh5ezj6KO1iMMcaIuD1grLAFBwfTqFGjKCgoiKytralmzZriN9wZGRnUpk0bun79OjVt2pT69u1LV65coStXrtDr168pMjKSJ//6T0JCAjVv3pyqVKlCf/31F2lqaqoMKU9OTiYPDw9ydHQkb2/vYi4tkwJ+080+ijtYjDHGiLg9YKww/fPPPzR37lxatmwZ9e7dm2rWrCnuUzzwCg8PpzFjxlBmZiZNnz6drl+/Tra2tnT27FlxJIrUE24iovLly5O3tzddv36d2rdvTzKZTEy47969S7169aLHjx+rTLLGWGHiN92MMcYYY4wVE8Wb7JCQEPL19aU9e/aQkZFRjuOyT/717Nkz0tfXF3/mN9xZFPFMSUmh1atX08qVKyklJYXatWtHiYmJ9OrVK1JTU6O///6bRwawIsNvuhljjDHGGCsmimX3/vnnH0pNTc014SbKmvzr/v37dOTIESIilVnMAXDi+B9BEEgul1PZsmXphx9+oAMHDtCQIUMIABkZGdHQoUPp5MmTPDKAFSkeJ8YYY4wxxlgxq1ixIt2/f19cvkqxRrdCZmYmrVixgmrXrk3t2rVT+dxDkbizLGpqagSASpUqRebm5mRubp5jTgrFet2MFQV+080YY4wxxlgxUXzp6ebmRmXKlKHhw4dTeno6qampUXp6unhcSkoK3bt3T2U9aSnav3+/+Lb/QxQPIhTxzZ5g8xtuVpQ46WaMMcYYY6yYKJLDunXrUt++fSk8PJyGDRtGAEhLS4uIiB4+fEh9+vShFy9eUP/+/YuzuMUqKCiI+vXrR6dPnyaZTJancwRBoOxTWMnl8sIoHmPvxROpMcYYY4wxVowUk3/JZDKaMmUK7dixg8qXL09du3alp0+fUlxcHKWlpdHp06clO/nXgQMHqE+fPuTv70+9e/cmopyTx2Ufkk/0LrZEREeOHCFzc3OqVKlS0RWcMeI33YwxxhhjjBUrxeRfOjo6tHz5ctq8eTPZ2NjQmTNnKD09nb799ls6c+aMpCf/unTpEg0cOJB69+5N165do1GjRpGTkxONGTOG9u3bR0TvvuVWUE64169fTx4eHnTnzp1iKT+TNn7TzRhjjDHGWCF79eoVVahQ4YPHZH9Tm5ycTOXKlRN/luIbboWuXbtS1apVafbs2WRlZUU2NjZkYGBAp06dInV1dRowYAANHz5cPF454fb396fJkydTUFAQ9ejRo7j+BCZh/KabMcYYY4yxQrR9+3aaOnXqR49TJNyKd2Jly5ZV2S+1hDssLIwOHz5MRETNmzen5ORk2rVrFzk6OtLGjRtp+fLltHv3bjI2NqZff/1VnHgut4R748aNnHCzYsNJN2OMMcYYY4UkMDCQ+vTpQwEBAXTs2LE8naOY/Et5KTCpDU5dv3499enTR5xMzt7enkJCQmjevHmkpqYmbq9WrRqNHj2aDhw4QFevXiWid5PTrVmzhqZOnUobN26k7t27F88fwhhx0s0YY4wxxlih8Pf3Jy8vL1q7di116tSJ9u7dS5mZmR+dPVs54Y6MjCSZTCaptbj9/f1pzJgxFBoaSg4ODkREZGdnRxs2bKAXL17QnTt36O7du+LxOjo6ZGVlRXp6euK2yMhIWrZsGfn7+3PCzYodJ92MMcYYY4x9ZqtXr6axY8fS7t27ycvLiywtLemXX36hZ8+e5ZjwS5lywr127VoaMGAAPXz4sCiLXqw2bNhA3t7etHPnTuratau4PTAwkMzNzWnhwoUUHh5Os2fPpj179lB0dDRNmjSJSpUqRTVq1BCPr1atGv3222/Uq1ev4vgzGFPBE6kxxhhjjDH2GT148IDq1KlDW7dupZ49exIRkUwmI3t7e2rTpg35+vrmWNqKKPdvkQMDA8Xf8bX766+/yMnJiWbPnk0zZ84Ut7u7u1N8fDwdPHiQKlSoQLt27aIZM2ZQQkICVa5cmfT09OjIkSOSXU6NlXwaxV0AxhhjjDHGvhZhYWFUsWJFevjwIenr64vby5YtSzY2NhQREUHp6elUunRplSQ7t4R706ZN1K1bt2L5O4qDoaEh2dra0vnz5+ncuXPUvHlz6tGjBz148ID27t1LFSpUoMzMTOrRowfZ2dlRUlISpaamUqNGjUhNTY0yMjJIQ4PTG1by8JtuxhhjjDHGPoP169fT2LFj6dChQ+K3yETvlgKLi4sjExMTWrBgAY0bNy7X37F27VqaNm0aBQUFSfJb5JiYGPL29iZ1dXWSyWSUnJxMYWFhZGxsLD6YAECnTp0iGxsb8bzsy60xVpLwlckYY4wxxtgnym3yLwU1NTWSy+VUrVo16tevHx06dIji4+NzfNd95MgRmj9/Pm3YsEGSCTcRUb169cjPz4/S0tLoypUr5OPjQ8bGxiSXy8WRAK6urjRx4kQCIMaQE25WkvHVyRhjjDHG2Cf40ORfilm21dTUSENDg9zd3emvv/6iy5cv55iR3NDQkPbu3SuZb7jfp169erR+/XqytramTZs20YkTJ8Sk2tXVle7cuUMnTpwgQRAkNas7+3Lx8HLGGGOMMcYK6H2Tf3Xq1ImePXtGBw8eJF1dXZVzWrVqRY0aNaKgoKAc63GzdxRDzdXU1GjatGm0YsUKunr1Kl29epU0NTX5G272xeA33YwxxhhjjBVQ9sm/iIh69OhBcXFxtH37dtLV1c0xjHzmzJkUEBBARMQJ9wcohpoLgkCOjo4UHR3NCTf7IvGbbsYYY4wxxj5BXib/IiI6ceIE2dvbi+fx8lZ5c+PGDVq7di2tWLGCNDQ0OOFmXxxOuhljjDHGGPtEMTEx9P3331NUVBRt2LCBevbsqTKjtouLC7169YpOnTpFRPyGu6A44WZfIk66GWOMMcYY+wxiY2Np1KhRpKamRlOnThXfaru6ulJsbKw4NJoxJi2cdDPGGGOMMfaZ8ORfjLHseCI1xhhjjDHGPhOe/Isxlh2/6WaMMcYYY+wz48m/GGMKnHQzxhhjjDFWiDjhZkzaOOlmjDHGGGOMMcYKCX/TzRhjjDHGGGOMFRJOuhljjDHGGGOMsULCSTdjjDHGGGOMMVZIOOlmjDHGGGOMMcYKCSfdjDHGGGOMMcZYIeGkmzHGGGOMMcYYKyScdDPGGGOMMcYYY4WEk27GGGOMMcYYY6yQcNLNGGOMMcYYY4wVEk66GWOMMcYYY4yxQvJ/IVjxF9KhjRkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IS data\n",
        "is_data = {\n",
        "    'Experiment': ['E3_L2_DiscIn', 'E4_L1_GenW', 'E1_Baseline', 'E5_L2_DiscW',\n",
        "                   'E2_L2_GenOut', 'E4_L2_GenW', 'E5_L1_DiscW', 'E2_L1_GenOut',\n",
        "                   'E3_L1_DiscIn', 'E7_CGAN_L1_DiscW', 'E6_CGAN_Baseline'],\n",
        "    'IS Score': [3.5425, 3.5390, 3.5376, 3.5081, 3.4781, 3.4440, 3.4408,\n",
        "                 3.4228, 3.3823, 3.0467, 2.7588]\n",
        "}\n",
        "\n",
        "df_is = pd.DataFrame(is_data).sort_values('IS Score', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(df_is['Experiment'], df_is['IS Score'], color='mediumseagreen')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Inception Scores by Experiment (Higher is Better)', fontsize=14)\n",
        "plt.ylabel('Inception Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f'{yval:.4f}', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "-SWi5QRSg0jD",
        "outputId": "f8b6c804-3a63-43cd-8d47-2b2956525591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}